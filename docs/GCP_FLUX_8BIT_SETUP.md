# GCP FLUX 8-bit ì–‘ìí™” ì„¤ì • ê°€ì´ë“œ

> 2025-11-22 ì‘ì„±
> GCP L4 GPU (22GB VRAM) í™˜ê²½ì—ì„œ FLUX ëª¨ë¸ì„ 8-bit ì–‘ìí™”ë¡œ ë¹ ë¥´ê²Œ ì‹¤í–‰í•˜ëŠ” ë°©ë²•

---

## ğŸ“‹ ìš”ì•½

| í•­ëª© | ê°’ |
|------|-----|
| ëª¨ë¸ | `diffusers/FLUX.1-dev-bnb-8bit` |
| ì–‘ìí™” ë°©ì‹ | bitsandbytes 8-bit |
| GPU ë©”ëª¨ë¦¬ | ~21GB |
| ìƒì„± ì‹œê°„ | 77ì´ˆ/ì´ë¯¸ì§€ (1024x1024, 28 steps) |
| í’ˆì§ˆ | ì •ìƒ (ëª¨ìì´í¬ ì—†ìŒ) |

---

## ğŸ”§ ì˜ì¡´ì„± ë²„ì „ (í˜¸í™˜ì„± ê²€ì¦ë¨)

```toml
# pyproject.toml
dependencies = [
    "diffusers>=0.26,<0.35",
    "transformers>=4.44.0,<4.57.0",
    "torchao>=0.9.0",
    "bitsandbytes>=0.41.0",
]
```

### âš ï¸ ë²„ì „ í˜¸í™˜ì„± ì£¼ì˜ì‚¬í•­

1. **diffusers 0.35.x + transformers 4.57.x**: `AutoImageProcessor` import ì—ëŸ¬ ë°œìƒ
2. **torchao 0.7.0**: `Int4WeightOnlyConfig` import ì—ëŸ¬ ë°œìƒ
3. **ìœ„ ë²„ì „ ì¡°í•©ì´ ê²€ì¦ëœ ì•ˆì •ì ì¸ ì¡°í•©ì„**

---

## ğŸ“¦ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸

### âœ… ê¶Œì¥: `flux-dev-bnb-8bit`

```yaml
# model_config.yaml
models:
  flux-dev-bnb-8bit:
    id: "diffusers/FLUX.1-dev-bnb-8bit"
    type: "flux-bnb-8bit"
    requires_auth: false
    params:
      default_steps: 28
      max_steps: 50
      use_negative_prompt: false
      guidance_scale: 3.5
      supports_i2i: true
      max_tokens: 512
      default_size: [1024, 1024]
      max_size: [2048, 2048]
    description: "FLUX.1-dev 8-bit ì‚¬ì „ ì–‘ìí™” (bitsandbytes)"

runtime:
  primary_model: "flux-dev-bnb-8bit"
```

### âŒ ì‚¬ìš© ë¶ˆê°€: `flux-dev-fp8-pre` (torchao FP8)

```
diffusers/FLUX.1-dev-torchao-fp8
```

- **ë¬¸ì œ**: torchao ë²„ì „ í˜¸í™˜ ë¬¸ì œ
- **ì—ëŸ¬**: `The size of tensor a (4096) must match the size of tensor b (10240)`
- **ì›ì¸**: ëª¨ë¸ì´ êµ¬ë²„ì „ torchaoë¡œ ì–‘ìí™”ë˜ì–´ í˜„ì¬ ë²„ì „ê³¼ í˜¸í™˜ ì•ˆ ë¨
- **ê²½ê³  ë©”ì‹œì§€**: `Models quantized with version 1 of Float8DynamicActivationFloat8WeightConfig is deprecated`

### âŒ ì‚¬ìš© ë¶ˆê°€: ì§ì ‘ FP8 ì–‘ìí™” (TorchAO)

```python
from torchao.quantization import quantize_
from torchao.quantization.quant_api import Float8WeightOnlyConfig
quantize_(transformer, Float8WeightOnlyConfig())
```

- **ë¬¸ì œ**: ì–‘ìí™”ê°€ ì‹¤ì œë¡œ ì ìš©ë˜ì§€ ì•ŠìŒ
- **ì¦ìƒ**: ì–‘ìí™” ì „í›„ ëª¨ë¸ í¬ê¸° ë™ì¼ (22GB â†’ 22GB)

### âŒ ì‚¬ìš© ì‹œ ëª¨ìì´í¬ ë°œìƒ: optimum-quanto FP8

```python
from optimum.quanto import quantize, freeze, qfloat8
quantize(transformer, weights=qfloat8)
freeze(transformer)
```

- **ë¬¸ì œ**: ì´ë¯¸ì§€ê°€ í”½ì…€ ëª¨ìì´í¬ë¡œ ì¶œë ¥ë¨
- **ì›ì¸**: VAE ë˜ëŠ” ì¸ì½”ë” ê´€ë ¨ ë¬¸ì œ ì¶”ì •

---

## ğŸš€ ì„¤ì • ë°©ë²•

### 1. pyproject.toml ìˆ˜ì •

```toml
dependencies = [
    "diffusers>=0.26,<0.35",
    "transformers>=4.44.0,<4.57.0",
    "torchao>=0.9.0",
    "bitsandbytes>=0.41.0",
]
```

### 2. model_config.yaml ìˆ˜ì •

```yaml
runtime:
  primary_model: "flux-dev-bnb-8bit"
```

### 3. test_flux_gcp.yaml ìˆ˜ì • (í…ŒìŠ¤íŠ¸ìš©)

```yaml
model:
  name: "flux-dev-bnb-8bit"
```

### 4. í™˜ê²½ ì„¤ì • ë° ì‹¤í–‰

```bash
# .venv ì‚­ì œ í›„ ì¬ì„¤ì¹˜
rm -rf .venv && uv sync

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
uv run python scripts/test_flux_gcp.py
```

---

## ğŸ“Š GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„

8-bit ì–‘ìí™” ì ìš© í›„ì—ë„ ~21GB ì‚¬ìš©ë˜ëŠ” ì´ìœ :

| ì»´í¬ë„ŒíŠ¸ | í¬ê¸° | ì–‘ìí™” |
|----------|------|--------|
| Transformer | ~12GB | âœ… 8-bit ì ìš© |
| T5 í…ìŠ¤íŠ¸ ì¸ì½”ë” | ~8GB | âœ… 8-bit ì ìš© |
| VAE | ~1GB | âŒ ì›ë³¸ |
| CLIP | ~1GB | âŒ ì›ë³¸ |

**ë¡œê·¸ì—ì„œ í™•ì¸:**
```
The module 'T5EncoderModel' has been loaded in `bitsandbytes` 8bit
The module 'FluxTransformer2DModel' has been loaded in `bitsandbytes` 8bit
```

---

## ğŸ” ë¬¸ì œ í•´ê²° íˆìŠ¤í† ë¦¬

### ì‹œë„ 1: TorchAO FP8 ì‚¬ì „ ì–‘ìí™” ëª¨ë¸

```
ëª¨ë¸: diffusers/FLUX.1-dev-torchao-fp8
ê²°ê³¼: ì‹¤íŒ¨
ì—ëŸ¬: The size of tensor a (4096) must match the size of tensor b (10240)
ì›ì¸: torchao ë²„ì „ í˜¸í™˜ ë¬¸ì œ
```

### ì‹œë„ 2: device_map="balanced"

```
ê²°ê³¼: ì‹¤íŒ¨
ì—ëŸ¬: You are attempting to perform cpu/disk offload with a pre-quantized torchao model
ì›ì¸: torchao ì–‘ìí™” ëª¨ë¸ì€ CPU offload ë¯¸ì§€ì›
```

### ì‹œë„ 3: device_map="cuda:0"

```
ê²°ê³¼: ì‹¤íŒ¨
ì—ëŸ¬: cuda:0 not supported. Supported strategies are: balanced
ì›ì¸: diffusersì—ì„œ cuda:0 ì§ì ‘ ì§€ì • ë¯¸ì§€ì›
```

### ì‹œë„ 4: TorchAO ì§ì ‘ ì–‘ìí™”

```
ë°©ì‹: Float8WeightOnlyConfig()
ê²°ê³¼: ì‹¤íŒ¨
ë¬¸ì œ: ì–‘ìí™”ê°€ ì ìš©ë˜ì§€ ì•ŠìŒ (ëª¨ë¸ í¬ê¸° ë³€í™” ì—†ìŒ)
```

### ì‹œë„ 5: optimum-quanto FP8

```
ë°©ì‹: quantize(transformer, weights=qfloat8)
ê²°ê³¼: ì´ë¯¸ì§€ ìƒì„± ì„±ê³µ (69ì´ˆ)
ë¬¸ì œ: ì „ì²´ê°€ í”½ì…€ ëª¨ìì´í¬
```

### ì‹œë„ 6: bitsandbytes 8-bit ì‚¬ì „ ì–‘ìí™” âœ…

```
ëª¨ë¸: diffusers/FLUX.1-dev-bnb-8bit
ê²°ê³¼: ì„±ê³µ
ì‹œê°„: 77ì´ˆ/ì´ë¯¸ì§€
í’ˆì§ˆ: ì •ìƒ
```

---

## ğŸ“ ì½”ë“œ êµ¬í˜„

### model_loader.py ê´€ë ¨ ì½”ë“œ

```python
if model_type == "flux-bnb-8bit":
    # ì‚¬ì „ ì–‘ìí™” 8-bit ëª¨ë¸ (diffusers/FLUX.1-dev-bnb-8bit)
    # ê³µì‹ ë¬¸ì„œ: pipe.to("cuda") ì‚¬ìš©
    from diffusers import FluxPipeline

    t2i = FluxPipeline.from_pretrained(
        model_id,
        torch_dtype=self.dtype,
        cache_dir=self.cache_dir
    )
    # bitsandbytes ëª¨ë¸ì€ ìë™ìœ¼ë¡œ GPUì— ë¡œë“œë¨

    # I2I íŒŒì´í”„ë¼ì¸
    try:
        i2i = AutoPipelineForImage2Image.from_pipe(t2i)
    except:
        i2i = t2i
```

---

## âš ï¸ ì£¼ì˜ì‚¬í•­

1. **ë²„ì „ ê³ ì • í•„ìˆ˜**: ìœ„ì— ëª…ì‹œëœ ë²„ì „ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ë©´ í˜¸í™˜ì„± ë¬¸ì œ ë°œìƒ
2. **ìºì‹œ ì •ë¦¬**: ë²„ì „ ë³€ê²½ ì‹œ `uv cache clean && rm -rf .venv && uv sync` í•„ìˆ˜
3. **GPU ë©”ëª¨ë¦¬**: ìµœì†Œ 22GB VRAM í•„ìš” (L4, A10G, A100 ë“±)
4. **CPU offload ë¶ˆê°€**: bitsandbytes ì–‘ìí™” ëª¨ë¸ì€ CPUë¡œ ì´ë™ ë¶ˆê°€

---

## ğŸ“š ì°¸ê³  ìë£Œ

- [diffusers/FLUX.1-dev-bnb-8bit](https://huggingface.co/diffusers/FLUX.1-dev-bnb-8bit)
- [Diffusers bitsandbytes ë¬¸ì„œ](https://huggingface.co/docs/diffusers/en/quantization/bitsandbytes)
- [Diffusers ì–‘ìí™” ê°œìš”](https://huggingface.co/docs/diffusers/quantization/overview)
