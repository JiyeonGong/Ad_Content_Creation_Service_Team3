[project]
name = "ad-content-creation-service-team3"
version = "0.1.0"
description = "FastAPI backend and Streamlit frontend for AI content generation (GPT/SDXL)."
authors = [
    {name = "devuser", email = "devuser@example.com"}
]
readme = "README.md"
requires-python = "==3.12"
license = {text = "MIT"}
keywords = ["fastapi", "streamlit", "openai", "diffusers", "pytorch", "ai"]
classifiers = [
    "Programming Language :: Python :: 3",
    "Operating System :: OS Independent",
]

# ----------------------------------------------------
# 📌 종속성 (Dependencies)
# * 모든 종속성은 문자열 배열(list of strings)로 나열해야 합니다.
# ----------------------------------------------------
dependencies = [
    # 웹 서비스 및 프론트엔드
    "fastapi>=0.110",
    "uvicorn[standard]>=0.27",
    "pydantic>=2.6",
    "streamlit>=1.31",
    "requests>=2.31",
    # AI 모델 및 유틸리티
    "openai>=1.12",
    "diffusers>=0.26",
    "Pillow>=10.2",
    "python-dotenv>=1.0",
    "transformers>=4.57.1",
    "accelerate>=1.11.0",
    "pyyaml>=6.0.3",
    "sentencepiece>=0.2.0",  # FLUX 토크나이저 필수
    "protobuf>=3.20.0",  # sentencepiece 의존성
    "safetensors>=0.4.1",  # FLUX 모델 로딩
    "huggingface-hub>=0.20.0",  # HF 모델 다운로드
    "tokenizers>=0.15.0",  # 토크나이저
    # 최적화 라이브러리
    "bitsandbytes>=0.41.0",  # NF4 양자화 (FLUX 속도 3-4배 개선)
    "torchao>=0.1.0",  # FP8 양자화 (FLUX 품질 우선, 속도 2-2.6배 개선)
    "sageattention>=1.0.0",  # SageAttention2++ (추가 2배 속도 개선)
]

# ----------------------------------------------------
# ⚠️ PyTorch/CUDA 관련 참고 사항
# * GPU 버전 설치는 별도 명령을 사용해야 합니다.
# uv sync로 설치된 torch를 삭제하고 다시 cuda가 반영된 torch를 설치합니다.
# ----------------------------------------------------
# uv pip uninstall torch
# uv pip install torch torchvision --index-url https://download.pytorch.org/whl/cu128