# services.py (ë¦¬íŒ©í† ë§ ë²„ì „)
"""
AI ì„œë¹„ìŠ¤ ë ˆì´ì–´ - ì„¤ì • ê¸°ë°˜ ëª¨ë¸ ê´€ë¦¬
"""
import os
import io
from typing import Optional

from openai import OpenAI
import torch
from PIL import Image
from dotenv import load_dotenv

from .model_registry import get_registry
from .model_loader import ModelLoader

# Load env
load_dotenv(os.path.join(os.path.dirname(__file__), "..", "..", ".env"))

# í† í¬ë‚˜ì´ì € ë³‘ë ¬ ì²˜ë¦¬ ê²½ê³  ì–µì œ
os.environ["TOKENIZERS_PARALLELISM"] = "false"

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
MODEL_GPT_MINI = "gpt-5-mini"

# HF cache location
# GCP: /home/shared ì‚¬ìš© (ì´ë¯¸ ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ ì¬ì‚¬ìš©)
# ë¡œì»¬: project_root/cache/hf_models ì‚¬ìš©
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
if os.path.exists("/home/shared"):
    hf_cache_dir = "/home/shared"
else:
    hf_cache_dir = os.path.join(project_root, "cache", "hf_models")
    os.makedirs(hf_cache_dir, exist_ok=True)

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
openai_client: Optional[OpenAI] = None
model_loader: Optional[ModelLoader] = None
registry = get_registry()

# Initialize OpenAI client
if OPENAI_API_KEY:
    try:
        openai_client = OpenAI(api_key=OPENAI_API_KEY)
    except Exception as e:
        print(f"âš ï¸ OpenAI ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        openai_client = None
else:
    print("âš ï¸ OPENAI_API_KEY ë¯¸ì„¤ì • â€” GPT ê¸°ëŠ¥ ë¶ˆê°€")

# ===========================
# Utility helpers
# ===========================
def align_to_64(x: int) -> int:
    """64ì˜ ë°°ìˆ˜ë¡œ ì •ë ¬ (ìµœì†Œ 64)"""
    try:
        xi = int(x)
    except Exception:
        xi = 64
    return max(64, (xi // 64) * 64)

def ensure_steps(steps: int) -> int:
    try:
        s = int(steps)
    except Exception:
        s = 1
    return max(1, s)

# ===========================
# ëª¨ë¸ ì´ˆê¸°í™”
# ===========================
def init_image_pipelines():
    """
    ì„¤ì • íŒŒì¼ ê¸°ë°˜ìœ¼ë¡œ ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸ ë¡œë“œ
    """
    global model_loader
    
    # ì´ë¯¸ ë¡œë“œëœ ê²½ìš° ìŠ¤í‚µ
    if model_loader and model_loader.is_loaded():
        print("â„¹ï¸ ì´ë¯¸ì§€ íŒŒì´í”„ë¼ì¸ ì´ë¯¸ ë¡œë“œë¨ â€” ìŠ¤í‚µ")
        return
    
    # ModelLoader ìƒì„±
    if model_loader is None:
        model_loader = ModelLoader(cache_dir=hf_cache_dir)
    
    # í´ë°± ì²´ì¸ìœ¼ë¡œ ë¡œë”© ì‹œë„
    success = model_loader.load_with_fallback()
    
    if success:
        info = model_loader.get_current_model_info()
        print(f"âœ… ì´ë¯¸ì§€ ìƒì„± ì¤€ë¹„ ì™„ë£Œ")
        print(f"   ëª¨ë¸: {info['name']} ({info['type']})")
        print(f"   ì¥ì¹˜: {info['device']}")
    else:
        print("âŒ ëª¨ë“  ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ - ì´ë¯¸ì§€ ìƒì„± ë¶ˆê°€")

# ===========================
# í”„ë¡¬í”„íŠ¸ ìµœì í™”
# ===========================
def optimize_prompt(text: str, model_config) -> str:
    """
    í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­ ë° ìµœì í™”
    ëª¨ë¸ë³„ í† í° ì œí•œ ê³ ë ¤
    """
    if not openai_client:
        return text
    
    # í”„ë¡¬í”„íŠ¸ ìµœì í™” ì„¤ì • í™•ì¸
    opt_config = registry.get_prompt_optimization_config()
    if not opt_config.get("enabled", True):
        return text
    
    # í•œê¸€ ë²ˆì—­ ë¹„í™œì„±í™” ì„¤ì • í™•ì¸
    if not opt_config.get("translate_korean", True):
        return text
    
    try:
        # ëª¨ë¸ë³„ ê¸¸ì´ ì œì•½
        max_tokens = model_config.max_tokens if model_config else 77
        
        if max_tokens <= 77:
            constraint = f"Keep it under 60 words (model has {max_tokens} token limit)."
        else:
            constraint = "Keep it concise but descriptive (under 150 words)."
        
        system_prompt = f"""You are a professional prompt engineer for image generation AI.
Translate Korean marketing text to optimized English prompts.
Focus on visual elements, style, mood, and composition.
{constraint}

IMPORTANT - Quality keywords to prevent AI artifacts:

1. If the scene involves people:
   - Hands: "detailed hands, five fingers, natural hand pose, anatomically correct hands, Hand Position Left Right Proper Position, correct thumb direction"
   - Faces: "detailed face, clear facial features, symmetric face, symmetric eyes, natural eye shape"
   - Body: "correct human anatomy, natural body proportions, well-fitted clothing"

2. If the scene involves objects being held or touched:
   - "proper object interaction, object not clipping through body"
   - "realistic grip, natural holding pose"
   - "physically accurate, no overlapping body parts with objects"

3. If the scene involves fitness/gym/sports equipment:
   - "equipment not penetrating body, proper form"
   - "hands gripping equipment correctly, realistic weight interaction"

Always include relevant keywords based on the scene content.

Output ONLY the English prompt, no explanations."""

        resp = openai_client.responses.create(
            model=MODEL_GPT_MINI,
            input=f"Convert to image prompt:\n{text}",
            reasoning={"effort": "minimal"},
            max_output_tokens=200,
        )
        
        result = getattr(resp, "output_text", None) or str(resp)
        optimized = result.strip()
        print(f"ğŸ”„ í”„ë¡¬í”„íŠ¸ ìµœì í™”:\n  ì›ë³¸: {text}\n  ìµœì í™”: {optimized}")
        return optimized
        
    except Exception as e:
        print(f"âš ï¸ í”„ë¡¬í”„íŠ¸ ìµœì í™” ì‹¤íŒ¨, ì›ë³¸ ì‚¬ìš©: {e}")
        return text

# ===========================
# GPT-5 Mini: ë¬¸êµ¬ ìƒì„±
# ===========================
def generate_caption_core(info: dict, tone: str) -> str:
    if not openai_client:
        raise RuntimeError("OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    prompt = f"""
ë‹¹ì‹ ì€ í—¬ìŠ¤ì¼€ì–´ ì†Œìƒê³µì¸ì„ ìœ„í•œ ì „ë¬¸ ì¸ìŠ¤íƒ€ê·¸ë¨ ì½˜í…ì¸  í¬ë¦¬ì—ì´í„°ì…ë‹ˆë‹¤.
ì•„ë˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¸ìŠ¤íƒ€ê·¸ë¨ ê²Œì‹œë¬¼ì— ìµœì í™”ëœ ì½˜í…ì¸ ë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”.

ìš”ì²­:
1. ì¸ìŠ¤íƒ€ê·¸ë¨ í™ë³´ ë¬¸êµ¬ 3ê°œ ì‘ì„±
    - ê° ë¬¸êµ¬: í›„í‚¹ â†’ í•µì‹¬ ë©”ì‹œì§€ â†’ CTA
    - ì´ëª¨í‹°ì½˜ ì‚¬ìš©
    - ë¬¸ì²´ ìŠ¤íƒ€ì¼: {tone}
2. í•´ì‹œíƒœê·¸ 15ê°œ ì¶”ì²œ (ì¤‘ë³µ ì œê±°)

[ì •ë³´]
ì„œë¹„ìŠ¤ ì¢…ë¥˜: {info.get('service_type')}
ì„œë¹„ìŠ¤ëª…: {info.get('service_name')}
í•µì‹¬ íŠ¹ì§•: {info.get('features')}
ì§€ì—­: {info.get('location')}

ì¶œë ¥ í˜•ì‹:
ë¬¸êµ¬:
1. [ë¬¸êµ¬1]
2. [ë¬¸êµ¬2]
3. [ë¬¸êµ¬3]

í•´ì‹œíƒœê·¸:
#[íƒœê·¸1] #[íƒœê·¸2] ... #[íƒœê·¸N]
"""
    try:
        resp = openai_client.responses.create(
            model=MODEL_GPT_MINI,
            input=prompt,
            reasoning={"effort": "minimal"},
            max_output_tokens=512,
        )
        text = getattr(resp, "output_text", None) or str(resp)
        return text.strip()
    except Exception as e:
        print(f"ğŸš¨ GPT í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        raise

# ===========================
# ğŸ†• ì´ë¯¸ì§€ ìƒì„± (T2I) - ComfyUI ê¸°ë°˜
# ===========================
def generate_t2i_core(
    prompt: str,
    width: int,
    height: int,
    steps: int,
    guidance_scale: float = None,
    enable_adetailer: bool = True,
    adetailer_targets: list = None,
    post_process_method: str = "none"  # "none", "impact_pack", "adetailer"
) -> bytes:
    """
    ComfyUIë¥¼ ì‚¬ìš©í•œ T2I ì´ë¯¸ì§€ ìƒì„±

    Args:
        post_process_method: í›„ì²˜ë¦¬ ë°©ì‹
            - "none": í›„ì²˜ë¦¬ ì—†ìŒ
            - "impact_pack": ComfyUI Impact Pack (YOLO+SAM)
            - "adetailer": ê¸°ì¡´ ADetailer (YOLO+MediaPipe)
    """
    from .comfyui_client import ComfyUIClient
    from .comfyui_workflows import (
        get_flux_t2i_workflow,
        get_flux_t2i_with_impact_workflow,
        update_flux_t2i_workflow,
        load_image_editing_config
    )

    # í˜„ì¬ ë¡œë“œëœ ëª¨ë¸ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
    global model_loader
    if not model_loader or not model_loader.is_loaded():
        raise RuntimeError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ëª¨ë¸ì„ ë¡œë“œí•˜ì„¸ìš”.")

    current_model_name = model_loader.current_model_name
    model_config = model_loader.current_model_config

    # í”„ë¡¬í”„íŠ¸ ìµœì í™”
    optimized_prompt = optimize_prompt(prompt, model_config)

    # Steps ê²€ì¦
    if steps < 1:
        steps = model_config.default_steps
    steps = min(steps, model_config.max_steps)

    # Guidance scale ì„¤ì •
    if guidance_scale is None:
        guidance_scale = model_config.guidance_scale or 3.5

    print(f"ğŸ¨ ComfyUIë¡œ T2I ì´ë¯¸ì§€ ìƒì„± ì¤‘")
    print(f"   ëª¨ë¸: {current_model_name}")
    print(f"   í›„ì²˜ë¦¬: {post_process_method}")
    print(f"   Steps: {steps}")
    print(f"   í¬ê¸°: {width}x{height}")
    print(f"   Guidance: {guidance_scale}")

    try:
        # ComfyUI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        config = load_image_editing_config()
        comfyui_config = config.get("comfyui", {})
        base_url = comfyui_config.get("base_url", "http://localhost:8188")
        timeout = comfyui_config.get("timeout", 300)

        client = ComfyUIClient(base_url=base_url, timeout=timeout)

        # ì›Œí¬í”Œë¡œìš° ì„ íƒ
        if post_process_method == "impact_pack":
            workflow = get_flux_t2i_with_impact_workflow()
        else:
            workflow = get_flux_t2i_workflow()

        # ì›Œí¬í”Œë¡œìš° íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        workflow = update_flux_t2i_workflow(
            workflow=workflow,
            model_name=current_model_name,
            prompt=optimized_prompt,
            width=width,
            height=height,
            steps=steps,
            guidance_scale=guidance_scale
        )

        # ComfyUI ì‹¤í–‰
        output_images, history = client.execute_workflow(workflow=workflow)

        if not output_images:
            raise Exception("ì¶œë ¥ ì´ë¯¸ì§€ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        image_bytes = output_images[0]

        # ê¸°ì¡´ ADetailer í›„ì²˜ë¦¬ (ì„ íƒ ì‹œ)
        if post_process_method == "adetailer" and enable_adetailer:
            image = Image.open(io.BytesIO(image_bytes))
            image = apply_adetailer(
                image=image,
                prompt=optimized_prompt,
                targets=adetailer_targets or ["hand"]
            )

            buf = io.BytesIO()
            image.save(buf, format="PNG")
            image_bytes = buf.getvalue()

        print(f"âœ… ìƒì„± ì™„ë£Œ: {len(image_bytes)} bytes")
        return image_bytes

    except Exception as e:
        print(f"âŒ ComfyUI T2I ìƒì„± ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        raise RuntimeError(f"ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")


# ===========================
# ADetailer í›„ì²˜ë¦¬
# ===========================
def apply_adetailer(
    image: Image,
    prompt: str,
    targets: list = None,
    strength: float = 0.4
) -> Image:
    """
    ADetailer ìŠ¤íƒ€ì¼ í›„ì²˜ë¦¬
    - ì†/ì–¼êµ´ ê°ì§€ í›„ í•´ë‹¹ ì˜ì—­ë§Œ Inpaintë¡œ ì¬ìƒì„±
    """
    global model_loader

    if targets is None:
        targets = ["hand"]

    try:
        from .post_processor import get_post_processor

        print(f"ğŸ”§ ADetailer í›„ì²˜ë¦¬ ì‹œì‘ (targets: {targets})")

        post_processor = get_post_processor()

        # I2I íŒŒì´í”„ë¼ì¸ì„ Inpaintìš©ìœ¼ë¡œ ì‚¬ìš©
        inpaint_pipe = model_loader.i2i_pipe

        processed_image, info = post_processor.full_pipeline(
            image=image,
            inpaint_pipeline=inpaint_pipe,
            prompt=prompt,
            auto_detect=True,
            adetailer_targets=targets,
            adetailer_strength=strength
        )

        if not info["processed"]:
            print(f"â„¹ï¸ ADetailer: ì´ìƒ ì—†ìŒ, ì›ë³¸ ìœ ì§€")

        return processed_image

    except Exception as e:
        print(f"âš ï¸ ADetailer ì‹¤íŒ¨, ì›ë³¸ ë°˜í™˜: {e}")
        return image

# ===========================
# ğŸ†• ì´ë¯¸ì§€ í¸ì§‘ (I2I) - ComfyUI ê¸°ë°˜
# ===========================
def generate_i2i_core(
    input_image_bytes: bytes,
    prompt: str,
    strength: float,
    width: int,
    height: int,
    steps: int,
    guidance_scale: float = None,
    enable_adetailer: bool = False,
    adetailer_targets: list = None,
    post_process_method: str = "none"  # "none", "impact_pack", "adetailer"
) -> bytes:
    """
    ComfyUIë¥¼ ì‚¬ìš©í•œ I2I ì´ë¯¸ì§€ í¸ì§‘

    Args:
        input_image_bytes: ì…ë ¥ ì´ë¯¸ì§€ ë°”ì´íŠ¸
        prompt: í¸ì§‘ í”„ë¡¬í”„íŠ¸
        strength: í¸ì§‘ ê°•ë„ (0.0~1.0)
        width, height: ì¶œë ¥ í¬ê¸°
        steps: ìƒ˜í”Œë§ ìŠ¤í…
        guidance_scale: CFG ìŠ¤ì¼€ì¼
        enable_adetailer: ADetailer í™œì„±í™” ì—¬ë¶€ (legacy)
        adetailer_targets: í›„ì²˜ë¦¬ íƒ€ê²Ÿ
        post_process_method: í›„ì²˜ë¦¬ ë°©ì‹
            - "none": í›„ì²˜ë¦¬ ì—†ìŒ
            - "impact_pack": ComfyUI Impact Pack (YOLO+SAM)
            - "adetailer": ê¸°ì¡´ ADetailer (YOLO+MediaPipe)
    """
    from .comfyui_client import ComfyUIClient
    from .comfyui_workflows import (
        get_flux_i2i_workflow,
        update_flux_i2i_workflow,
        load_image_editing_config
    )

    # í˜„ì¬ ë¡œë“œëœ ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    global model_loader
    if not model_loader or not model_loader.is_loaded():
        raise RuntimeError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ëª¨ë¸ì„ ë¡œë“œí•˜ì„¸ìš”.")

    current_model_name = model_loader.current_model_name
    model_config = model_loader.current_model_config

    # I2I ì§€ì› í™•ì¸
    if not model_config.supports_i2i:
        raise RuntimeError(f"í˜„ì¬ ëª¨ë¸({current_model_name})ì€ I2Ië¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    # í”„ë¡¬í”„íŠ¸ ìµœì í™”
    optimized_prompt = optimize_prompt(prompt, model_config)

    # Steps ê²€ì¦
    if steps < 1:
        steps = model_config.default_steps
    steps = min(steps, model_config.max_steps)

    # Guidance scale ì„¤ì •
    if guidance_scale is None:
        guidance_scale = model_config.guidance_scale or 3.5

    print(f"âœï¸ ComfyUIë¡œ I2I ì´ë¯¸ì§€ í¸ì§‘ ì¤‘")
    print(f"   ëª¨ë¸: {current_model_name}")
    print(f"   í›„ì²˜ë¦¬: {post_process_method}")
    print(f"   Strength: {strength}")
    print(f"   Steps: {steps}")
    print(f"   í¬ê¸°: {width}x{height}")
    print(f"   Guidance: {guidance_scale}")

    try:
        # ComfyUI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        config = load_image_editing_config()
        comfyui_config = config.get("comfyui", {})
        base_url = comfyui_config.get("base_url", "http://localhost:8188")
        timeout = comfyui_config.get("timeout", 300)

        client = ComfyUIClient(base_url=base_url, timeout=timeout)

        # I2I ì›Œí¬í”Œë¡œìš° ê°€ì ¸ì˜¤ê¸°
        workflow = get_flux_i2i_workflow()

        # ì›Œí¬í”Œë¡œìš° íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        workflow = update_flux_i2i_workflow(
            workflow=workflow,
            model_name=current_model_name,
            prompt=optimized_prompt,
            strength=strength,
            steps=steps,
            guidance_scale=guidance_scale
        )

        # ComfyUI ì‹¤í–‰ (ì…ë ¥ ì´ë¯¸ì§€ í¬í•¨)
        output_images, history = client.execute_workflow(
            workflow=workflow,
            input_image=input_image_bytes,
            input_image_node_id="11"  # LoadImage ë…¸ë“œ ID
        )

        if not output_images:
            raise Exception("ì¶œë ¥ ì´ë¯¸ì§€ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        image_bytes = output_images[0]

        # ê¸°ì¡´ ADetailer í›„ì²˜ë¦¬ (ì„ íƒ ì‹œ)
        if post_process_method == "adetailer" and enable_adetailer:
            image = Image.open(io.BytesIO(image_bytes))
            image = apply_adetailer(
                image=image,
                prompt=optimized_prompt,
                targets=adetailer_targets or ["hand"]
            )

            buf = io.BytesIO()
            image.save(buf, format="PNG")
            image_bytes = buf.getvalue()

        print(f"âœ… í¸ì§‘ ì™„ë£Œ: {len(image_bytes)} bytes")
        return image_bytes

    except Exception as e:
        print(f"âŒ ComfyUI I2I í¸ì§‘ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        raise RuntimeError(f"ì´ë¯¸ì§€ í¸ì§‘ ì‹¤íŒ¨: {e}")

# ===========================
# ëª¨ë¸ ì „í™˜
# ===========================
def switch_model(model_name: str) -> dict:
    """
    ë‹¤ë¥¸ ëª¨ë¸ë¡œ ì „í™˜
    Returns: {"success": bool, "message": str, "model_info": dict}
    """
    global model_loader

    if not model_loader:
        model_loader = ModelLoader(cache_dir=hf_cache_dir)

    # ëª¨ë¸ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    model_config = registry.get_model(model_name)
    if not model_config:
        return {
            "success": False,
            "message": f"ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸: {model_name}",
            "model_info": None
        }

    # ì´ë¯¸ ë¡œë“œëœ ê²½ìš°
    if model_loader.is_loaded() and model_loader.current_model_name == model_name:
        return {
            "success": True,
            "message": f"ëª¨ë¸ '{model_name}' ì´ë¯¸ ë¡œë“œë¨",
            "model_info": model_loader.get_current_model_info()
        }

    # ëª¨ë¸ ë¡œë“œ
    print(f"ğŸ”„ ëª¨ë¸ ì „í™˜ ì¤‘: {model_name}")
    success = model_loader.load_model(model_name)

    if success:
        info = model_loader.get_current_model_info()
        return {
            "success": True,
            "message": f"ëª¨ë¸ '{model_name}' ë¡œë“œ ì„±ê³µ",
            "model_info": info
        }
    else:
        return {
            "success": False,
            "message": f"ëª¨ë¸ '{model_name}' ë¡œë“œ ì‹¤íŒ¨",
            "model_info": None
        }

# ===========================
# ìƒíƒœ ì¡°íšŒ
# ===========================
def get_service_status() -> dict:
    """ì„œë¹„ìŠ¤ ìƒíƒœ ë°˜í™˜"""
    status = {
        "gpt_ready": openai_client is not None,
        "image_ready": model_loader and model_loader.is_loaded(),
    }

    if model_loader:
        status.update(model_loader.get_current_model_info())

    return status

def unload_model_service() -> dict:
    """ëª¨ë¸ ì–¸ë¡œë“œ"""
    global model_loader

    if model_loader and model_loader.is_loaded():
        model_loader.unload_model()
        return {"success": True, "message": "ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ"}

    return {"success": True, "message": "ì´ë¯¸ ì–¸ë¡œë“œ ìƒíƒœì…ë‹ˆë‹¤."}

# ===========================
# ğŸ†• ì´ë¯¸ì§€ í¸ì§‘ (ComfyUI)
# ===========================
def edit_image_with_comfyui(
    experiment_id: str,
    input_image_bytes: bytes,
    prompt: str,
    steps: int = None,
    guidance_scale: float = None,
    strength: float = None
) -> dict:
    """
    ComfyUIë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ í¸ì§‘ (BEN2 + ì„ íƒ ëª¨ë¸)

    Args:
        experiment_id: ì‹¤í—˜ ID ("ben2_flux_fill" ë˜ëŠ” "ben2_qwen_image")
        input_image_bytes: ì…ë ¥ ì´ë¯¸ì§€ ë°”ì´íŠ¸
        prompt: í¸ì§‘ í”„ë¡¬í”„íŠ¸
        steps: ì¶”ë¡  ë‹¨ê³„
        guidance_scale: Guidance scale
        strength: ë³€í™” ê°•ë„

    Returns:
        {
            "success": bool,
            "experiment_id": str,
            "experiment_name": str,
            "output_image_base64": str,
            "background_removed_image_base64": str,
            "error": str,
            "elapsed_time": float
        }
    """
    import base64
    import time
    from .comfyui_client import ComfyUIClient
    from .comfyui_workflows import (
        get_workflow_template,
        update_workflow_inputs,
        get_workflow_input_image_node_id,
        load_image_editing_config
    )

    start_time = time.time()

    try:
        # ì„¤ì • ë¡œë“œ
        config = load_image_editing_config()

        # ì‹¤í—˜ ì •ë³´ ì°¾ê¸°
        experiment = None
        for exp in config.get("experiments", []):
            if exp["id"] == experiment_id:
                experiment = exp
                break

        if not experiment:
            return {
                "success": False,
                "experiment_id": experiment_id,
                "experiment_name": "Unknown",
                "output_image_base64": None,
                "background_removed_image_base64": None,
                "error": f"ì•Œ ìˆ˜ ì—†ëŠ” ì‹¤í—˜ ID: {experiment_id}",
                "elapsed_time": None
            }

        # ComfyUI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        comfyui_config = config.get("comfyui", {})
        base_url = comfyui_config.get("base_url", "http://localhost:8188")
        timeout = comfyui_config.get("timeout", 300)

        client = ComfyUIClient(base_url=base_url, timeout=timeout)

        # ì›Œí¬í”Œë¡œìš° í…œí”Œë¦¿ ê°€ì ¸ì˜¤ê¸°
        workflow = get_workflow_template(experiment_id)

        # ì›Œí¬í”Œë¡œìš° ì—…ë°ì´íŠ¸ (ì‚¬ìš©ì ì…ë ¥ ë°˜ì˜)
        workflow = update_workflow_inputs(
            workflow=workflow,
            experiment_id=experiment_id,
            prompt=prompt,
            steps=steps,
            guidance_scale=guidance_scale,
            strength=strength
        )

        # ì…ë ¥ ì´ë¯¸ì§€ ë…¸ë“œ ID
        input_node_id = get_workflow_input_image_node_id(experiment_id)

        print(f"ğŸ¨ ComfyUI ì´ë¯¸ì§€ í¸ì§‘ ì‹œì‘")
        print(f"   ì‹¤í—˜: {experiment['name']}")
        print(f"   í”„ë¡¬í”„íŠ¸: {prompt}")

        # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        output_images, history = client.execute_workflow(
            workflow=workflow,
            input_image=input_image_bytes,
            input_image_node_id=input_node_id
        )

        if not output_images:
            raise Exception("ì¶œë ¥ ì´ë¯¸ì§€ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ë¥¼ ìµœì¢… ê²°ê³¼ë¡œ ì‚¬ìš©
        output_image_bytes = output_images[0]
        output_image_base64 = base64.b64encode(output_image_bytes).decode("utf-8")

        # ë°°ê²½ ì œê±° ì´ë¯¸ì§€ (ì„ íƒì )
        background_removed_base64 = None
        if len(output_images) > 1:
            background_removed_base64 = base64.b64encode(output_images[1]).decode("utf-8")

        elapsed_time = time.time() - start_time

        print(f"âœ… ComfyUI í¸ì§‘ ì™„ë£Œ! (ì†Œìš” ì‹œê°„: {elapsed_time:.1f}ì´ˆ)")

        return {
            "success": True,
            "experiment_id": experiment_id,
            "experiment_name": experiment["name"],
            "output_image_base64": output_image_base64,
            "background_removed_image_base64": background_removed_base64,
            "error": None,
            "elapsed_time": elapsed_time
        }

    except Exception as e:
        elapsed_time = time.time() - start_time
        error_msg = str(e)
        print(f"âŒ ComfyUI í¸ì§‘ ì‹¤íŒ¨: {error_msg}")

        return {
            "success": False,
            "experiment_id": experiment_id,
            "experiment_name": experiment.get("name", "Unknown") if experiment else "Unknown",
            "output_image_base64": None,
            "background_removed_image_base64": None,
            "error": error_msg,
            "elapsed_time": elapsed_time
        }


def get_image_editing_experiments() -> dict:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ì´ë¯¸ì§€ í¸ì§‘ ì‹¤í—˜ ëª©ë¡ ë°˜í™˜"""
    from .comfyui_workflows import load_image_editing_config

    try:
        config = load_image_editing_config()
        experiments = config.get("experiments", [])

        return {
            "success": True,
            "experiments": [
                {
                    "id": exp["id"],
                    "name": exp["name"],
                    "description": exp["description"],
                    "background_removal_model": exp["background_removal"]["model"],
                    "editing_model": exp["image_editing"]["model"]
                }
                for exp in experiments
            ]
        }

    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "experiments": []
        }


def check_comfyui_status() -> dict:
    """ComfyUI ì„œë²„ ìƒíƒœ í™•ì¸"""
    from .comfyui_client import ComfyUIClient
    from .comfyui_workflows import load_image_editing_config

    try:
        config = load_image_editing_config()
        comfyui_config = config.get("comfyui", {})
        base_url = comfyui_config.get("base_url", "http://localhost:8188")

        client = ComfyUIClient(base_url=base_url)
        connected = client.check_connection()

        if connected:
            queue_info = client.get_queue_info()
            return {
                "connected": True,
                "base_url": base_url,
                "queue_info": queue_info
            }
        else:
            return {
                "connected": False,
                "base_url": base_url,
                "error": "ComfyUI ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            }

    except Exception as e:
        return {
            "connected": False,
            "error": str(e)
        }
