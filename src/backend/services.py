# # src/backend/services.py
# # ============================================================
# # âš™ï¸ AI í•µì‹¬ ì¶”ë¡  ì—”ì§„ (Web-Independent)
# # - GPT-5 Mini í˜¸ì¶œ (OpenAI API)
# # - SDXL T2I/I2I ë¡œì»¬ ì¶”ë¡  (Diffusers)
# # - ì´ íŒŒì¼ì€ FastAPIì— ì˜ì¡´í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
# # ============================================================

# import os
# import io
# import base64
# from typing import Dict, Any, List

# # AI ëª¨ë¸ ë° ì´ë¯¸ì§€ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
# from openai import OpenAI
# from diffusers import StableDiffusionXLPipeline, StableDiffusionXLImg2ImgPipeline
# import torch
# from PIL import Image
# from dotenv import load_dotenv

# # ============================================================
# # ğŸŒ í™˜ê²½ ì„¤ì • ë° í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
# # ============================================================

# # .env íŒŒì¼ ë¡œë”© (í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€)
# load_dotenv(os.path.join(os.path.dirname(__file__), "..", "..", ".env"))

# OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
# MODEL_GPT_MINI = "gpt-5-mini"

# # Hugging Face ëª¨ë¸ ìºì‹œ ê²½ë¡œ ì„¤ì •
# project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
# hf_cache_dir = os.path.join(project_root, "cache", "hf_models")
# os.makedirs(hf_cache_dir, exist_ok=True)

# # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
# openai_client = None
# if OPENAI_API_KEY:
#     try:
#         openai_client = OpenAI(api_key=OPENAI_API_KEY)
#     except Exception as e:
#         print(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
# else:
#     print("âš ï¸ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ GPT ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# # SDXL ëª¨ë¸ ë³€ìˆ˜ (ì´ˆê¸°í™”ëŠ” init í•¨ìˆ˜ì—ì„œ ìˆ˜í–‰)
# T2I_PIPE = None
# I2I_PIPE = None
# DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
# MODEL_ID = "stabilityai/stable-diffusion-xl-base-1.0"

# def init_sdxl_pipelines():
#     """SDXL T2I ë° I2I íŒŒì´í”„ë¼ì¸ì„ ì´ˆê¸°í™”í•˜ê³  ì „ì—­ ë³€ìˆ˜ì— ì €ì¥í•©ë‹ˆë‹¤."""
#     global T2I_PIPE, I2I_PIPE
#     try:
#         print(f"SDXL ëª¨ë¸ ë¡œë”© ì¤‘... (Device: {DEVICE}, Cache: {hf_cache_dir})")
#         dtype = torch.float16 if torch.cuda.is_available() else torch.float32

#         # T2I (Text-to-Image) íŒŒì´í”„ë¼ì¸
#         T2I_PIPE = StableDiffusionXLPipeline.from_pretrained(
#             MODEL_ID,
#             cache_dir=hf_cache_dir,
#             torch_dtype=dtype
#         ).to(DEVICE)
        
#         # I2I (Image-to-Image) íŒŒì´í”„ë¼ì¸
#         I2I_PIPE = StableDiffusionXLImg2ImgPipeline.from_pretrained(
#             MODEL_ID,
#             cache_dir=hf_cache_dir,
#             torch_dtype=dtype
#         ).to(DEVICE)
        
#         print("SDXL ëª¨ë¸ ë¡œë”© ì™„ë£Œ.")
#     except Exception as e:
#         print(f"SDXL ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
#         T2I_PIPE = None
#         I2I_PIPE = None

# # ============================================================
# # ğŸ¯ AI ì¶”ë¡  í•¨ìˆ˜ (í•µì‹¬ ë¡œì§)
# # ============================================================

# def generate_caption_core(info: dict, tone: str) -> str:
#     """GPT-5 Minië¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸êµ¬ ë° í•´ì‹œíƒœê·¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
#     if not openai_client:
#         raise ValueError("OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
#     prompt = f"""
# ë‹¹ì‹ ì€ í—¬ìŠ¤ì¼€ì–´ ì†Œìƒê³µì¸ì„ ìœ„í•œ ì „ë¬¸ ì¸ìŠ¤íƒ€ê·¸ë¨ ì½˜í…ì¸  í¬ë¦¬ì—ì´í„°ì…ë‹ˆë‹¤.
# ì•„ë˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¸ìŠ¤íƒ€ê·¸ë¨ ê²Œì‹œë¬¼ì— ìµœì í™”ëœ ì½˜í…ì¸ ë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”.

# ìš”ì²­:
# 1. ì¸ìŠ¤íƒ€ê·¸ë¨ í™ë³´ ë¬¸êµ¬ 3ê°œ ì‘ì„±
#     - ê° ë¬¸êµ¬: í›„í‚¹ â†’ í•µì‹¬ ë©”ì‹œì§€ â†’ CTA
#     - ì´ëª¨í‹°ì½˜ ì‚¬ìš©
#     - ë¬¸ì²´ ìŠ¤íƒ€ì¼: {tone}
# 2. í•´ì‹œíƒœê·¸ 15ê°œ ì¶”ì²œ (ì¤‘ë³µ ì œê±°)

# [ì •ë³´]
# ì„œë¹„ìŠ¤ ì¢…ë¥˜: {info['service_type']}
# ì„œë¹„ìŠ¤ëª…: {info['service_name']}
# í•µì‹¬ íŠ¹ì§•: {info['features']}
# ì§€ì—­: {info['location']}
# ì´ë²¤íŠ¸: ì—†ìŒ

# ì¶œë ¥ í˜•ì‹:
# ë¬¸êµ¬:
# 1. [ë¬¸êµ¬1]
# 2. [ë¬¸êµ¬2]
# 3. [ë¬¸êµ¬3]

# í•´ì‹œíƒœê·¸:
# #[íƒœê·¸1] #[íƒœê·¸2] ... #[íƒœê·¸N]
# """
#     try:
#         response = openai_client.responses.create(
#             model=MODEL_GPT_MINI,
#             input=prompt,
#             reasoning={"effort":"minimal"},
#             max_output_tokens=512, 
#         )
#         return response.output_text.strip()
#     except Exception as e:
#         print(f"GPT-5 Mini í˜¸ì¶œ ì˜¤ë¥˜: {e}")
#         raise

# def generate_t2i_core(prompt: str, width: int, height: int, steps: int) -> bytes:
#     """SDXL T2Ië¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ê³  PNG ë°”ì´íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
#     if not T2I_PIPE:
#         raise ValueError("T2I íŒŒì´í”„ë¼ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

#     negative_prompt = "low quality, blurry, text, watermark, distorted, ugly, tiling, poorly drawn"
    
#     result = T2I_PIPE(
#         prompt=prompt, 
#         negative_prompt=negative_prompt, 
#         width=width, 
#         height=height, 
#         num_inference_steps=steps,
#         guidance_scale=7.5 # ê¸°ë³¸ê°’ ì‚¬ìš©
#     )
#     image = result.images[0]
    
#     # BytesIOë¡œ ë³€í™˜ í›„ ë°˜í™˜
#     buf = io.BytesIO()
#     image.save(buf, format="PNG")
#     return buf.getvalue()

# def generate_i2i_core(input_image_bytes: bytes, prompt: str, strength: float, width: int, height: int, steps: int) -> bytes:
#     """SDXL I2Ië¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ í¸ì§‘/í•©ì„±í•˜ê³  PNG ë°”ì´íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
#     if not I2I_PIPE:
#         raise ValueError("I2I íŒŒì´í”„ë¼ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

#     negative_prompt = "low quality, blurry, text, watermark, distorted, ugly, tiling, poorly drawn"
    
#     # 1. ì›ë³¸ ì´ë¯¸ì§€ ë¡œë“œ ë° ë¦¬ì‚¬ì´ì¦ˆ
#     input_image = Image.open(io.BytesIO(input_image_bytes)).convert("RGB").resize((width, height))
    
#     # 2. I2I íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
#     result = I2I_PIPE(
#         prompt=prompt, 
#         image=input_image,
#         strength=strength,
#         negative_prompt=negative_prompt, 
#         num_inference_steps=steps,
#         guidance_scale=7.5
#     )
#     image = result.images[0]
    
#     # 3. BytesIOë¡œ ë³€í™˜ í›„ ë°˜í™˜
#     buf = io.BytesIO()
#     image.save(buf, format="PNG")
#     return buf.getvalue()


















# services.py (ì•ˆì •í™” ë²„ì „ - SDXL í´ë°± í¬í•¨)
import os
import io
import traceback
import base64
from typing import Optional

from openai import OpenAI
from diffusers import (
    StableDiffusionXLPipeline, 
    StableDiffusionXLImg2ImgPipeline,
    DiffusionPipeline,
    AutoPipelineForText2Image,
    AutoPipelineForImage2Image
)
import torch
from PIL import Image
from dotenv import load_dotenv

# Load env
load_dotenv(os.path.join(os.path.dirname(__file__), "..", "..", ".env"))

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
MODEL_GPT_MINI = "gpt-5-mini"

# ğŸ†• ëª¨ë¸ ìš°ì„ ìˆœìœ„ ì„¤ì • (í™˜ê²½ë³€ìˆ˜ë¡œ ì œì–´ ê°€ëŠ¥)
PRIMARY_MODEL = os.getenv("IMAGE_MODEL_ID", "black-forest-labs/FLUX.1-schnell")
FALLBACK_MODEL = "stabilityai/stable-diffusion-xl-base-1.0"
USE_FALLBACK = os.getenv("USE_SDXL_FALLBACK", "true").lower() == "true"

# HF cache location
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
hf_cache_dir = os.path.join(project_root, "cache", "hf_models")
os.makedirs(hf_cache_dir, exist_ok=True)

# Globals
openai_client: Optional[OpenAI] = None
T2I_PIPE = None
I2I_PIPE = None
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
CURRENT_MODEL = None  # ì‹¤ì œ ë¡œë“œëœ ëª¨ë¸ ì¶”ì 

# Initialize OpenAI client
if OPENAI_API_KEY:
    try:
        openai_client = OpenAI(api_key=OPENAI_API_KEY)
    except Exception as e:
        print(f"âš ï¸ OpenAI ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        openai_client = None
else:
    print("âš ï¸ OPENAI_API_KEY ë¯¸ì„¤ì • â€” GPT ê¸°ëŠ¥ ë¶ˆê°€")

# ===========================
# Utility helpers
# ===========================
def align_to_64(x: int) -> int:
    """64ì˜ ë°°ìˆ˜ë¡œ ì •ë ¬ (ìµœì†Œ 64)"""
    try:
        xi = int(x)
    except Exception:
        xi = 64
    return max(64, (xi // 64) * 64)

def ensure_steps(steps: int) -> int:
    try:
        s = int(steps)
    except Exception:
        s = 1
    return max(1, s)

# ===========================
# ëª¨ë¸ë³„ ì¶”ë¡  íŒŒë¼ë¯¸í„°
# ===========================
def get_model_params(model_id: str):
    """ëª¨ë¸ë³„ ìµœì  íŒŒë¼ë¯¸í„° ë°˜í™˜"""
    if "FLUX" in model_id.upper():
        return {
            "default_steps": 4,
            "use_negative_prompt": False,
            "guidance_scale": None,
            "supports_i2i": True
        }
    else:  # SDXL
        return {
            "default_steps": 30,
            "use_negative_prompt": True,
            "guidance_scale": 7.5,
            "supports_i2i": True
        }

# ===========================
# ğŸ†• ëª¨ë¸ ì´ˆê¸°í™” (ì•ˆì •í™” + í´ë°±)
# ===========================
def init_image_pipelines():
    """
    ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    1. FLUX ì‹œë„ (ì„±ê³µ ì‹œ ì¢…ë£Œ)
    2. ì‹¤íŒ¨ ì‹œ SDXLë¡œ í´ë°±
    3. ì´ë¯¸ ë¡œë“œëœ ê²½ìš° ìŠ¤í‚µ
    """
    global T2I_PIPE, I2I_PIPE, DEVICE, CURRENT_MODEL

    # ì´ë¯¸ ë¡œë“œë˜ì—ˆìœ¼ë©´ ìŠ¤í‚µ
    if T2I_PIPE is not None:
        print(f"â„¹ï¸ ì´ë¯¸ì§€ íŒŒì´í”„ë¼ì¸ ì´ë¯¸ ë¡œë“œë¨ (ëª¨ë¸: {CURRENT_MODEL}) â€” ìŠ¤í‚µ")
        return

    print(f"ğŸ“¦ ì´ë¯¸ì§€ ëª¨ë¸ ë¡œë”© ì‹œì‘ (Device={DEVICE})")
    dtype = torch.float16 if DEVICE == "cuda" else torch.float32

    # 1ë‹¨ê³„: PRIMARY ëª¨ë¸ ì‹œë„ (FLUX)
    try:
        print(f"ğŸ”„ 1ì°¨ ì‹œë„: {PRIMARY_MODEL} ë¡œë”© ì¤‘...")
        
        T2I_PIPE = DiffusionPipeline.from_pretrained(
            PRIMARY_MODEL,
            cache_dir=hf_cache_dir,
            torch_dtype=dtype,
        ).to(DEVICE)
        
        # I2I íŒŒì´í”„ë¼ì¸ ìƒì„±
        try:
            I2I_PIPE = AutoPipelineForImage2Image.from_pipe(T2I_PIPE)
        except:
            I2I_PIPE = T2I_PIPE  # í´ë°±
        
        CURRENT_MODEL = PRIMARY_MODEL
        print(f"âœ… {PRIMARY_MODEL} ë¡œë”© ì„±ê³µ!")
        return  # ì„±ê³µ ì‹œ ì¢…ë£Œ
        
    except Exception as e:
        error_msg = str(e).lower()
        print(f"âš ï¸ {PRIMARY_MODEL} ë¡œë”© ì‹¤íŒ¨: {e}")
        
        # HF ì¸ì¦ í•„ìš” ì—ëŸ¬ì¸ì§€ í™•ì¸
        if "401" in error_msg or "authentication" in error_msg or "gated" in error_msg:
            print("â— Hugging Face ì¸ì¦ì´ í•„ìš”í•œ ëª¨ë¸ì…ë‹ˆë‹¤.")
            print("í•´ê²° ë°©ë²•:")
            print("1. https://huggingface.co/black-forest-labs/FLUX.1-schnell ë°©ë¬¸")
            print("2. 'Agree and access repository' í´ë¦­")
            print("3. HF í† í° ìƒì„±: https://huggingface.co/settings/tokens")
            print("4. í„°ë¯¸ë„ì—ì„œ: huggingface-cli login")
        
        # í´ë°± ì‹œë„
        if not USE_FALLBACK:
            print("âŒ í´ë°±ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. USE_SDXL_FALLBACK=true ì„¤ì • í•„ìš”")
            T2I_PIPE = None
            I2I_PIPE = None
            return

    # 2ë‹¨ê³„: FALLBACK ëª¨ë¸ ì‹œë„ (SDXL)
    try:
        print(f"ğŸ”„ 2ì°¨ ì‹œë„: {FALLBACK_MODEL} (SDXL) ë¡œë”© ì¤‘...")
        
        T2I_PIPE = StableDiffusionXLPipeline.from_pretrained(
            FALLBACK_MODEL,
            cache_dir=hf_cache_dir,
            torch_dtype=dtype,
        ).to(DEVICE)

        I2I_PIPE = StableDiffusionXLImg2ImgPipeline.from_pretrained(
            FALLBACK_MODEL,
            cache_dir=hf_cache_dir,
            torch_dtype=dtype,
        ).to(DEVICE)
        
        CURRENT_MODEL = FALLBACK_MODEL
        print(f"âœ… {FALLBACK_MODEL} (í´ë°±) ë¡œë”© ì„±ê³µ!")
        return
        
    except Exception as e2:
        print(f"âŒ SDXL í´ë°±ë„ ì‹¤íŒ¨: {e2}")
        print(traceback.format_exc())
        
        # GPU OOM ì‹œ CPU í´ë°±
        if DEVICE == "cuda" and "out of memory" in str(e2).lower():
            print("âš ï¸ GPU OOM ë°œìƒ â€” CPU í´ë°± ì‹œë„")
            try:
                DEVICE = "cpu"
                T2I_PIPE = StableDiffusionXLPipeline.from_pretrained(
                    FALLBACK_MODEL,
                    cache_dir=hf_cache_dir,
                    torch_dtype=torch.float32,
                ).to("cpu")
                
                I2I_PIPE = StableDiffusionXLImg2ImgPipeline.from_pretrained(
                    FALLBACK_MODEL,
                    cache_dir=hf_cache_dir,
                    torch_dtype=torch.float32,
                ).to("cpu")
                
                CURRENT_MODEL = FALLBACK_MODEL
                print("âœ… SDXL CPU ë¡œë”© ì™„ë£Œ (ëŠë¦½ë‹ˆë‹¤)")
                return
                
            except Exception as e3:
                print(f"âŒ CPU í´ë°± ìµœì¢… ì‹¤íŒ¨: {e3}")
        
        T2I_PIPE = None
        I2I_PIPE = None
        CURRENT_MODEL = None

# ===========================
# GPTë¡œ í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸ ë²ˆì—­/ìµœì í™”
# ===========================
def optimize_prompt(text: str) -> str:
    """
    í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­ ë° ì´ë¯¸ì§€ ìƒì„±ì— ìµœì í™”
    SDXLì˜ ê²½ìš° 77 í† í° ì œí•œ ê³ ë ¤
    """
    if not openai_client:
        return text
    
    # ì´ë¯¸ ì˜ì–´ì¸ ê²½ìš° ìŠ¤í‚µ
    if all(ord(char) < 128 for char in text[:20]):
        return text
    
    try:
        # SDXLì¸ ê²½ìš° ì§§ê²Œ ìš”ì²­
        if CURRENT_MODEL and "stable-diffusion" in CURRENT_MODEL.lower():
            constraint = "Keep it under 60 words (SDXL has 77 token limit)."
        else:
            constraint = "Keep it concise but descriptive (under 150 words)."
        
        system_prompt = f"""You are a professional prompt engineer for image generation AI.
Translate Korean marketing text to optimized English prompts.
Focus on visual elements, style, mood, and composition.
{constraint}
Output ONLY the English prompt, no explanations."""

        resp = openai_client.responses.create(
            model=MODEL_GPT_MINI,
            input=f"Convert to image prompt:\n{text}",
            reasoning={"effort": "minimal"},
            max_output_tokens=200,
        )
        
        result = getattr(resp, "output_text", None) or str(resp)
        optimized = result.strip()
        print(f"ğŸ”„ í”„ë¡¬í”„íŠ¸ ìµœì í™”:\n  ì›ë³¸: {text[:80]}...\n  ìµœì í™”: {optimized[:80]}...")
        return optimized
        
    except Exception as e:
        print(f"âš ï¸ í”„ë¡¬í”„íŠ¸ ìµœì í™” ì‹¤íŒ¨, ì›ë³¸ ì‚¬ìš©: {e}")
        return text

# ===========================
# GPT-5 Mini: ë¬¸êµ¬ ìƒì„±
# ===========================
def generate_caption_core(info: dict, tone: str) -> str:
    if not openai_client:
        raise RuntimeError("OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    prompt = f"""
ë‹¹ì‹ ì€ í—¬ìŠ¤ì¼€ì–´ ì†Œìƒê³µì¸ì„ ìœ„í•œ ì „ë¬¸ ì¸ìŠ¤íƒ€ê·¸ë¨ ì½˜í…ì¸  í¬ë¦¬ì—ì´í„°ì…ë‹ˆë‹¤.
ì•„ë˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¸ìŠ¤íƒ€ê·¸ë¨ ê²Œì‹œë¬¼ì— ìµœì í™”ëœ ì½˜í…ì¸ ë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”.

ìš”ì²­:
1. ì¸ìŠ¤íƒ€ê·¸ë¨ í™ë³´ ë¬¸êµ¬ 3ê°œ ì‘ì„±
    - ê° ë¬¸êµ¬: í›„í‚¹ â†’ í•µì‹¬ ë©”ì‹œì§€ â†’ CTA
    - ì´ëª¨í‹°ì½˜ ì‚¬ìš©
    - ë¬¸ì²´ ìŠ¤íƒ€ì¼: {tone}
2. í•´ì‹œíƒœê·¸ 15ê°œ ì¶”ì²œ (ì¤‘ë³µ ì œê±°)

[ì •ë³´]
ì„œë¹„ìŠ¤ ì¢…ë¥˜: {info.get('service_type')}
ì„œë¹„ìŠ¤ëª…: {info.get('service_name')}
í•µì‹¬ íŠ¹ì§•: {info.get('features')}
ì§€ì—­: {info.get('location')}

ì¶œë ¥ í˜•ì‹:
ë¬¸êµ¬:
1. [ë¬¸êµ¬1]
2. [ë¬¸êµ¬2]
3. [ë¬¸êµ¬3]

í•´ì‹œíƒœê·¸:
#[íƒœê·¸1] #[íƒœê·¸2] ... #[íƒœê·¸N]
"""
    try:
        resp = openai_client.responses.create(
            model=MODEL_GPT_MINI,
            input=prompt,
            reasoning={"effort": "minimal"},
            max_output_tokens=512,
        )
        text = getattr(resp, "output_text", None) or str(resp)
        return text.strip()
    except Exception as e:
        print(f"ğŸš¨ GPT í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        raise

# ===========================
# ì´ë¯¸ì§€ ìƒì„± (T2I)
# ===========================
def generate_t2i_core(prompt: str, width: int, height: int, steps: int) -> bytes:
    global T2I_PIPE, CURRENT_MODEL
    
    if T2I_PIPE is None:
        raise RuntimeError("ì´ë¯¸ì§€ íŒŒì´í”„ë¼ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    # í”„ë¡¬í”„íŠ¸ ìµœì í™”
    optimized_prompt = optimize_prompt(prompt)
    
    # ëª¨ë¸ë³„ íŒŒë¼ë¯¸í„°
    params = get_model_params(CURRENT_MODEL)
    
    # ê³µí†µ íŒŒë¼ë¯¸í„°
    gen_params = {
        "prompt": optimized_prompt,
        "width": width,
        "height": height,
        "num_inference_steps": steps if steps > 1 else params["default_steps"],
    }
    
    # ì¡°ê±´ë¶€ íŒŒë¼ë¯¸í„° ì¶”ê°€
    if params["use_negative_prompt"]:
        gen_params["negative_prompt"] = "low quality, blurry, text, watermark, distorted"
    
    if params["guidance_scale"] is not None:
        gen_params["guidance_scale"] = params["guidance_scale"]
    
    print(f"ğŸ¨ ì´ë¯¸ì§€ ìƒì„± ì¤‘ (ëª¨ë¸: {CURRENT_MODEL}, steps: {gen_params['num_inference_steps']})")
    
    result = T2I_PIPE(**gen_params)
    image = result.images[0]
    
    buf = io.BytesIO()
    image.save(buf, format="PNG")
    return buf.getvalue()

# ===========================
# ì´ë¯¸ì§€ í¸ì§‘ (I2I)
# ===========================
def generate_i2i_core(input_image_bytes: bytes, prompt: str, strength: float, 
                      width: int, height: int, steps: int) -> bytes:
    global I2I_PIPE, CURRENT_MODEL
    
    if I2I_PIPE is None:
        raise RuntimeError("ì´ë¯¸ì§€ íŒŒì´í”„ë¼ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    # í”„ë¡¬í”„íŠ¸ ìµœì í™”
    optimized_prompt = optimize_prompt(prompt)
    
    # ì…ë ¥ ì´ë¯¸ì§€ ì¤€ë¹„
    input_image = Image.open(io.BytesIO(input_image_bytes)).convert("RGB").resize((width, height))
    
    # ëª¨ë¸ë³„ íŒŒë¼ë¯¸í„°
    params = get_model_params(CURRENT_MODEL)
    
    gen_params = {
        "prompt": optimized_prompt,
        "image": input_image,
        "strength": float(strength),
        "num_inference_steps": steps if steps > 1 else params["default_steps"],
    }
    
    if params["use_negative_prompt"]:
        gen_params["negative_prompt"] = "low quality, blurry, text, watermark, distorted"
    
    if params["guidance_scale"] is not None:
        gen_params["guidance_scale"] = params["guidance_scale"]
    
    print(f"âœï¸ ì´ë¯¸ì§€ í¸ì§‘ ì¤‘ (ëª¨ë¸: {CURRENT_MODEL}, strength: {strength})")
    
    result = I2I_PIPE(**gen_params)
    image = result.images[0]
    
    buf = io.BytesIO()
    image.save(buf, format="PNG")
    return buf.getvalue()