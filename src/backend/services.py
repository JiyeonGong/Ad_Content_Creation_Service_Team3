# services.py (ë¦¬íŒ©í† ë§ ë²„ì „)
"""
AI ì„œë¹„ìŠ¤ ë ˆì´ì–´ - ì„¤ì • ê¸°ë°˜ ëª¨ë¸ ê´€ë¦¬
"""
import os
import io
from typing import Optional

from openai import OpenAI
import torch
from PIL import Image
from dotenv import load_dotenv

from .model_registry import get_registry
from .model_loader import ModelLoader

# Load env
load_dotenv(os.path.join(os.path.dirname(__file__), "..", "..", ".env"))

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
MODEL_GPT_MINI = "gpt-5-mini"

# HF cache location
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
hf_cache_dir = os.path.join(project_root, "cache", "hf_models")
os.makedirs(hf_cache_dir, exist_ok=True)

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
openai_client: Optional[OpenAI] = None
model_loader: Optional[ModelLoader] = None
registry = get_registry()

# Initialize OpenAI client
if OPENAI_API_KEY:
    try:
        openai_client = OpenAI(api_key=OPENAI_API_KEY)
    except Exception as e:
        print(f"âš ï¸ OpenAI ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        openai_client = None
else:
    print("âš ï¸ OPENAI_API_KEY ë¯¸ì„¤ì • â€” GPT ê¸°ëŠ¥ ë¶ˆê°€")

# ===========================
# Utility helpers
# ===========================
def align_to_64(x: int) -> int:
    """64ì˜ ë°°ìˆ˜ë¡œ ì •ë ¬ (ìµœì†Œ 64)"""
    try:
        xi = int(x)
    except Exception:
        xi = 64
    return max(64, (xi // 64) * 64)

def ensure_steps(steps: int) -> int:
    try:
        s = int(steps)
    except Exception:
        s = 1
    return max(1, s)

# ===========================
# ëª¨ë¸ ì´ˆê¸°í™”
# ===========================
def init_image_pipelines():
    """
    ì„¤ì • íŒŒì¼ ê¸°ë°˜ìœ¼ë¡œ ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸ ë¡œë“œ
    """
    global model_loader
    
    # ì´ë¯¸ ë¡œë“œëœ ê²½ìš° ìŠ¤í‚µ
    if model_loader and model_loader.is_loaded():
        print("â„¹ï¸ ì´ë¯¸ì§€ íŒŒì´í”„ë¼ì¸ ì´ë¯¸ ë¡œë“œë¨ â€” ìŠ¤í‚µ")
        return
    
    # ModelLoader ìƒì„±
    if model_loader is None:
        model_loader = ModelLoader(cache_dir=hf_cache_dir)
    
    # í´ë°± ì²´ì¸ìœ¼ë¡œ ë¡œë”© ì‹œë„
    success = model_loader.load_with_fallback()
    
    if success:
        info = model_loader.get_current_model_info()
        print(f"âœ… ì´ë¯¸ì§€ ìƒì„± ì¤€ë¹„ ì™„ë£Œ")
        print(f"   ëª¨ë¸: {info['name']} ({info['type']})")
        print(f"   ì¥ì¹˜: {info['device']}")
    else:
        print("âŒ ëª¨ë“  ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ - ì´ë¯¸ì§€ ìƒì„± ë¶ˆê°€")

# ===========================
# í”„ë¡¬í”„íŠ¸ ìµœì í™”
# ===========================
def optimize_prompt(text: str, model_config) -> str:
    """
    í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­ ë° ìµœì í™”
    ëª¨ë¸ë³„ í† í° ì œí•œ ê³ ë ¤
    """
    if not openai_client:
        return text
    
    # í”„ë¡¬í”„íŠ¸ ìµœì í™” ì„¤ì • í™•ì¸
    opt_config = registry.get_prompt_optimization_config()
    if not opt_config.get("enabled", True):
        return text
    
    # ì´ë¯¸ ì˜ì–´ì¸ ê²½ìš° ìŠ¤í‚µ
    if not opt_config.get("translate_korean", True):
        return text
    
    if all(ord(char) < 128 for char in text[:20]):
        return text
    
    try:
        # ëª¨ë¸ë³„ ê¸¸ì´ ì œì•½
        max_tokens = model_config.max_tokens if model_config else 77
        
        if max_tokens <= 77:
            constraint = f"Keep it under 60 words (model has {max_tokens} token limit)."
        else:
            constraint = "Keep it concise but descriptive (under 150 words)."
        
        system_prompt = f"""You are a professional prompt engineer for image generation AI.
Translate Korean marketing text to optimized English prompts.
Focus on visual elements, style, mood, and composition.
{constraint}
Output ONLY the English prompt, no explanations."""

        resp = openai_client.responses.create(
            model=MODEL_GPT_MINI,
            input=f"Convert to image prompt:\n{text}",
            reasoning={"effort": "minimal"},
            max_output_tokens=200,
        )
        
        result = getattr(resp, "output_text", None) or str(resp)
        optimized = result.strip()
        print(f"ğŸ”„ í”„ë¡¬í”„íŠ¸ ìµœì í™”:\n  ì›ë³¸: {text[:80]}...\n  ìµœì í™”: {optimized[:80]}...")
        return optimized
        
    except Exception as e:
        print(f"âš ï¸ í”„ë¡¬í”„íŠ¸ ìµœì í™” ì‹¤íŒ¨, ì›ë³¸ ì‚¬ìš©: {e}")
        return text

# ===========================
# GPT-5 Mini: ë¬¸êµ¬ ìƒì„±
# ===========================
def generate_caption_core(info: dict, tone: str) -> str:
    if not openai_client:
        raise RuntimeError("OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    prompt = f"""
ë‹¹ì‹ ì€ í—¬ìŠ¤ì¼€ì–´ ì†Œìƒê³µì¸ì„ ìœ„í•œ ì „ë¬¸ ì¸ìŠ¤íƒ€ê·¸ë¨ ì½˜í…ì¸  í¬ë¦¬ì—ì´í„°ì…ë‹ˆë‹¤.
ì•„ë˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¸ìŠ¤íƒ€ê·¸ë¨ ê²Œì‹œë¬¼ì— ìµœì í™”ëœ ì½˜í…ì¸ ë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”.

ìš”ì²­:
1. ì¸ìŠ¤íƒ€ê·¸ë¨ í™ë³´ ë¬¸êµ¬ 3ê°œ ì‘ì„±
    - ê° ë¬¸êµ¬: í›„í‚¹ â†’ í•µì‹¬ ë©”ì‹œì§€ â†’ CTA
    - ì´ëª¨í‹°ì½˜ ì‚¬ìš©
    - ë¬¸ì²´ ìŠ¤íƒ€ì¼: {tone}
2. í•´ì‹œíƒœê·¸ 15ê°œ ì¶”ì²œ (ì¤‘ë³µ ì œê±°)

[ì •ë³´]
ì„œë¹„ìŠ¤ ì¢…ë¥˜: {info.get('service_type')}
ì„œë¹„ìŠ¤ëª…: {info.get('service_name')}
í•µì‹¬ íŠ¹ì§•: {info.get('features')}
ì§€ì—­: {info.get('location')}

ì¶œë ¥ í˜•ì‹:
ë¬¸êµ¬:
1. [ë¬¸êµ¬1]
2. [ë¬¸êµ¬2]
3. [ë¬¸êµ¬3]

í•´ì‹œíƒœê·¸:
#[íƒœê·¸1] #[íƒœê·¸2] ... #[íƒœê·¸N]
"""
    try:
        resp = openai_client.responses.create(
            model=MODEL_GPT_MINI,
            input=prompt,
            reasoning={"effort": "minimal"},
            max_output_tokens=512,
        )
        text = getattr(resp, "output_text", None) or str(resp)
        return text.strip()
    except Exception as e:
        print(f"ğŸš¨ GPT í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        raise

# ===========================
# ì´ë¯¸ì§€ ìƒì„± (T2I)
# ===========================
def generate_t2i_core(prompt: str, width: int, height: int, steps: int) -> bytes:
    global model_loader
    
    if not model_loader or not model_loader.is_loaded():
        raise RuntimeError("ì´ë¯¸ì§€ íŒŒì´í”„ë¼ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    model_config = model_loader.current_model_config
    
    # í”„ë¡¬í”„íŠ¸ ìµœì í™”
    optimized_prompt = optimize_prompt(prompt, model_config)
    
    # Steps ê²€ì¦
    if steps < 1:
        steps = model_config.default_steps
    steps = min(steps, model_config.max_steps)
    
    # ìƒì„± íŒŒë¼ë¯¸í„° êµ¬ì„±
    gen_params = {
        "prompt": optimized_prompt,
        "width": width,
        "height": height,
        "num_inference_steps": steps,
    }
    
    # ì¡°ê±´ë¶€ íŒŒë¼ë¯¸í„° ì¶”ê°€
    if model_config.use_negative_prompt:
        gen_params["negative_prompt"] = model_config.negative_prompt
    
    if model_config.guidance_scale is not None:
        gen_params["guidance_scale"] = model_config.guidance_scale
    
    print(f"ğŸ¨ ì´ë¯¸ì§€ ìƒì„± ì¤‘")
    print(f"   ëª¨ë¸: {model_loader.current_model_name}")
    print(f"   Steps: {steps}")
    print(f"   í¬ê¸°: {width}x{height}")
    
    # ìƒì„±
    result = model_loader.t2i_pipe(**gen_params)
    image = result.images[0]
    
    buf = io.BytesIO()
    image.save(buf, format="PNG")
    return buf.getvalue()

# ===========================
# ì´ë¯¸ì§€ í¸ì§‘ (I2I)
# ===========================
def generate_i2i_core(input_image_bytes: bytes, prompt: str, strength: float, 
                      width: int, height: int, steps: int) -> bytes:
    global model_loader
    
    if not model_loader or not model_loader.is_loaded():
        raise RuntimeError("ì´ë¯¸ì§€ íŒŒì´í”„ë¼ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    model_config = model_loader.current_model_config
    
    # I2I ì§€ì› í™•ì¸
    if not model_config.supports_i2i:
        raise RuntimeError(f"í˜„ì¬ ëª¨ë¸({model_loader.current_model_name})ì€ I2Ië¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    # í”„ë¡¬í”„íŠ¸ ìµœì í™”
    optimized_prompt = optimize_prompt(prompt, model_config)
    
    # ì…ë ¥ ì´ë¯¸ì§€ ì¤€ë¹„
    input_image = Image.open(io.BytesIO(input_image_bytes)).convert("RGB").resize((width, height))
    
    # Steps ê²€ì¦
    if steps < 1:
        steps = model_config.default_steps
    steps = min(steps, model_config.max_steps)
    
    # ìƒì„± íŒŒë¼ë¯¸í„°
    gen_params = {
        "prompt": optimized_prompt,
        "image": input_image,
        "strength": float(strength),
        "num_inference_steps": steps,
    }
    
    if model_config.use_negative_prompt:
        gen_params["negative_prompt"] = model_config.negative_prompt
    
    if model_config.guidance_scale is not None:
        gen_params["guidance_scale"] = model_config.guidance_scale
    
    print(f"âœï¸ ì´ë¯¸ì§€ í¸ì§‘ ì¤‘")
    print(f"   ëª¨ë¸: {model_loader.current_model_name}")
    print(f"   Strength: {strength}")
    print(f"   Steps: {steps}")
    
    # ìƒì„±
    result = model_loader.i2i_pipe(**gen_params)
    image = result.images[0]
    
    buf = io.BytesIO()
    image.save(buf, format="PNG")
    return buf.getvalue()

# ===========================
# ìƒíƒœ ì¡°íšŒ
# ===========================
def get_service_status() -> dict:
    """ì„œë¹„ìŠ¤ ìƒíƒœ ë°˜í™˜"""
    status = {
        "gpt_ready": openai_client is not None,
        "image_ready": model_loader and model_loader.is_loaded(),
    }
    
    if model_loader:
        status.update(model_loader.get_current_model_info())
    
    return status