# services.py (ë¦¬íŒ©í† ë§ + FLUX 3ë‹¨ê³„ í”„ë¡¬í”„íŒ… í†µí•© ë²„ì „)
"""
AI ì„œë¹„ìŠ¤ ë ˆì´ì–´ - ì„¤ì • ê¸°ë°˜ ëª¨ë¸ ê´€ë¦¬
"""
import os
import io
import logging
from typing import Optional

from openai import OpenAI
import torch
from PIL import Image
from dotenv import load_dotenv

from .model_registry import get_registry
from .model_loader import ModelLoader
from .exceptions import (
    ServiceError,
    PromptOptimizationError,
    ModelLoadError,
    WorkflowExecutionError,
    ImageProcessingError,
    ConfigurationError
)

logger = logging.getLogger(__name__)

# Load env
load_dotenv(os.path.join(os.path.dirname(__file__), "..", "..", ".env"))

# í† í¬ë‚˜ì´ì € ë³‘ë ¬ ì²˜ë¦¬ ê²½ê³  ì–µì œ
os.environ["TOKENIZERS_PARALLELISM"] = "false"

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
MODEL_GPT_MINI = "gpt-5-mini"

# HF cache location
# /mnt/data4/models ìš°ì„  ì‚¬ìš© (ëª¨ë“  ëª¨ë¸ í†µí•© ì €ì¥ì†Œ)
# GCP: /home/shared ì‚¬ìš©
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
if os.path.exists("/mnt/data4/models"):
    hf_cache_dir = "/mnt/data4/models"
elif os.path.exists("/home/shared"):
    hf_cache_dir = "/home/shared"
else:
    raise RuntimeError("ëª¨ë¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
openai_client: Optional[OpenAI] = None
model_loader: Optional[ModelLoader] = None
registry = get_registry()

# Initialize OpenAI client
if OPENAI_API_KEY:
    try:
        openai_client = OpenAI(api_key=OPENAI_API_KEY)
    except Exception as e:
        logger.warning(f"âš ï¸ OpenAI ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        openai_client = None
else:
    logger.warning("âš ï¸ OPENAI_API_KEY ë¯¸ì„¤ì • â€” GPT ê¸°ëŠ¥ ë¶ˆê°€")

# ===========================
# Utility helpers
# ===========================
def align_to_64(x: int) -> int:
    """64ì˜ ë°°ìˆ˜ë¡œ ì •ë ¬ (ìµœì†Œ 64)"""
    try:
        xi = int(x)
    except Exception:
        xi = 64
    return max(64, (xi // 64) * 64)

def ensure_steps(steps: int) -> int:
    try:
        s = int(steps)
    except Exception:
        s = 1
    return max(1, s)

# ===========================
# ëª¨ë¸ ì´ˆê¸°í™”
# ===========================
def init_image_pipelines():
    """
    ì„¤ì • íŒŒì¼ ê¸°ë°˜ìœ¼ë¡œ ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸ ë¡œë“œ
    """
    global model_loader
    
    # ì´ë¯¸ ë¡œë“œëœ ê²½ìš° ìŠ¤í‚µ
    if model_loader and model_loader.is_loaded():
        print("â„¹ï¸ ì´ë¯¸ì§€ íŒŒì´í”„ë¼ì¸ ì´ë¯¸ ë¡œë“œë¨ â€” ìŠ¤í‚µ")
        return
    
    # ModelLoader ìƒì„±
    if model_loader is None:
        model_loader = ModelLoader(cache_dir=hf_cache_dir)
    
    # í´ë°± ì²´ì¸ìœ¼ë¡œ ë¡œë”© ì‹œë„
    success = model_loader.load_with_fallback()
    
    if success:
        info = model_loader.get_current_model_info()
        logger.info(f"âœ… ì´ë¯¸ì§€ ìƒì„± ì¤€ë¹„ ì™„ë£Œ")
        print(f"   ëª¨ë¸: {info['name']} ({info['type']})")
        print(f"   ì¥ì¹˜: {info['device']}")
    else:
        print("âŒ ëª¨ë“  ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ - ì´ë¯¸ì§€ ìƒì„± ë¶ˆê°€")

# ===========================
# í”„ë¡¬í”„íŠ¸ ìµœì í™” (FLUX 3ë‹¨ê³„ í†µí•©)
# ===========================
def expand_prompt_with_gpt(text: str) -> str:
    """
    1ë‹¨ê³„: í•œêµ­ì–´ ì‹œê° ë¬˜ì‚¬ í™•ì¥
    - ë°°ê²½/ì¡°ëª…/ë¶„ìœ„ê¸°/ë™ì‘ ë“±ì„ 2~3ë¬¸ì¥ í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ í™•ì¥
    """
    if not openai_client:
        return text
    
    opt_config = registry.get_prompt_optimization_config()
    if not opt_config.get("enabled", True):
        return text
    if not opt_config.get("translate_korean", True):
        return text

    system = """
ë‹¹ì‹ ì€ ì´ë¯¸ì§€ ìƒì„±ìš© ì‹œê° ë¬˜ì‚¬ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ í™•ì¥í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ê·œì¹™:
- ì¥ë©´ì˜ ë°°ê²½, ì¡°ëª…, ë™ì‘, ë¶„ìœ„ê¸°, êµ¬ë„ ë“±ì„ ìì—°ìŠ¤ëŸ½ê³  êµ¬ì²´ì ìœ¼ë¡œ í™•ì¥í•©ë‹ˆë‹¤.
- 2~3 ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ë©°, ì¶œë ¥ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ìœ ì§€í•©ë‹ˆë‹¤.
"""

    prompt = f"{system}\n\n[ì›ë³¸ ë¬¸ì¥]\n{text}\n\nìœ„ ë¬¸ì¥ì„ ì‹œê°ì ìœ¼ë¡œ ë” ìì„¸í•œ ë¬˜ì‚¬ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ í™•ì¥í•´ì¤˜."

    try:
        resp = openai_client.responses.create(
            model=MODEL_GPT_MINI,
            reasoning={"effort": "minimal"},
            input=prompt,
            max_output_tokens=300,
        )
        expanded = getattr(resp, "output_text", None) or str(resp)
        expanded = expanded.strip()
        logger.info(f"ğŸ”„ 1/3 í™•ì¥ (í•œêµ­ì–´): {expanded[:80]}...")
        return expanded
    except Exception as e:
        logger.warning(f"âš ï¸ 1ë‹¨ê³„ í•œêµ­ì–´ í™•ì¥ ì‹¤íŒ¨ â†’ ì›ë³¸ ì‚¬ìš©: {e}")
        return text


def apply_flux_template(expanded_kor_text: str) -> str:
    """
    2ë‹¨ê³„: í™•ì¥ëœ í•œêµ­ì–´ ì„¤ëª…ì„ FLUX ìŠ¤íƒ€ì¼ ì˜ì–´ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜
    - 2~3ê°œì˜ ìì—°ìŠ¤ëŸ¬ìš´ ì˜ì–´ ë¬¸ì¥
    """
    if not openai_client:
        return expanded_kor_text

    opt_config = registry.get_prompt_optimization_config()
    if not opt_config.get("enabled", True):
        return expanded_kor_text

    system = """
You are an expert FLUX prompt engineer. Convert the expanded Korean visual description into a compact FLUX-style English prompt.

Rules:
- MUST stay under 60 English tokens.
- Use 2â€“3 short natural sentences (NOT keyword lists).
- Include: Subject, Action/Pose, Environment, Lighting, (Optional) Camera/Style.
- Insert concise realism hints: "realistic hands and face", "correct anatomy".
- Do NOT add negative prompts.

Output ONLY the final English FLUX prompt.
"""
    prompt = f"{system}\n\n[Korean expanded description]\n{expanded_kor_text}\n\nConvert following the FLUX rules."

    try:
        resp = openai_client.responses.create(
            model=MODEL_GPT_MINI,
            reasoning={"effort": "minimal"},
            input=prompt,
            max_output_tokens=200,
        )
        templated = getattr(resp, "output_text", None) or str(resp)
        templated = templated.strip()
        logger.info(f"ğŸ”„ 2/3 í…œí”Œë¦¿ (ì˜ì–´): {templated[:80]}...")
        return templated
    except Exception as e:
        logger.warning(f"âš ï¸ 2ë‹¨ê³„ FLUX í…œí”Œë¦¿ ë³€í™˜ ì‹¤íŒ¨ â†’ ì›ë³¸ ì‚¬ìš©: {e}")
        return expanded_kor_text


def optimize_prompt(text: str, model_config) -> str:
    """
    3ë‹¨ê³„: FLUX/SDXL ìµœì¢… í”„ë¡¬í”„íŠ¸ ë‹¤ë“¬ê¸°
    - FLUX: 60 í† í° ì´ë‚´, 2~3ë¬¸ì¥, negative prompt ì¶”ê°€ ê¸ˆì§€
    - ê·¸ ì™¸: ê¸¸ì´ ì œì•½ì— ë§ê²Œ ëª…ë£Œí•˜ê²Œ ë‹¤ë“¬ê¸°
    """
    if not openai_client:
        return text
    
    opt_config = registry.get_prompt_optimization_config()
    if not opt_config.get("enabled", True):
        return text

    model_type = (model_config.type if model_config else "").lower()
    is_flux = "flux" in model_type

    try:
        if is_flux:
            system_prompt = """
You are an expert FLUX prompt polisher.
Polish the prompt below.

IMPORTANT:
- Keep under 60 tokens.
- 2â€“3 short descriptive sentences (no keyword lists).
- Do NOT add negative prompts.
"""
        else:
            max_tokens = getattr(model_config, "max_tokens", 77) if model_config else 77
            if max_tokens <= 77:
                constraint = f"Keep it under 60 words (model has {max_tokens} token limit)."
            else:
                constraint = "Keep it concise but descriptive (under 150 words)."

            system_prompt = f"""
You are a professional prompt engineer for image generation AI.
Refine the prompt below for better clarity, realism, and aesthetic quality.
{constraint}
Always include relevant quality hints based on the scene to prevent artifacts 
(e.g., "detailed hands, correct anatomy, clear facial features").
"""

        full_prompt = f"{system_prompt}\n\n[Input Prompt]\n{text}\n\nOutput ONLY the polished final English prompt."

        resp = openai_client.responses.create(
            model=MODEL_GPT_MINI,
            reasoning={"effort": "minimal"},
            input=full_prompt,
            max_output_tokens=200,
        )
        optimized = getattr(resp, "output_text", None) or str(resp)
        optimized = optimized.strip()
        logger.info(f"ğŸ”„ 3/3 ìµœì¢… ìµœì í™”: {optimized[:80]}...")
        return optimized

    except Exception as e:
        logger.warning(f"âš ï¸ 3ë‹¨ê³„ ìµœì¢… ìµœì í™” ì‹¤íŒ¨ â†’ ì›ë³¸ ì‚¬ìš©: {e}")
        return text







def build_final_prompt_v2(raw_prompt: str, context: dict = None, model_config=None) -> str:
    """í†µí•© í”„ë¡¬í”„íŠ¸ ë¹Œë” (Phase 1 ê°œì„  ë²„ì „)
    
    GPT í˜¸ì¶œì„ 3íšŒ â†’ 1íšŒë¡œ í†µí•©í•˜ì—¬ ë¹„ìš© 66% ì ˆê°, ì²˜ë¦¬ ì‹œê°„ 50% ë‹¨ì¶•
    
    Args:
        raw_prompt: ì›ë³¸ í”„ë¡¬í”„íŠ¸ (í•œêµ­ì–´/ì˜ì–´ ëª¨ë‘ ê°€ëŠ¥)
        context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ (style, mood, caption, hashtags ë“±)
        model_config: ëª¨ë¸ ì„¤ì • (Noneì´ë©´ í˜„ì¬ ComfyUI ëª¨ë¸ ê¸°ì¤€)
        
    Returns:
        ìµœì í™”ëœ ìµœì¢… í”„ë¡¬í”„íŠ¸
        
    Raises:
        PromptOptimizationError: í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ
    """
    # 0) model_config ìë™ ì¶”ë¡ 
    if model_config is None:
        try:
            current_model_name = get_current_comfyui_model()
        except NameError as e:
            raise PromptOptimizationError(
                f"ëª¨ë¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}"
            ) from e

        if current_model_name:
            try:
                model_config = registry.get_model(current_model_name)
            except Exception as e:
                logger.exception("ëª¨ë¸ ì„¤ì • ì¡°íšŒ ì‹¤íŒ¨")
                raise PromptOptimizationError(
                    f"ëª¨ë¸ ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {current_model_name}"
                ) from e

    # 1) ëª¨ë¸ ì •ë³´ë¥¼ ì–»ì§€ ëª»í–ˆë‹¤ë©´ ì›ë³¸ ë°˜í™˜
    if not model_config:
        return raw_prompt.strip()

    # 2) GPT ìµœì í™” ë¹„í™œì„±í™” ì‹œ ì›ë³¸ ë°˜í™˜
    opt_config = registry.get_prompt_optimization_config()
    if not opt_config.get("enabled", True):
        return raw_prompt.strip()

    # 3) Context í†µí•©
    context = context or {}
    full_input = raw_prompt
    if context.get("caption"):
        full_input = f"{full_input} ({context['caption']})".strip()
    
    model_type = (getattr(model_config, "type", "") or "").lower()
    is_flux = "flux" in model_type

    # 4) ë‹¨ì¼ GPT í˜¸ì¶œë¡œ ì²˜ë¦¬ (ê¸°ì¡´ 3ë‹¨ê³„ í†µí•©)
    if not openai_client:
        return full_input

    try:
        if is_flux:
            # FLUX ì „ìš© í†µí•© í”„ë¡¬í”„íŠ¸ (ê¸°ì¡´ 3ë‹¨ê³„ë¥¼ í•˜ë‚˜ë¡œ)
            system_prompt = f"""You are an expert FLUX prompt engineer.
Convert Korean/English input to an optimized FLUX prompt.

Required style: {context.get('style', 'professional')}
Mood: {context.get('mood', 'natural, vivid')}

Rules:
- Expand visual details: background, lighting, action, atmosphere, composition
- Output 2-3 natural English sentences (NOT keyword lists)
- Keep under 60 tokens total
- Include concise realism hints: "realistic hands and face", "correct anatomy"
- Do NOT add negative prompts

Output ONLY the final FLUX prompt."""
        else:
            # ê¸°íƒ€ ëª¨ë¸ìš© í†µí•© í”„ë¡¬í”„íŠ¸
            max_tokens = getattr(model_config, "max_tokens", 77) if model_config else 77
            constraint = f"Keep under {max_tokens} tokens" if max_tokens <= 77 else "Keep concise but descriptive (under 150 words)"
            
            system_prompt = f"""You are a professional prompt engineer for image generation AI.
Convert and refine the input for clarity, realism, and aesthetic quality.

Style: {context.get('style', 'professional')}
Mood: {context.get('mood', 'natural')}

Rules:
- Expand visual details naturally
- {constraint}
- Include quality hints: "detailed hands, correct anatomy, clear facial features"

Output ONLY the polished English prompt."""

        full_prompt = f"{system_prompt}\n\n[Input]\n{full_input}\n\nGenerate the optimized prompt:"

        resp = openai_client.responses.create(
            model=MODEL_GPT_MINI,
            reasoning={"effort": "minimal"},
            input=full_prompt,
            max_output_tokens=200,
        )
        result = getattr(resp, "output_text", None) or str(resp)
        result = result.strip()
        logger.info(f"âœ… í†µí•© í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ (1íšŒ GPT í˜¸ì¶œ): {result[:80]}...")
        return result

    except Exception as e:
        logger.exception("í”„ë¡¬í”„íŠ¸ ìµœì í™” ì¤‘ ì˜ˆì™¸ ë°œìƒ")
        raise PromptOptimizationError(
            f"í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}"
        ) from e


def build_final_prompt(raw_prompt: str, model_config=None) -> str:
    """ê³µìš© ìµœì¢… í”„ë¡¬í”„íŠ¸ ë¹Œë” (T2I / I2I / í¸ì§‘ ê³µìš©)

    - FLUX ê³„ì—´ ëª¨ë¸:
        1) expand_prompt_with_gpt   : í•œêµ­ì–´/ì§§ì€ ë¬¸ì¥ì„ ìì—°ìŠ¤ëŸ¬ìš´ ì˜ì–´ ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ë¡œ í™•ì¥
        2) apply_flux_template      : FLUX ì „ìš© ìŠ¤íƒ€ì¼/êµ¬ë„ í…œí”Œë¦¿ ì ìš©
        3) optimize_prompt          : ëª¨ë¸ë³„ í† í° ì œí•œ/ì •ì±…ì— ë§ê²Œ ìµœì¢… ë‹¤ë“¬ê¸°
    - ê·¸ ì™¸ ëª¨ë¸: optimize_prompt í•œ ë²ˆë§Œ ì ìš©

    model_config ê°€ None ì¸ ê²½ìš°:
        - í˜„ì¬ ComfyUIì—ì„œ ë¡œë“œëœ ëª¨ë¸ ì´ë¦„ì„ ê°€ì ¸ì™€(registry ê¸°ë°˜)
        - í•´ë‹¹ ModelConfig ë¥¼ ìë™ìœ¼ë¡œ ì‚¬ìš©
        - ëª¨ë¸ ì •ë³´ë¥¼ ì–»ì§€ ëª»í•˜ë©´ raw_prompt ë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜
        
    NOTE: ì´ í•¨ìˆ˜ëŠ” í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€ë©ë‹ˆë‹¤. ìƒˆ ì½”ë“œëŠ” build_final_prompt_v2() ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.
    """
    # 0) model_config ê°€ ëª…ì‹œë˜ì§€ ì•Šì€ ê²½ìš°(ì˜ˆ: í¸ì§‘ ëª¨ë“œ)ëŠ”
    #    í˜„ì¬ ComfyUI ëª¨ë¸ ê¸°ì¤€ìœ¼ë¡œ ìë™ ì¶”ë¡ 
    if model_config is None:
        try:
            current_model_name = get_current_comfyui_model()
        except NameError:
            current_model_name = None

        if current_model_name:
            try:
                model_config = registry.get_model(current_model_name)
            except Exception:
                model_config = None

    # 1) ì—¬ì „íˆ ëª¨ë¸ ì •ë³´ë¥¼ ì–»ì§€ ëª»í–ˆë‹¤ë©´, ì›ë³¸ í”„ë¡¬í”„íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    if not model_config:
        return raw_prompt.strip()

    model_type = (getattr(model_config, "type", "") or "").lower()

    # 2) FLUX ê³„ì—´ ëª¨ë¸ì¸ ê²½ìš° 3ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì ìš©
    if "flux" in model_type:
        expanded = expand_prompt_with_gpt(raw_prompt)
        templated = apply_flux_template(expanded)
        final_prompt = optimize_prompt(templated, model_config)
    else:
        # 3) ê·¸ ì™¸ ëª¨ë¸ì€ ë‹¨ì¼ ìµœì í™”ë§Œ ì ìš©
        final_prompt = optimize_prompt(raw_prompt, model_config)

    # 4) í˜¹ì‹œ ê²°ê³¼ê°€ ë¹„ì–´ ìˆìœ¼ë©´ í´ë°±ìœ¼ë¡œ raw_prompt ì‚¬ìš©
    return final_prompt.strip() or raw_prompt.strip()













# def build_final_prompt(raw_prompt: str, model_config) -> str:
#     """
#     ê³µìš© ìµœì¢… í”„ë¡¬í”„íŠ¸ ë¹Œë” (T2I / I2I / í¸ì§‘ ê³µìš©)
#     - FLUX: 3ë‹¨ê³„ (í•œêµ­ì–´ í™•ì¥ â†’ FLUX í…œí”Œë¦¿ â†’ ìµœì¢… í´ë¦¬ì‹œ)
#     - ê·¸ ì™¸: ë‹¨ì¼ ìµœì¢… í´ë¦¬ì‹œ
#     """
#     if not model_config:
#         return raw_prompt

#     model_type = (model_config.type if model_config else "").lower()

#     if "flux" in model_type:
#         expanded = expand_prompt_with_gpt(raw_prompt)
#         templated = apply_flux_template(expanded)
#         final_prompt = optimize_prompt(templated, model_config)
#     else:
#         final_prompt = optimize_prompt(raw_prompt, model_config)

#     return final_prompt.strip() or raw_prompt

# ===========================
# GPT-5 Mini: ë¬¸êµ¬ ìƒì„±
# ===========================
def generate_caption_core(info: dict, tone: str) -> str:
    if not openai_client:
        raise RuntimeError("OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    prompt = f"""
ë‹¹ì‹ ì€ í—¬ìŠ¤ì¼€ì–´ ì†Œìƒê³µì¸ì„ ìœ„í•œ ì „ë¬¸ ì¸ìŠ¤íƒ€ê·¸ë¨ ì½˜í…ì¸  í¬ë¦¬ì—ì´í„°ì…ë‹ˆë‹¤.
ì•„ë˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¸ìŠ¤íƒ€ê·¸ë¨ ê²Œì‹œë¬¼ì— ìµœì í™”ëœ ì½˜í…ì¸ ë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”.

ìš”ì²­:
1. ì¸ìŠ¤íƒ€ê·¸ë¨ í™ë³´ ë¬¸êµ¬ 3ê°œ ì‘ì„±
    - ê° ë¬¸êµ¬: í›„í‚¹ â†’ í•µì‹¬ ë©”ì‹œì§€ â†’ CTA
    - ì´ëª¨í‹°ì½˜ ì‚¬ìš©
    - ë¬¸ì²´ ìŠ¤íƒ€ì¼: {tone}
2. í•´ì‹œíƒœê·¸ 15ê°œ ì¶”ì²œ (ì¤‘ë³µ ì œê±°)

[ì •ë³´]
ì„œë¹„ìŠ¤ ì¢…ë¥˜: {info.get('service_type')}
ì„œë¹„ìŠ¤ëª…: {info.get('service_name')}
í•µì‹¬ íŠ¹ì§•: {info.get('features')}
ì§€ì—­: {info.get('location')}

ì¶œë ¥ í˜•ì‹:
ë¬¸êµ¬:
1. [ë¬¸êµ¬1]
2. [ë¬¸êµ¬2]
3. [ë¬¸êµ¬3]

í•´ì‹œíƒœê·¸:
#[íƒœê·¸1] #[íƒœê·¸2] ... #[íƒœê·¸N]
"""
    try:
        resp = openai_client.responses.create(
            model=MODEL_GPT_MINI,
            input=prompt,
            reasoning={"effort": "minimal"},
            max_output_tokens=512,
        )
        text = getattr(resp, "output_text", None) or str(resp)
        return text.strip()
    except Exception as e:
        logger.error(f"ğŸš¨ GPT í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        raise

# ===========================
# ğŸ†• ì´ë¯¸ì§€ ìƒì„± (T2I) - ComfyUI ê¸°ë°˜
# ===========================
def generate_t2i_core(
    prompt: str,
    width: int,
    height: int,
    steps: int,
    guidance_scale: float = None,
    enable_adetailer: bool = True,
    adetailer_targets: list = None,
    post_process_method: str = "none",  # "none", "impact_pack", "adetailer"
    model_name: str = None  # ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„ (ì—†ìœ¼ë©´ í˜„ì¬ ë¡œë“œëœ ëª¨ë¸ ì‚¬ìš©)
) -> bytes:
    """
    ComfyUIë¥¼ ì‚¬ìš©í•œ T2I ì´ë¯¸ì§€ ìƒì„±

    Args:
        post_process_method: í›„ì²˜ë¦¬ ë°©ì‹
            - "none": í›„ì²˜ë¦¬ ì—†ìŒ
            - "impact_pack": ComfyUI Impact Pack (YOLO+SAM)
            - "adetailer": ê¸°ì¡´ ADetailer (YOLO+MediaPipe)
        model_name: ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„ (ì„ íƒì‚¬í•­, ì—†ìœ¼ë©´ í˜„ì¬ ë¡œë“œëœ ëª¨ë¸ ì‚¬ìš©)
    """
    from .comfyui_client import ComfyUIClient
    from .comfyui_workflows import (
        get_flux_t2i_workflow,
        get_flux_t2i_with_impact_workflow,
        update_flux_t2i_workflow,
        load_image_editing_config
    )

    # í˜„ì¬ ë¡œë“œëœ ComfyUI ëª¨ë¸ í™•ì¸
    current_model_name = get_current_comfyui_model()

    # ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ê³ , ìš”ì²­ì— model_nameì´ ìˆìœ¼ë©´ ìë™ ë¡œë“œ
    if not current_model_name and model_name:
        logger.info(f"ğŸ”„ ëª¨ë¸ ìë™ ë¡œë“œ ì‹œì‘: {model_name}")
        # ì „ì—­ ë³€ìˆ˜ ì—…ë°ì´íŠ¸ (ì‹¤ì œ ì›Œí¬í”Œë¡œìš°ì—ì„œ ì‚¬ìš©ë¨)
        global current_comfyui_model
        current_comfyui_model = model_name
        current_model_name = model_name
    elif not current_model_name:
        raise RuntimeError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”.")

    # ëª¨ë¸ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    model_config = registry.get_model(current_model_name)
    if not model_config:
        raise RuntimeError(f"ëª¨ë¸ ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {current_model_name}")

    # âœ… í†µí•© í”„ë¡¬í”„íŠ¸ ë¹Œë” ì‚¬ìš© (Phase 1 ê°œì„ )
    context = {
        "style": "Instagram banner, professional",
        "mood": "vibrant, motivational"
    }
    final_prompt = build_final_prompt_v2(prompt, context, model_config)

    # Steps ê²€ì¦
    if steps < 1:
        steps = model_config.default_steps
    steps = min(steps, model_config.max_steps)

    # Guidance scale ì„¤ì •
    if guidance_scale is None:
        guidance_scale = model_config.guidance_scale

    logger.info(f"ğŸ¨ ComfyUIë¡œ T2I ì´ë¯¸ì§€ ìƒì„± ì¤‘")
    print(f"   ëª¨ë¸: {current_model_name}")
    print(f"   í›„ì²˜ë¦¬: {post_process_method}")
    print(f"   Steps: {steps}")
    print(f"   í¬ê¸°: {width}x{height}")
    print(f"   Guidance: {guidance_scale}")

    try:
        # ComfyUI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        config = load_image_editing_config()
        comfyui_config = config.get("comfyui", {})
        base_url = comfyui_config.get("base_url", "http://localhost:8188")
        timeout = comfyui_config.get("timeout", 600)

        client = ComfyUIClient(base_url=base_url, timeout=timeout)

        # ì›Œí¬í”Œë¡œìš° ì„ íƒ
        if post_process_method == "impact_pack":
            workflow = get_flux_t2i_with_impact_workflow()
        else:
            workflow = get_flux_t2i_workflow()

        # ì›Œí¬í”Œë¡œìš° íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        workflow = update_flux_t2i_workflow(
            workflow=workflow,
            model_name=current_model_name,
            prompt=final_prompt,
            width=width,
            height=height,
            steps=steps,
            guidance_scale=guidance_scale
        )

        # ComfyUI ì‹¤í–‰
        output_images, history = client.execute_workflow(workflow=workflow)

        if not output_images:
            raise Exception("ì¶œë ¥ ì´ë¯¸ì§€ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        image_bytes = output_images[0]

        # ê¸°ì¡´ ADetailer í›„ì²˜ë¦¬ (ì„ íƒ ì‹œ)
        if post_process_method == "adetailer" and enable_adetailer:
            image = Image.open(io.BytesIO(image_bytes))
            image = apply_adetailer(
                image=image,
                prompt=final_prompt,
                targets=adetailer_targets or ["hand"]
            )

            buf = io.BytesIO()
            image.save(buf, format="PNG")
            image_bytes = buf.getvalue()

        logger.info(f"âœ… ìƒì„± ì™„ë£Œ: {len(image_bytes)} bytes")
        return image_bytes

    except Exception as e:
        logger.error(f"âŒ ComfyUI T2I ìƒì„± ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        raise RuntimeError(f"ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")


# ===========================
# ADetailer í›„ì²˜ë¦¬
# ===========================
def apply_adetailer(
    image: Image,
    prompt: str,
    targets: list = None,
    strength: float = 0.4
) -> Image:
    """
    ADetailer ìŠ¤íƒ€ì¼ í›„ì²˜ë¦¬
    - ì†/ì–¼êµ´ ê°ì§€ í›„ í•´ë‹¹ ì˜ì—­ë§Œ Inpaintë¡œ ì¬ìƒì„±
    """
    global model_loader

    if targets is None:
        targets = ["hand"]

    try:
        from .post_processor import get_post_processor

        logger.info(f"ğŸ”§ ADetailer í›„ì²˜ë¦¬ ì‹œì‘ (targets: {targets})")

        # model_loader ì´ˆê¸°í™” (ADetailerìš©)
        if model_loader is None or not model_loader.is_loaded():
            model_loader = ModelLoader(cache_dir=hf_cache_dir)
            success = model_loader.load_with_fallback()
            if not success:
                logger.warning("âš ï¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ - ADetailer ê±´ë„ˆëœ€")
                return image

        post_processor = get_post_processor()

        # I2I íŒŒì´í”„ë¼ì¸ì„ Inpaintìš©ìœ¼ë¡œ ì‚¬ìš©
        inpaint_pipe = model_loader.i2i_pipe

        # ComfyUI ì‚¬ìš© ì‹œ i2i_pipeê°€ Noneì´ë¯€ë¡œ ADetailer ì‚¬ìš© ë¶ˆê°€
        if inpaint_pipe is None:
            logger.warning("âš ï¸ ComfyUI ì‚¬ìš© ì¤‘ - ADetailerëŠ” Impact Packì„ ì‚¬ìš©í•˜ì„¸ìš”")
            return image

        processed_image, info = post_processor.full_pipeline(
            image=image,
            inpaint_pipeline=inpaint_pipe,
            prompt=prompt,
            auto_detect=True,
            adetailer_targets=targets,
            adetailer_strength=strength
        )

        if not info["processed"]:
            logger.info(f"â„¹ï¸ ADetailer: ì´ìƒ ì—†ìŒ, ì›ë³¸ ìœ ì§€")

        return processed_image

    except Exception as e:
        logger.warning(f"âš ï¸ ADetailer ì‹¤íŒ¨, ì›ë³¸ ë°˜í™˜: {e}")
        return image

# ===========================
# ğŸ†• ì´ë¯¸ì§€ í¸ì§‘ (I2I) - ComfyUI ê¸°ë°˜
# ===========================
def generate_i2i_core(
    input_image_bytes: bytes,
    prompt: str,
    strength: float,
    width: int,
    height: int,
    steps: int,
    guidance_scale: float = None,
    enable_adetailer: bool = False,
    adetailer_targets: list = None,
    post_process_method: str = "none",  # "none", "impact_pack", "adetailer"
    model_name: str = None  # ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„ (ì—†ìœ¼ë©´ í˜„ì¬ ë¡œë“œëœ ëª¨ë¸ ì‚¬ìš©)
) -> bytes:
    """
    ComfyUIë¥¼ ì‚¬ìš©í•œ I2I ì´ë¯¸ì§€ í¸ì§‘

    Args:
        input_image_bytes: ì…ë ¥ ì´ë¯¸ì§€ ë°”ì´íŠ¸
        prompt: í¸ì§‘ í”„ë¡¬í”„íŠ¸
        strength: í¸ì§‘ ê°•ë„ (0.0~1.0)
        width, height: ì¶œë ¥ í¬ê¸°
        steps: ìƒ˜í”Œë§ ìŠ¤í…
        guidance_scale: CFG ìŠ¤ì¼€ì¼
        enable_adetailer: ADetailer í™œì„±í™” ì—¬ë¶€ (legacy)
        adetailer_targets: í›„ì²˜ë¦¬ íƒ€ê²Ÿ
        post_process_method: í›„ì²˜ë¦¬ ë°©ì‹
            - "none": í›„ì²˜ë¦¬ ì—†ìŒ
            - "impact_pack": ComfyUI Impact Pack (YOLO+SAM)
            - "adetailer": ê¸°ì¡´ ADetailer (YOLO+MediaPipe)
        model_name: ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„ (ì„ íƒì‚¬í•­, ì—†ìœ¼ë©´ í˜„ì¬ ë¡œë“œëœ ëª¨ë¸ ì‚¬ìš©)
    """
    from .comfyui_client import ComfyUIClient
    from .comfyui_workflows import (
        get_flux_i2i_workflow,
        update_flux_i2i_workflow,
        load_image_editing_config
    )

    # í˜„ì¬ ë¡œë“œëœ ComfyUI ëª¨ë¸ í™•ì¸
    current_model_name = get_current_comfyui_model()

    # ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ê³ , ìš”ì²­ì— model_nameì´ ìˆìœ¼ë©´ ìë™ ë¡œë“œ
    if not current_model_name and model_name:
        logger.info(f"ğŸ”„ ëª¨ë¸ ìë™ ë¡œë“œ ì‹œì‘: {model_name}")
        # ì „ì—­ ë³€ìˆ˜ ì—…ë°ì´íŠ¸ (ì‹¤ì œ ì›Œí¬í”Œë¡œìš°ì—ì„œ ì‚¬ìš©ë¨)
        global current_comfyui_model
        current_comfyui_model = model_name
        current_model_name = model_name
    elif not current_model_name:
        raise RuntimeError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”.")

    # ëª¨ë¸ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    model_config = registry.get_model(current_model_name)
    if not model_config:
        raise RuntimeError(f"ëª¨ë¸ ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {current_model_name}")

    # âœ… í†µí•© í”„ë¡¬í”„íŠ¸ ë¹Œë” ì‚¬ìš© (Phase 1 ê°œì„ )
    context = {
        "style": "professional, natural",
        "mood": "balanced, refined"
    }
    final_prompt = build_final_prompt_v2(prompt, context, model_config)

    # Steps ê²€ì¦
    if steps < 1:
        steps = model_config.default_steps
    steps = min(steps, model_config.max_steps)

    # Guidance scale ì„¤ì •
    if guidance_scale is None:
        guidance_scale = model_config.guidance_scale

    print(f"âœï¸ ComfyUIë¡œ I2I ì´ë¯¸ì§€ í¸ì§‘ ì¤‘")
    print(f"   ëª¨ë¸: {current_model_name}")
    print(f"   í›„ì²˜ë¦¬: {post_process_method}")
    print(f"   Strength: {strength}")
    print(f"   Steps: {steps}")
    print(f"   í¬ê¸°: {width}x{height}")
    print(f"   Guidance: {guidance_scale}")

    try:
        # ComfyUI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        config = load_image_editing_config()
        comfyui_config = config.get("comfyui", {})
        base_url = comfyui_config.get("base_url", "http://localhost:8188")
        timeout = comfyui_config.get("timeout", 600)

        client = ComfyUIClient(base_url=base_url, timeout=timeout)

        # I2I ì›Œí¬í”Œë¡œìš° ê°€ì ¸ì˜¤ê¸°
        workflow = get_flux_i2i_workflow()

        # ì›Œí¬í”Œë¡œìš° íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        workflow = update_flux_i2i_workflow(
            workflow=workflow,
            model_name=current_model_name,
            prompt=final_prompt,
            strength=strength,
            steps=steps,
            guidance_scale=guidance_scale
        )

        # ComfyUI ì‹¤í–‰ (ì…ë ¥ ì´ë¯¸ì§€ í¬í•¨)
        output_images, history = client.execute_workflow(
            workflow=workflow,
            input_image=input_image_bytes,
            input_image_node_id="11"  # LoadImage ë…¸ë“œ ID
        )

        if not output_images:
            raise Exception("ì¶œë ¥ ì´ë¯¸ì§€ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        image_bytes = output_images[0]

        # ê¸°ì¡´ ADetailer í›„ì²˜ë¦¬ (ì„ íƒ ì‹œ)
        if post_process_method == "adetailer" and enable_adetailer:
            image = Image.open(io.BytesIO(image_bytes))
            image = apply_adetailer(
                image=image,
                prompt=final_prompt,
                targets=adetailer_targets or ["hand"]
            )

            buf = io.BytesIO()
            image.save(buf, format="PNG")
            image_bytes = buf.getvalue()

        logger.info(f"âœ… í¸ì§‘ ì™„ë£Œ: {len(image_bytes)} bytes")
        return image_bytes

    except Exception as e:
        logger.error(f"âŒ ComfyUI I2I í¸ì§‘ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        raise RuntimeError(f"ì´ë¯¸ì§€ í¸ì§‘ ì‹¤íŒ¨: {e}")

# ===========================
# ëª¨ë¸ ì „í™˜
# ===========================
# ===========================
# ìƒíƒœ ì¡°íšŒ
# ===========================
def get_service_status() -> dict:
    """ì„œë¹„ìŠ¤ ìƒíƒœ ë°˜í™˜"""
    current_model = get_current_comfyui_model()
    status = {
        "gpt_ready": openai_client is not None,
        "image_ready": current_model is not None,
        "current_model": current_model
    }

    return status

# ===========================
# ğŸ†• ì´ë¯¸ì§€ í¸ì§‘ (ComfyUI)
# ===========================
def edit_image_with_comfyui(
    experiment_id: str,
    input_image_bytes: bytes,
    prompt: str,
    negative_prompt: str = "",
    steps: int = None,
    guidance_scale: float = None,
    strength: float = None,
    # ìƒˆë¡œìš´ ëª¨ë“œìš© íŒŒë¼ë¯¸í„°
    controlnet_type: str = "depth",
    controlnet_strength: float = 0.7,
    denoise_strength: float = 1.0,
    blending_strength: float = 0.35,
    background_prompt: str = None
) -> dict:
    """
    ComfyUIë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ í¸ì§‘

    Args:
        experiment_id: ì‹¤í—˜ ID ("portrait_mode", "product_mode", "hybrid_mode", "ben2_flux_fill", "ben2_qwen_image")
        input_image_bytes: ì…ë ¥ ì´ë¯¸ì§€ ë°”ì´íŠ¸
        prompt: í¸ì§‘ í”„ë¡¬í”„íŠ¸
        steps: ì¶”ë¡  ë‹¨ê³„
        guidance_scale: Guidance scale
        strength: ë³€í™” ê°•ë„
        controlnet_type: ControlNet íƒ€ì… ("depth" ë˜ëŠ” "canny")
        controlnet_strength: ControlNet ê°•ë„
        denoise_strength: ë³€ê²½ ê°•ë„
        blending_strength: í•©ì„± ìì—°ìŠ¤ëŸ¬ì›€ (Product ëª¨ë“œ)
        background_prompt: ë°°ê²½ í”„ë¡¬í”„íŠ¸ (Product ëª¨ë“œ)
    """
    import base64
    import time
    import logging
    from .comfyui_client import ComfyUIClient
    from .comfyui_workflows import (
        get_workflow_template,
        update_workflow_inputs,
        get_workflow_input_image_node_id,
        load_image_editing_config,
        get_pipeline_steps_for_mode
    )

    logger = logging.getLogger(__name__)
    start_time = time.time()

    try:
        # ì„¤ì • ë¡œë“œ
        config = load_image_editing_config()

        # íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ ë§¤í•‘ ë¡œë“œ
        pipeline_steps = get_pipeline_steps_for_mode(experiment_id)

        # ëª¨ë“œ ì •ë³´ ì°¾ê¸° (ìƒˆë¡œìš´ êµ¬ì¡°)
        mode_info = None
        for mode_id, mode_data in config.get("editing_modes", {}).items():
            if mode_data["id"] == experiment_id:
                mode_info = mode_data
                break

        if not mode_info:
            return {
                "success": False,
                "experiment_id": experiment_id,
                "experiment_name": "Unknown",
                "output_image_base64": None,
                "background_removed_image_base64": None,
                "error": f"ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë“œ ID: {experiment_id}",
                "elapsed_time": None
            }

        # âœ… í¸ì§‘ í”„ë¡¬í”„íŠ¸ì—ë„ í†µí•© ë¹Œë” ì ìš© (Phase 1 ê°œì„ )
        # í¸ì§‘ ëª¨ë“œëŠ” íŠ¹ì • ëª¨ë¸ ì„¤ì •ì„ ë°”ë¡œ ê°€ì ¸ì˜¤ê¸° ì–´ë ¤ìš°ë¯€ë¡œ model_config=Noneìœ¼ë¡œ ë™ì‘ (fallback)
        context = {
            "style": "professional editing",
            "mood": "refined, enhanced"
        }
        final_prompt = build_final_prompt_v2(prompt, context, model_config=None)

        # ComfyUI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        comfyui_config = config.get("comfyui", {})
        base_url = comfyui_config.get("base_url", "http://localhost:8188")
        timeout = comfyui_config.get("timeout", 600)

        client = ComfyUIClient(base_url=base_url, timeout=timeout)

        # ì›Œí¬í”Œë¡œìš° í…œí”Œë¦¿ ê°€ì ¸ì˜¤ê¸°
        workflow = get_workflow_template(experiment_id)

        # ì›Œí¬í”Œë¡œìš° ì—…ë°ì´íŠ¸ (ì‚¬ìš©ì ì…ë ¥ ë°˜ì˜)
        workflow = update_workflow_inputs(
            workflow=workflow,
            experiment_id=experiment_id,
            prompt=final_prompt,
            negative_prompt=negative_prompt,
            steps=steps,
            guidance_scale=guidance_scale,
            strength=strength,
            # ìƒˆë¡œìš´ ëª¨ë“œ íŒŒë¼ë¯¸í„°
            controlnet_type=controlnet_type,
            controlnet_strength=controlnet_strength,
            denoise_strength=denoise_strength,
            blending_strength=blending_strength,
            background_prompt=background_prompt
        )

        # ì…ë ¥ ì´ë¯¸ì§€ ë…¸ë“œ ID
        input_node_id = get_workflow_input_image_node_id(experiment_id)

        logger.info(f"ğŸ¨ ComfyUI ì´ë¯¸ì§€ í¸ì§‘ ì‹œì‘")
        logger.info(f"   ëª¨ë“œ: {mode_info['name']}")
        logger.info(f"   ì„¤ëª…: {mode_info['description']}")
        logger.info(f"   í”„ë¡¬í”„íŠ¸: {final_prompt}")
        logger.info(f"   íŒŒë¼ë¯¸í„°: steps={steps}, guidance={guidance_scale}")
        if experiment_id == "portrait_mode" or experiment_id == "hybrid_mode":
            logger.info(f"   ControlNet: type={controlnet_type}, strength={controlnet_strength}, denoise={denoise_strength}")
        elif experiment_id == "product_mode":
            logger.info(f"   ë°°ê²½: {background_prompt or final_prompt}, blending={blending_strength}")

        # ì§„í–‰ìƒí™© ì½œë°± í•¨ìˆ˜ ì •ì˜
        step_count = [0]  # ì™„ë£Œëœ ë‹¨ê³„ ìˆ˜ (mutable ë¦¬ìŠ¤íŠ¸ë¡œ í´ë¡œì €ì—ì„œ ìˆ˜ì • ê°€ëŠ¥)

        def progress_callback(node_id: str, elapsed: float):
            """ë…¸ë“œ ì™„ë£Œ ì‹œ í˜¸ì¶œë˜ëŠ” ì½œë°±"""
            step_name = pipeline_steps.get(node_id, f"ë…¸ë“œ {node_id}")
            step_count[0] += 1
            logger.info(f"   [{step_count[0]:2d}/{len(pipeline_steps):2d}] {step_name} (ê²½ê³¼: {elapsed:.1f}ì´ˆ)")

        # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        logger.info(f"ğŸ”„ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹œì‘ (ì´ {len(pipeline_steps)}ë‹¨ê³„)")
        output_images, history = client.execute_workflow(
            workflow=workflow,
            input_image=input_image_bytes,
            input_image_node_id=input_node_id,
            progress_callback=progress_callback
        )

        if not output_images:
            raise Exception("ì¶œë ¥ ì´ë¯¸ì§€ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ë¥¼ ìµœì¢… ê²°ê³¼ë¡œ ì‚¬ìš©
        output_image_bytes = output_images[0]
        output_image_base64 = base64.b64encode(output_image_bytes).decode("utf-8")

        # ë°°ê²½ ì œê±° ì´ë¯¸ì§€ (ì„ íƒì )
        background_removed_base64 = None
        if len(output_images) > 1:
            background_removed_base64 = base64.b64encode(output_images[1]).decode("utf-8")

        elapsed_time = time.time() - start_time

        logger.info(f"âœ… ComfyUI í¸ì§‘ ì™„ë£Œ! (ì†Œìš” ì‹œê°„: {elapsed_time:.1f}ì´ˆ)")

        return {
            "success": True,
            "experiment_id": experiment_id,
            "experiment_name": mode_info["name"],
            "output_image_base64": output_image_base64,
            "background_removed_image_base64": background_removed_base64,
            "error": None,
            "elapsed_time": elapsed_time
        }

    except Exception as e:
        elapsed_time = time.time() - start_time
        error_msg = str(e)
        logger.error(f"âŒ ComfyUI í¸ì§‘ ì‹¤íŒ¨: {error_msg}")

        return {
            "success": False,
            "experiment_id": experiment_id,
            "experiment_name": "Unknown",
            "output_image_base64": None,
            "background_removed_image_base64": None,
            "error": error_msg,
            "elapsed_time": elapsed_time
        }


def get_image_editing_experiments() -> dict:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ì´ë¯¸ì§€ í¸ì§‘ ì‹¤í—˜ ëª©ë¡ ë°˜í™˜ (ìƒì„± ëª¨ë¸ + í¸ì§‘ ëª¨ë¸)"""
    from .comfyui_workflows import load_image_editing_config

    try:
        config = load_image_editing_config()
        experiments = config.get("experiments", [])

        # í¸ì§‘ ì‹¤í—˜ ëª©ë¡
        editing_experiments = [
            {
                "id": exp["id"],
                "name": exp["name"],
                "description": exp["description"],
                "background_removal_model": exp["background_removal"]["model"],
                "editing_model": exp["image_editing"]["model"],
                "features": exp.get("features", [])  # ëª¨ë¸ë³„ ê¸°ëŠ¥ ëª©ë¡ í¬í•¨
            }
            for exp in experiments
        ]

        # ìƒì„± ëª¨ë¸ ëª©ë¡ ì¶”ê°€
        generation_models = [
            {
                "id": "FLUX.1-dev-Q8",
                "name": "FLUX.1-dev Q8",
                "description": "FLUX.1-dev GGUF 8-bit ì–‘ìí™” (ì´ë¯¸ì§€ ìƒì„±, ê¶Œì¥)"
            },
            {
                "id": "FLUX.1-dev-Q4",
                "name": "FLUX.1-dev Q4",
                "description": "FLUX.1-dev GGUF 4-bit ì–‘ìí™” (ë©”ëª¨ë¦¬ ì ˆì•½)"
            }
        ]

        # ìƒì„± ëª¨ë¸ + í¸ì§‘ ëª¨ë¸ ëª¨ë‘ ë°˜í™˜
        all_experiments = generation_models + editing_experiments

        return {
            "success": True,
            "experiments": all_experiments
        }

    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "experiments": []
        }


# ===========================
# ğŸ†• ComfyUI ëª¨ë¸ ê´€ë¦¬ (í”„ë¦¬ë¡œë”©/ì–¸ë¡œë“œ)
# ===========================
current_comfyui_model: Optional[str] = None

# í”„ë¦¬ë¡œë“œ ê¸°ëŠ¥ ì œê±°ë¨
def _removed_preload_model_in_comfyui(experiment_id: str) -> dict:
    """
    ComfyUIì— ëª¨ë¸ ë¯¸ë¦¬ ë¡œë“œ (ìµœì†Œ ì‹¤í–‰ ì›Œí¬í”Œë¡œìš° ì „ì†¡)
    """
    global current_comfyui_model
    
    # ì´ë¯¸ ë¡œë“œëœ ëª¨ë¸ì´ë©´ ìŠ¤í‚µ
    if current_comfyui_model == experiment_id:
        return {"success": True, "message": "ì´ë¯¸ ë¡œë“œëœ ëª¨ë¸ì…ë‹ˆë‹¤.", "model": experiment_id}

    from .comfyui_client import ComfyUIClient
    from .comfyui_workflows import _removed_get_preload_workflow, load_image_editing_config, get_workflow_input_image_node_id
    import io
    from PIL import Image
    
    try:
        config = load_image_editing_config()
        comfyui_config = config.get("comfyui", {})
        base_url = comfyui_config.get("base_url", "http://localhost:8188")
        
        client = ComfyUIClient(base_url=base_url)
        
        # 1. ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„± (64x64 ê²€ì€ìƒ‰)
        dummy_image = Image.new('RGB', (64, 64), color='black')
        img_byte_arr = io.BytesIO()
        dummy_image.save(img_byte_arr, format='PNG')
        img_byte_arr.seek(0)
        
        # 2. ì´ë¯¸ì§€ ì—…ë¡œë“œ
        filename = "preload_dummy.png"
        upload_resp = client.upload_image(img_byte_arr.read(), filename)
        if not upload_resp:
            return {"success": False, "message": "ë”ë¯¸ ì´ë¯¸ì§€ ì—…ë¡œë“œ ì‹¤íŒ¨"}
        
        # 3. í”„ë¦¬ë¡œë”© ì›Œí¬í”Œë¡œìš° ìƒì„±
        workflow = _removed_get_preload_workflow(experiment_id)
        if not workflow:
            return {"success": False, "message": "í”„ë¦¬ë¡œë”© ì›Œí¬í”Œë¡œìš° ìƒì„± ì‹¤íŒ¨"}
        
        # 4. ì…ë ¥ ì´ë¯¸ì§€ ë…¸ë“œ ì„¤ì •
        input_node_id = get_workflow_input_image_node_id(experiment_id)
        if input_node_id in workflow:
            workflow[input_node_id]["inputs"]["image"] = filename
            
        # 5. íì— ì „ì†¡
        logger.info(f"ğŸš€ ëª¨ë¸ í”„ë¦¬ë¡œë”© ì‹œì‘: {experiment_id}")
        client.queue_prompt(workflow)
        
        # 6. ìƒíƒœ ì—…ë°ì´íŠ¸
        current_comfyui_model = experiment_id
        
        return {"success": True, "message": "ëª¨ë¸ ë¡œë”© ìš”ì²­ ì™„ë£Œ", "model": experiment_id}
        
    except Exception as e:
        error_msg = str(e)
        # prompt_no_outputs ì—ëŸ¬ëŠ” ëª…í™•í•˜ê²Œ ì‹¤íŒ¨ë¡œ ì²˜ë¦¬
        if "prompt_no_outputs" in error_msg.lower():
            logger.error(f"âŒ ì›Œí¬í”Œë¡œìš°ì— ì¶œë ¥ ë…¸ë“œê°€ ì—†ì–´ ì‹¤í–‰ ë¶ˆê°€")
            return {"success": False, "message": "ì›Œí¬í”Œë¡œìš° êµ¬ì„± ì˜¤ë¥˜ (ì¶œë ¥ ë…¸ë“œ í•„ìš”)"}
        
        logger.error(f"âŒ ëª¨ë¸ í”„ë¦¬ë¡œë”© ì‹¤íŒ¨: {e}")
        return {"success": False, "message": str(e)}

def unload_comfyui_model() -> dict:
    """ComfyUI ëª¨ë¸ ì–¸ë¡œë“œ ë° ë©”ëª¨ë¦¬ í•´ì œ"""
    global current_comfyui_model
    
    from .comfyui_client import ComfyUIClient
    from .comfyui_workflows import load_image_editing_config
    
    try:
        config = load_image_editing_config()
        base_url = config.get("comfyui", {}).get("base_url", "http://localhost:8188")
        
        client = ComfyUIClient(base_url=base_url)
        
        # ë©”ëª¨ë¦¬ í•´ì œ ìš”ì²­
        success = client.free_memory(unload_models=True, free_memory=True)
        
        if success:
            current_comfyui_model = None
            return {"success": True, "message": "ëª¨ë¸ ì–¸ë¡œë“œ ë° ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ"}
        else:
            return {"success": False, "message": "ë©”ëª¨ë¦¬ í•´ì œ ìš”ì²­ ì‹¤íŒ¨"}
            
    except Exception as e:
        return {"success": False, "message": str(e)}

def get_current_comfyui_model() -> Optional[str]:
    """í˜„ì¬ ë¡œë“œëœ ComfyUI ëª¨ë¸ ID ë°˜í™˜"""
    return current_comfyui_model

def check_comfyui_status() -> dict:
    """ComfyUI ì„œë²„ ìƒíƒœ í™•ì¸"""
    from .comfyui_client import ComfyUIClient
    from .comfyui_workflows import load_image_editing_config

    try:
        config = load_image_editing_config()
        comfyui_config = config.get("comfyui", {})
        base_url = comfyui_config.get("base_url", "http://localhost:8188")

        client = ComfyUIClient(base_url=base_url)
        connected = client.check_connection()

        if connected:
            queue_info = client.get_queue_info()
            return {
                "connected": True,
                "base_url": base_url,
                "queue_info": queue_info,
                "current_model": current_comfyui_model  # í˜„ì¬ ëª¨ë¸ ì •ë³´ ì¶”ê°€
            }
        else:
            return {
                "connected": False,
                "base_url": base_url,
                "error": "ComfyUI ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            }

    except Exception as e:
        return {
            "connected": False,
            "error": str(e)
        }
