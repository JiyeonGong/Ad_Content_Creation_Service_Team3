# services.py (ë¦¬íŒ©í† ë§ ë²„ì „)
"""
AI ì„œë¹„ìŠ¤ ë ˆì´ì–´ - ì„¤ì • ê¸°ë°˜ ëª¨ë¸ ê´€ë¦¬
"""
import os
import io
import logging
from typing import Optional

from openai import OpenAI
import torch
from PIL import Image
from dotenv import load_dotenv

from .model_registry import get_registry
from .model_loader import ModelLoader

logger = logging.getLogger(__name__)

# Load env
load_dotenv(os.path.join(os.path.dirname(__file__), "..", "..", ".env"))

# í† í¬ë‚˜ì´ì € ë³‘ë ¬ ì²˜ë¦¬ ê²½ê³  ì–µì œ
os.environ["TOKENIZERS_PARALLELISM"] = "false"

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
MODEL_GPT_MINI = "gpt-5-mini"

# HF cache location
# /mnt/data4/models ìš°ì„  ì‚¬ìš© (ëª¨ë“  ëª¨ë¸ í†µí•© ì €ì¥ì†Œ)
# GCP: /home/shared ì‚¬ìš©
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
if os.path.exists("/mnt/data4/models"):
    hf_cache_dir = "/mnt/data4/models"
elif os.path.exists("/home/shared"):
    hf_cache_dir = "/home/shared"
else:
    raise RuntimeError("ëª¨ë¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
openai_client: Optional[OpenAI] = None
model_loader: Optional[ModelLoader] = None
registry = get_registry()

# Initialize OpenAI client
if OPENAI_API_KEY:
    try:
        openai_client = OpenAI(api_key=OPENAI_API_KEY)
    except Exception as e:
        logger.warning(f"âš ï¸ OpenAI ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        openai_client = None
else:
    logger.warning("âš ï¸ OPENAI_API_KEY ë¯¸ì„¤ì • â€” GPT ê¸°ëŠ¥ ë¶ˆê°€")

# ===========================
# Utility helpers
# ===========================
def align_to_64(x: int) -> int:
    """64ì˜ ë°°ìˆ˜ë¡œ ì •ë ¬ (ìµœì†Œ 64)"""
    try:
        xi = int(x)
    except Exception:
        xi = 64
    return max(64, (xi // 64) * 64)

def ensure_steps(steps: int) -> int:
    try:
        s = int(steps)
    except Exception:
        s = 1
    return max(1, s)

# ===========================
# ëª¨ë¸ ì´ˆê¸°í™”
# ===========================
def init_image_pipelines():
    """
    ì„¤ì • íŒŒì¼ ê¸°ë°˜ìœ¼ë¡œ ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸ ë¡œë“œ
    """
    global model_loader
    
    # ì´ë¯¸ ë¡œë“œëœ ê²½ìš° ìŠ¤í‚µ
    if model_loader and model_loader.is_loaded():
        print("â„¹ï¸ ì´ë¯¸ì§€ íŒŒì´í”„ë¼ì¸ ì´ë¯¸ ë¡œë“œë¨ â€” ìŠ¤í‚µ")
        return
    
    # ModelLoader ìƒì„±
    if model_loader is None:
        model_loader = ModelLoader(cache_dir=hf_cache_dir)
    
    # í´ë°± ì²´ì¸ìœ¼ë¡œ ë¡œë”© ì‹œë„
    success = model_loader.load_with_fallback()
    
    if success:
        info = model_loader.get_current_model_info()
        logger.info(f"âœ… ì´ë¯¸ì§€ ìƒì„± ì¤€ë¹„ ì™„ë£Œ")
        print(f"   ëª¨ë¸: {info['name']} ({info['type']})")
        print(f"   ì¥ì¹˜: {info['device']}")
    else:
        print("âŒ ëª¨ë“  ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ - ì´ë¯¸ì§€ ìƒì„± ë¶ˆê°€")

# ===========================
# í”„ë¡¬í”„íŠ¸ ìµœì í™”
# ===========================
def optimize_prompt(text: str, model_config) -> str:
    """
    í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­ ë° ìµœì í™”
    ëª¨ë¸ë³„ í† í° ì œí•œ ê³ ë ¤
    """
    if not openai_client:
        return text
    
    # í”„ë¡¬í”„íŠ¸ ìµœì í™” ì„¤ì • í™•ì¸
    opt_config = registry.get_prompt_optimization_config()
    if not opt_config.get("enabled", True):
        return text
    
    # í•œê¸€ ë²ˆì—­ ë¹„í™œì„±í™” ì„¤ì • í™•ì¸
    if not opt_config.get("translate_korean", True):
        return text
    
    try:
        # ëª¨ë¸ë³„ ê¸¸ì´ ì œì•½
        max_tokens = model_config.max_tokens if model_config else 77
        
        if max_tokens <= 77:
            constraint = f"Keep it under 60 words (model has {max_tokens} token limit)."
        else:
            constraint = "Keep it concise but descriptive (under 150 words)."
        
        system_prompt = f"""You are a professional prompt engineer for image generation AI.
Translate Korean marketing text to optimized English prompts.
Focus on visual elements, style, mood, and composition.
{constraint}

IMPORTANT - Quality keywords to prevent AI artifacts:

1. If the scene involves people:
   - Hands: "detailed hands, five fingers, natural hand pose, anatomically correct hands, Hand Position Left Right Proper Position, correct thumb direction"
   - Faces: "detailed face, clear facial features, symmetric face, symmetric eyes, natural eye shape"
   - Body: "correct human anatomy, natural body proportions, well-fitted clothing"

2. If the scene involves objects being held or touched:
   - "proper object interaction, object not clipping through body"
   - "realistic grip, natural holding pose"
   - "physically accurate, no overlapping body parts with objects"

3. If the scene involves fitness/gym/sports equipment:
   - "equipment not penetrating body, proper form"
   - "hands gripping equipment correctly, realistic weight interaction"

Always include relevant keywords based on the scene content.

Output ONLY the English prompt, no explanations."""

        resp = openai_client.responses.create(
            model=MODEL_GPT_MINI,
            input=f"Convert to image prompt:\n{text}",
            reasoning={"effort": "minimal"},
            max_output_tokens=200,
        )
        
        result = getattr(resp, "output_text", None) or str(resp)
        optimized = result.strip()
        logger.info(f"ğŸ”„ í”„ë¡¬í”„íŠ¸ ìµœì í™”:\n  ì›ë³¸: {text}\n  ìµœì í™”: {optimized}")
        return optimized
        
    except Exception as e:
        logger.warning(f"âš ï¸ í”„ë¡¬í”„íŠ¸ ìµœì í™” ì‹¤íŒ¨, ì›ë³¸ ì‚¬ìš©: {e}")
        return text

# ===========================
# GPT-5 Mini: ë¬¸êµ¬ ìƒì„±
# ===========================
def generate_caption_core(info: dict, tone: str) -> str:
    if not openai_client:
        raise RuntimeError("OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    prompt = f"""
ë‹¹ì‹ ì€ í—¬ìŠ¤ì¼€ì–´ ì†Œìƒê³µì¸ì„ ìœ„í•œ ì „ë¬¸ ì¸ìŠ¤íƒ€ê·¸ë¨ ì½˜í…ì¸  í¬ë¦¬ì—ì´í„°ì…ë‹ˆë‹¤.
ì•„ë˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¸ìŠ¤íƒ€ê·¸ë¨ ê²Œì‹œë¬¼ì— ìµœì í™”ëœ ì½˜í…ì¸ ë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”.

ìš”ì²­:
1. ì¸ìŠ¤íƒ€ê·¸ë¨ í™ë³´ ë¬¸êµ¬ 3ê°œ ì‘ì„±
    - ê° ë¬¸êµ¬: í›„í‚¹ â†’ í•µì‹¬ ë©”ì‹œì§€ â†’ CTA
    - ì´ëª¨í‹°ì½˜ ì‚¬ìš©
    - ë¬¸ì²´ ìŠ¤íƒ€ì¼: {tone}
2. í•´ì‹œíƒœê·¸ 15ê°œ ì¶”ì²œ (ì¤‘ë³µ ì œê±°)

[ì •ë³´]
ì„œë¹„ìŠ¤ ì¢…ë¥˜: {info.get('service_type')}
ì„œë¹„ìŠ¤ëª…: {info.get('service_name')}
í•µì‹¬ íŠ¹ì§•: {info.get('features')}
ì§€ì—­: {info.get('location')}

ì¶œë ¥ í˜•ì‹:
ë¬¸êµ¬:
1. [ë¬¸êµ¬1]
2. [ë¬¸êµ¬2]
3. [ë¬¸êµ¬3]

í•´ì‹œíƒœê·¸:
#[íƒœê·¸1] #[íƒœê·¸2] ... #[íƒœê·¸N]
"""
    try:
        resp = openai_client.responses.create(
            model=MODEL_GPT_MINI,
            input=prompt,
            reasoning={"effort": "minimal"},
            max_output_tokens=512,
        )
        text = getattr(resp, "output_text", None) or str(resp)
        return text.strip()
    except Exception as e:
        logger.error(f"ğŸš¨ GPT í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        raise

# ===========================
# ğŸ†• ì´ë¯¸ì§€ ìƒì„± (T2I) - ComfyUI ê¸°ë°˜
# ===========================
def generate_t2i_core(
    prompt: str,
    width: int,
    height: int,
    steps: int,
    guidance_scale: float = None,
    enable_adetailer: bool = True,
    adetailer_targets: list = None,
    post_process_method: str = "none",  # "none", "impact_pack", "adetailer"
    model_name: str = None  # ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„ (ì—†ìœ¼ë©´ í˜„ì¬ ë¡œë“œëœ ëª¨ë¸ ì‚¬ìš©)
) -> bytes:
    """
    ComfyUIë¥¼ ì‚¬ìš©í•œ T2I ì´ë¯¸ì§€ ìƒì„±

    Args:
        post_process_method: í›„ì²˜ë¦¬ ë°©ì‹
            - "none": í›„ì²˜ë¦¬ ì—†ìŒ
            - "impact_pack": ComfyUI Impact Pack (YOLO+SAM)
            - "adetailer": ê¸°ì¡´ ADetailer (YOLO+MediaPipe)
        model_name: ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„ (ì„ íƒì‚¬í•­, ì—†ìœ¼ë©´ í˜„ì¬ ë¡œë“œëœ ëª¨ë¸ ì‚¬ìš©)
    """
    from .comfyui_client import ComfyUIClient
    from .comfyui_workflows import (
        get_flux_t2i_workflow,
        get_flux_t2i_with_impact_workflow,
        update_flux_t2i_workflow,
        load_image_editing_config
    )

    # í˜„ì¬ ë¡œë“œëœ ComfyUI ëª¨ë¸ í™•ì¸
    current_model_name = get_current_comfyui_model()

    # ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ê³ , ìš”ì²­ì— model_nameì´ ìˆìœ¼ë©´ ìë™ ë¡œë“œ
    if not current_model_name and model_name:
        logger.info(f"ğŸ”„ ëª¨ë¸ ìë™ ë¡œë“œ ì‹œì‘: {model_name}")
        # ì „ì—­ ë³€ìˆ˜ ì—…ë°ì´íŠ¸ (ì‹¤ì œ ì›Œí¬í”Œë¡œìš°ì—ì„œ ì‚¬ìš©ë¨)
        global current_comfyui_model
        current_comfyui_model = model_name
        current_model_name = model_name
    elif not current_model_name:
        raise RuntimeError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”.")

    # ëª¨ë¸ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    model_config = registry.get_model(current_model_name)
    if not model_config:
        raise RuntimeError(f"ëª¨ë¸ ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {current_model_name}")

    # í”„ë¡¬í”„íŠ¸ ìµœì í™”
    optimized_prompt = optimize_prompt(prompt, model_config)

    # Steps ê²€ì¦
    if steps < 1:
        steps = model_config.default_steps
    steps = min(steps, model_config.max_steps)

    # Guidance scale ì„¤ì •
    if guidance_scale is None:
        guidance_scale = model_config.guidance_scale

    logger.info(f"ğŸ¨ ComfyUIë¡œ T2I ì´ë¯¸ì§€ ìƒì„± ì¤‘")
    print(f"   ëª¨ë¸: {current_model_name}")
    print(f"   í›„ì²˜ë¦¬: {post_process_method}")
    print(f"   Steps: {steps}")
    print(f"   í¬ê¸°: {width}x{height}")
    print(f"   Guidance: {guidance_scale}")

    try:
        # ComfyUI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        config = load_image_editing_config()
        comfyui_config = config.get("comfyui", {})
        base_url = comfyui_config.get("base_url", "http://localhost:8188")
        timeout = comfyui_config.get("timeout", 300)

        client = ComfyUIClient(base_url=base_url, timeout=timeout)

        # ì›Œí¬í”Œë¡œìš° ì„ íƒ
        if post_process_method == "impact_pack":
            workflow = get_flux_t2i_with_impact_workflow()
        else:
            workflow = get_flux_t2i_workflow()

        # ì›Œí¬í”Œë¡œìš° íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        workflow = update_flux_t2i_workflow(
            workflow=workflow,
            model_name=current_model_name,
            prompt=optimized_prompt,
            width=width,
            height=height,
            steps=steps,
            guidance_scale=guidance_scale
        )

        # ComfyUI ì‹¤í–‰
        output_images, history = client.execute_workflow(workflow=workflow)

        if not output_images:
            raise Exception("ì¶œë ¥ ì´ë¯¸ì§€ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        image_bytes = output_images[0]

        # ê¸°ì¡´ ADetailer í›„ì²˜ë¦¬ (ì„ íƒ ì‹œ)
        if post_process_method == "adetailer" and enable_adetailer:
            image = Image.open(io.BytesIO(image_bytes))
            image = apply_adetailer(
                image=image,
                prompt=optimized_prompt,
                targets=adetailer_targets or ["hand"]
            )

            buf = io.BytesIO()
            image.save(buf, format="PNG")
            image_bytes = buf.getvalue()

        logger.info(f"âœ… ìƒì„± ì™„ë£Œ: {len(image_bytes)} bytes")
        return image_bytes

    except Exception as e:
        logger.error(f"âŒ ComfyUI T2I ìƒì„± ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        raise RuntimeError(f"ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")


# ===========================
# ADetailer í›„ì²˜ë¦¬
# ===========================
def apply_adetailer(
    image: Image,
    prompt: str,
    targets: list = None,
    strength: float = 0.4
) -> Image:
    """
    ADetailer ìŠ¤íƒ€ì¼ í›„ì²˜ë¦¬
    - ì†/ì–¼êµ´ ê°ì§€ í›„ í•´ë‹¹ ì˜ì—­ë§Œ Inpaintë¡œ ì¬ìƒì„±
    """
    global model_loader

    if targets is None:
        targets = ["hand"]

    try:
        from .post_processor import get_post_processor

        logger.info(f"ğŸ”§ ADetailer í›„ì²˜ë¦¬ ì‹œì‘ (targets: {targets})")

        # model_loader ì´ˆê¸°í™” (ADetailerìš©)
        if model_loader is None or not model_loader.is_loaded():
            model_loader = ModelLoader(cache_dir=hf_cache_dir)
            success = model_loader.load_with_fallback()
            if not success:
                logger.warning("âš ï¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ - ADetailer ê±´ë„ˆëœ€")
                return image

        post_processor = get_post_processor()

        # I2I íŒŒì´í”„ë¼ì¸ì„ Inpaintìš©ìœ¼ë¡œ ì‚¬ìš©
        inpaint_pipe = model_loader.i2i_pipe

        # ComfyUI ì‚¬ìš© ì‹œ i2i_pipeê°€ Noneì´ë¯€ë¡œ ADetailer ì‚¬ìš© ë¶ˆê°€
        if inpaint_pipe is None:
            logger.warning("âš ï¸ ComfyUI ì‚¬ìš© ì¤‘ - ADetailerëŠ” Impact Packì„ ì‚¬ìš©í•˜ì„¸ìš”")
            return image

        processed_image, info = post_processor.full_pipeline(
            image=image,
            inpaint_pipeline=inpaint_pipe,
            prompt=prompt,
            auto_detect=True,
            adetailer_targets=targets,
            adetailer_strength=strength
        )

        if not info["processed"]:
            logger.info(f"â„¹ï¸ ADetailer: ì´ìƒ ì—†ìŒ, ì›ë³¸ ìœ ì§€")

        return processed_image

    except Exception as e:
        logger.warning(f"âš ï¸ ADetailer ì‹¤íŒ¨, ì›ë³¸ ë°˜í™˜: {e}")
        return image

# ===========================
# ğŸ†• ì´ë¯¸ì§€ í¸ì§‘ (I2I) - ComfyUI ê¸°ë°˜
# ===========================
def generate_i2i_core(
    input_image_bytes: bytes,
    prompt: str,
    strength: float,
    width: int,
    height: int,
    steps: int,
    guidance_scale: float = None,
    enable_adetailer: bool = False,
    adetailer_targets: list = None,
    post_process_method: str = "none",  # "none", "impact_pack", "adetailer"
    model_name: str = None  # ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„ (ì—†ìœ¼ë©´ í˜„ì¬ ë¡œë“œëœ ëª¨ë¸ ì‚¬ìš©)
) -> bytes:
    """
    ComfyUIë¥¼ ì‚¬ìš©í•œ I2I ì´ë¯¸ì§€ í¸ì§‘

    Args:
        input_image_bytes: ì…ë ¥ ì´ë¯¸ì§€ ë°”ì´íŠ¸
        prompt: í¸ì§‘ í”„ë¡¬í”„íŠ¸
        strength: í¸ì§‘ ê°•ë„ (0.0~1.0)
        width, height: ì¶œë ¥ í¬ê¸°
        steps: ìƒ˜í”Œë§ ìŠ¤í…
        guidance_scale: CFG ìŠ¤ì¼€ì¼
        enable_adetailer: ADetailer í™œì„±í™” ì—¬ë¶€ (legacy)
        adetailer_targets: í›„ì²˜ë¦¬ íƒ€ê²Ÿ
        post_process_method: í›„ì²˜ë¦¬ ë°©ì‹
            - "none": í›„ì²˜ë¦¬ ì—†ìŒ
            - "impact_pack": ComfyUI Impact Pack (YOLO+SAM)
            - "adetailer": ê¸°ì¡´ ADetailer (YOLO+MediaPipe)
        model_name: ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„ (ì„ íƒì‚¬í•­, ì—†ìœ¼ë©´ í˜„ì¬ ë¡œë“œëœ ëª¨ë¸ ì‚¬ìš©)
    """
    from .comfyui_client import ComfyUIClient
    from .comfyui_workflows import (
        get_flux_i2i_workflow,
        update_flux_i2i_workflow,
        load_image_editing_config
    )

    # í˜„ì¬ ë¡œë“œëœ ComfyUI ëª¨ë¸ í™•ì¸
    current_model_name = get_current_comfyui_model()

    # ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ê³ , ìš”ì²­ì— model_nameì´ ìˆìœ¼ë©´ ìë™ ë¡œë“œ
    if not current_model_name and model_name:
        logger.info(f"ğŸ”„ ëª¨ë¸ ìë™ ë¡œë“œ ì‹œì‘: {model_name}")
        # ì „ì—­ ë³€ìˆ˜ ì—…ë°ì´íŠ¸ (ì‹¤ì œ ì›Œí¬í”Œë¡œìš°ì—ì„œ ì‚¬ìš©ë¨)
        global current_comfyui_model
        current_comfyui_model = model_name
        current_model_name = model_name
    elif not current_model_name:
        raise RuntimeError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”.")

    # ëª¨ë¸ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    model_config = registry.get_model(current_model_name)
    if not model_config:
        raise RuntimeError(f"ëª¨ë¸ ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {current_model_name}")

    # í”„ë¡¬í”„íŠ¸ ìµœì í™”
    optimized_prompt = optimize_prompt(prompt, model_config)

    # Steps ê²€ì¦
    if steps < 1:
        steps = model_config.default_steps
    steps = min(steps, model_config.max_steps)

    # Guidance scale ì„¤ì •
    if guidance_scale is None:
        guidance_scale = model_config.guidance_scale

    print(f"âœï¸ ComfyUIë¡œ I2I ì´ë¯¸ì§€ í¸ì§‘ ì¤‘")
    print(f"   ëª¨ë¸: {current_model_name}")
    print(f"   í›„ì²˜ë¦¬: {post_process_method}")
    print(f"   Strength: {strength}")
    print(f"   Steps: {steps}")
    print(f"   í¬ê¸°: {width}x{height}")
    print(f"   Guidance: {guidance_scale}")

    try:
        # ComfyUI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        config = load_image_editing_config()
        comfyui_config = config.get("comfyui", {})
        base_url = comfyui_config.get("base_url", "http://localhost:8188")
        timeout = comfyui_config.get("timeout", 300)

        client = ComfyUIClient(base_url=base_url, timeout=timeout)

        # I2I ì›Œí¬í”Œë¡œìš° ê°€ì ¸ì˜¤ê¸°
        workflow = get_flux_i2i_workflow()

        # ì›Œí¬í”Œë¡œìš° íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        workflow = update_flux_i2i_workflow(
            workflow=workflow,
            model_name=current_model_name,
            prompt=optimized_prompt,
            strength=strength,
            steps=steps,
            guidance_scale=guidance_scale
        )

        # ComfyUI ì‹¤í–‰ (ì…ë ¥ ì´ë¯¸ì§€ í¬í•¨)
        output_images, history = client.execute_workflow(
            workflow=workflow,
            input_image=input_image_bytes,
            input_image_node_id="11"  # LoadImage ë…¸ë“œ ID
        )

        if not output_images:
            raise Exception("ì¶œë ¥ ì´ë¯¸ì§€ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        image_bytes = output_images[0]

        # ê¸°ì¡´ ADetailer í›„ì²˜ë¦¬ (ì„ íƒ ì‹œ)
        if post_process_method == "adetailer" and enable_adetailer:
            image = Image.open(io.BytesIO(image_bytes))
            image = apply_adetailer(
                image=image,
                prompt=optimized_prompt,
                targets=adetailer_targets or ["hand"]
            )

            buf = io.BytesIO()
            image.save(buf, format="PNG")
            image_bytes = buf.getvalue()

        logger.info(f"âœ… í¸ì§‘ ì™„ë£Œ: {len(image_bytes)} bytes")
        return image_bytes

    except Exception as e:
        logger.error(f"âŒ ComfyUI I2I í¸ì§‘ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        raise RuntimeError(f"ì´ë¯¸ì§€ í¸ì§‘ ì‹¤íŒ¨: {e}")

# ===========================
# ëª¨ë¸ ì „í™˜
# ===========================
# ===========================
# ìƒíƒœ ì¡°íšŒ
# ===========================
def get_service_status() -> dict:
    """ì„œë¹„ìŠ¤ ìƒíƒœ ë°˜í™˜"""
    current_model = get_current_comfyui_model()
    status = {
        "gpt_ready": openai_client is not None,
        "image_ready": current_model is not None,
        "current_model": current_model
    }

    return status

# ===========================
# ğŸ†• ì´ë¯¸ì§€ í¸ì§‘ (ComfyUI)
# ===========================
def edit_image_with_comfyui(
    experiment_id: str,
    input_image_bytes: bytes,
    prompt: str,
    negative_prompt: str = "",
    steps: int = None,
    guidance_scale: float = None,
    strength: float = None
) -> dict:
    """
    ComfyUIë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ í¸ì§‘ (BEN2 + ì„ íƒ ëª¨ë¸)

    Args:
        experiment_id: ì‹¤í—˜ ID ("ben2_flux_fill" ë˜ëŠ” "ben2_qwen_image")
        input_image_bytes: ì…ë ¥ ì´ë¯¸ì§€ ë°”ì´íŠ¸
        prompt: í¸ì§‘ í”„ë¡¬í”„íŠ¸
        steps: ì¶”ë¡  ë‹¨ê³„
        guidance_scale: Guidance scale
        strength: ë³€í™” ê°•ë„

    Returns:
        {
            "success": bool,
            "experiment_id": str,
            "experiment_name": str,
            "output_image_base64": str,
            "background_removed_image_base64": str,
            "error": str,
            "elapsed_time": float
        }
    """
    import base64
    import time
    import logging
    from .comfyui_client import ComfyUIClient
    from .comfyui_workflows import (
        get_workflow_template,
        update_workflow_inputs,
        get_workflow_input_image_node_id,
        load_image_editing_config
    )

    logger = logging.getLogger(__name__)
    start_time = time.time()

    try:
        # ì„¤ì • ë¡œë“œ
        config = load_image_editing_config()

        # ì‹¤í—˜ ì •ë³´ ì°¾ê¸°
        experiment = None
        for exp in config.get("experiments", []):
            if exp["id"] == experiment_id:
                experiment = exp
                break

        if not experiment:
            return {
                "success": False,
                "experiment_id": experiment_id,
                "experiment_name": "Unknown",
                "output_image_base64": None,
                "background_removed_image_base64": None,
                "error": f"ì•Œ ìˆ˜ ì—†ëŠ” ì‹¤í—˜ ID: {experiment_id}",
                "elapsed_time": None
            }

        # ComfyUI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        comfyui_config = config.get("comfyui", {})
        base_url = comfyui_config.get("base_url", "http://localhost:8188")
        timeout = comfyui_config.get("timeout", 300)

        client = ComfyUIClient(base_url=base_url, timeout=timeout)

        # ì›Œí¬í”Œë¡œìš° í…œí”Œë¦¿ ê°€ì ¸ì˜¤ê¸°
        workflow = get_workflow_template(experiment_id)

        # ì›Œí¬í”Œë¡œìš° ì—…ë°ì´íŠ¸ (ì‚¬ìš©ì ì…ë ¥ ë°˜ì˜)
        workflow = update_workflow_inputs(
            workflow=workflow,
            experiment_id=experiment_id,
            prompt=prompt,
            negative_prompt=negative_prompt,
            steps=steps,
            guidance_scale=guidance_scale,
            strength=strength
        )

        # ì…ë ¥ ì´ë¯¸ì§€ ë…¸ë“œ ID
        input_node_id = get_workflow_input_image_node_id(experiment_id)

        logger.info(f"ğŸ¨ ComfyUI ì´ë¯¸ì§€ í¸ì§‘ ì‹œì‘")
        logger.info(f"   ì‹¤í—˜: {experiment['name']}")
        logger.info(f"   ë°°ê²½ ì œê±°: {experiment.get('background_removal', {}).get('model', 'N/A')}")
        logger.info(f"   ì´ë¯¸ì§€ í¸ì§‘: {experiment.get('image_editing', {}).get('model', 'N/A')}")
        logger.info(f"   í”„ë¡¬í”„íŠ¸: {prompt}")
        logger.info(f"   íŒŒë¼ë¯¸í„°: steps={steps}, guidance={guidance_scale}, strength={strength}")

        # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        logger.info(f"ğŸ”„ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘...")
        output_images, history = client.execute_workflow(
            workflow=workflow,
            input_image=input_image_bytes,
            input_image_node_id=input_node_id
        )

        if not output_images:
            raise Exception("ì¶œë ¥ ì´ë¯¸ì§€ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ë¥¼ ìµœì¢… ê²°ê³¼ë¡œ ì‚¬ìš©
        output_image_bytes = output_images[0]
        output_image_base64 = base64.b64encode(output_image_bytes).decode("utf-8")

        # ë°°ê²½ ì œê±° ì´ë¯¸ì§€ (ì„ íƒì )
        background_removed_base64 = None
        if len(output_images) > 1:
            background_removed_base64 = base64.b64encode(output_images[1]).decode("utf-8")

        elapsed_time = time.time() - start_time

        logger.info(f"âœ… ComfyUI í¸ì§‘ ì™„ë£Œ! (ì†Œìš” ì‹œê°„: {elapsed_time:.1f}ì´ˆ)")

        return {
            "success": True,
            "experiment_id": experiment_id,
            "experiment_name": experiment["name"],
            "output_image_base64": output_image_base64,
            "background_removed_image_base64": background_removed_base64,
            "error": None,
            "elapsed_time": elapsed_time
        }

    except Exception as e:
        elapsed_time = time.time() - start_time
        error_msg = str(e)
        logger.error(f"âŒ ComfyUI í¸ì§‘ ì‹¤íŒ¨: {error_msg}")

        return {
            "success": False,
            "experiment_id": experiment_id,
            "experiment_name": experiment.get("name", "Unknown") if experiment else "Unknown",
            "output_image_base64": None,
            "background_removed_image_base64": None,
            "error": error_msg,
            "elapsed_time": elapsed_time
        }


def get_image_editing_experiments() -> dict:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ì´ë¯¸ì§€ í¸ì§‘ ì‹¤í—˜ ëª©ë¡ ë°˜í™˜ (ìƒì„± ëª¨ë¸ + í¸ì§‘ ëª¨ë¸)"""
    from .comfyui_workflows import load_image_editing_config

    try:
        config = load_image_editing_config()
        experiments = config.get("experiments", [])

        # í¸ì§‘ ì‹¤í—˜ ëª©ë¡
        editing_experiments = [
            {
                "id": exp["id"],
                "name": exp["name"],
                "description": exp["description"],
                "background_removal_model": exp["background_removal"]["model"],
                "editing_model": exp["image_editing"]["model"],
                "features": exp.get("features", [])  # ëª¨ë¸ë³„ ê¸°ëŠ¥ ëª©ë¡ í¬í•¨
            }
            for exp in experiments
        ]

        # ìƒì„± ëª¨ë¸ ëª©ë¡ ì¶”ê°€
        generation_models = [
            {
                "id": "FLUX.1-dev-Q8",
                "name": "FLUX.1-dev Q8",
                "description": "FLUX.1-dev GGUF 8-bit ì–‘ìí™” (ì´ë¯¸ì§€ ìƒì„±, ê¶Œì¥)"
            },
            {
                "id": "FLUX.1-dev-Q4",
                "name": "FLUX.1-dev Q4",
                "description": "FLUX.1-dev GGUF 4-bit ì–‘ìí™” (ë©”ëª¨ë¦¬ ì ˆì•½)"
            }
        ]

        # ìƒì„± ëª¨ë¸ + í¸ì§‘ ëª¨ë¸ ëª¨ë‘ ë°˜í™˜
        all_experiments = generation_models + editing_experiments

        return {
            "success": True,
            "experiments": all_experiments
        }

    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "experiments": []
        }


# ===========================
# ğŸ†• ComfyUI ëª¨ë¸ ê´€ë¦¬ (í”„ë¦¬ë¡œë”©/ì–¸ë¡œë“œ)
# ===========================
current_comfyui_model: Optional[str] = None

# í”„ë¦¬ë¡œë“œ ê¸°ëŠ¥ ì œê±°ë¨
def _removed_preload_model_in_comfyui(experiment_id: str) -> dict:
    """
    ComfyUIì— ëª¨ë¸ ë¯¸ë¦¬ ë¡œë“œ (ìµœì†Œ ì‹¤í–‰ ì›Œí¬í”Œë¡œìš° ì „ì†¡)
    """
    global current_comfyui_model
    
    # ì´ë¯¸ ë¡œë“œëœ ëª¨ë¸ì´ë©´ ìŠ¤í‚µ
    if current_comfyui_model == experiment_id:
        return {"success": True, "message": "ì´ë¯¸ ë¡œë“œëœ ëª¨ë¸ì…ë‹ˆë‹¤.", "model": experiment_id}

    from .comfyui_client import ComfyUIClient
    from .comfyui_workflows import _removed_get_preload_workflow, load_image_editing_config, get_workflow_input_image_node_id
    import io
    from PIL import Image
    
    try:
        config = load_image_editing_config()
        comfyui_config = config.get("comfyui", {})
        base_url = comfyui_config.get("base_url", "http://localhost:8188")
        
        client = ComfyUIClient(base_url=base_url)
        
        # 1. ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„± (64x64 ê²€ì€ìƒ‰)
        dummy_image = Image.new('RGB', (64, 64), color='black')
        img_byte_arr = io.BytesIO()
        dummy_image.save(img_byte_arr, format='PNG')
        img_byte_arr.seek(0)
        
        # 2. ì´ë¯¸ì§€ ì—…ë¡œë“œ
        filename = "preload_dummy.png"
        upload_resp = client.upload_image(img_byte_arr.read(), filename)
        if not upload_resp:
            return {"success": False, "message": "ë”ë¯¸ ì´ë¯¸ì§€ ì—…ë¡œë“œ ì‹¤íŒ¨"}
        
        # 3. í”„ë¦¬ë¡œë”© ì›Œí¬í”Œë¡œìš° ìƒì„±
        workflow = _removed_get_preload_workflow(experiment_id)
        if not workflow:
            return {"success": False, "message": "í”„ë¦¬ë¡œë”© ì›Œí¬í”Œë¡œìš° ìƒì„± ì‹¤íŒ¨"}
        
        # 4. ì…ë ¥ ì´ë¯¸ì§€ ë…¸ë“œ ì„¤ì •
        input_node_id = get_workflow_input_image_node_id(experiment_id)
        if input_node_id in workflow:
            workflow[input_node_id]["inputs"]["image"] = filename
            
        # 5. íì— ì „ì†¡
        logger.info(f"ğŸš€ ëª¨ë¸ í”„ë¦¬ë¡œë”© ì‹œì‘: {experiment_id}")
        client.queue_prompt(workflow)
        
        # 6. ìƒíƒœ ì—…ë°ì´íŠ¸
        current_comfyui_model = experiment_id
        
        return {"success": True, "message": "ëª¨ë¸ ë¡œë”© ìš”ì²­ ì™„ë£Œ", "model": experiment_id}
        
    except Exception as e:
        error_msg = str(e)
        # prompt_no_outputs ì—ëŸ¬ëŠ” ëª…í™•í•˜ê²Œ ì‹¤íŒ¨ë¡œ ì²˜ë¦¬
        if "prompt_no_outputs" in error_msg.lower():
            logger.error(f"âŒ ì›Œí¬í”Œë¡œìš°ì— ì¶œë ¥ ë…¸ë“œê°€ ì—†ì–´ ì‹¤í–‰ ë¶ˆê°€")
            return {"success": False, "message": "ì›Œí¬í”Œë¡œìš° êµ¬ì„± ì˜¤ë¥˜ (ì¶œë ¥ ë…¸ë“œ í•„ìš”)"}
        
        logger.error(f"âŒ ëª¨ë¸ í”„ë¦¬ë¡œë”© ì‹¤íŒ¨: {e}")
        return {"success": False, "message": str(e)}

def unload_comfyui_model() -> dict:
    """ComfyUI ëª¨ë¸ ì–¸ë¡œë“œ ë° ë©”ëª¨ë¦¬ í•´ì œ"""
    global current_comfyui_model
    
    from .comfyui_client import ComfyUIClient
    from .comfyui_workflows import load_image_editing_config
    
    try:
        config = load_image_editing_config()
        base_url = config.get("comfyui", {}).get("base_url", "http://localhost:8188")
        
        client = ComfyUIClient(base_url=base_url)
        
        # ë©”ëª¨ë¦¬ í•´ì œ ìš”ì²­
        success = client.free_memory(unload_models=True, free_memory=True)
        
        if success:
            current_comfyui_model = None
            return {"success": True, "message": "ëª¨ë¸ ì–¸ë¡œë“œ ë° ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ"}
        else:
            return {"success": False, "message": "ë©”ëª¨ë¦¬ í•´ì œ ìš”ì²­ ì‹¤íŒ¨"}
            
    except Exception as e:
        return {"success": False, "message": str(e)}

def get_current_comfyui_model() -> Optional[str]:
    """í˜„ì¬ ë¡œë“œëœ ComfyUI ëª¨ë¸ ID ë°˜í™˜"""
    return current_comfyui_model

def check_comfyui_status() -> dict:
    """ComfyUI ì„œë²„ ìƒíƒœ í™•ì¸"""
    from .comfyui_client import ComfyUIClient
    from .comfyui_workflows import load_image_editing_config

    try:
        config = load_image_editing_config()
        comfyui_config = config.get("comfyui", {})
        base_url = comfyui_config.get("base_url", "http://localhost:8188")

        client = ComfyUIClient(base_url=base_url)
        connected = client.check_connection()

        if connected:
            queue_info = client.get_queue_info()
            return {
                "connected": True,
                "base_url": base_url,
                "queue_info": queue_info,
                "current_model": current_comfyui_model  # í˜„ì¬ ëª¨ë¸ ì •ë³´ ì¶”ê°€
            }
        else:
            return {
                "connected": False,
                "base_url": base_url,
                "error": "ComfyUI ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            }

    except Exception as e:
        return {
            "connected": False,
            "error": str(e)
        }
