# src/backend/services.py
# ============================================================
# âš™ï¸ AI í•µì‹¬ ì¶”ë¡  ì—”ì§„ (Web-Independent)
# - GPT-5 Mini í˜¸ì¶œ (OpenAI API)
# - SDXL T2I/I2I ë¡œì»¬ ì¶”ë¡  (Diffusers)
# - ì´ íŒŒì¼ì€ FastAPIì— ì˜ì¡´í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
# ============================================================

import os
import io
import base64
from typing import Dict, Any, List

# AI ëª¨ë¸ ë° ì´ë¯¸ì§€ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from openai import OpenAI
from diffusers import StableDiffusionXLPipeline, StableDiffusionXLImg2ImgPipeline
import torch
from PIL import Image
from dotenv import load_dotenv

# ============================================================
# ğŸŒ í™˜ê²½ ì„¤ì • ë° í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
# ============================================================

# .env íŒŒì¼ ë¡œë”© (í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€)
load_dotenv(os.path.join(os.path.dirname(__file__), "..", "..", ".env"))

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
MODEL_GPT_MINI = "gpt-5-mini"

# Hugging Face ëª¨ë¸ ìºì‹œ ê²½ë¡œ ì„¤ì •
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
hf_cache_dir = os.path.join(project_root, "cache", "hf_models")
os.makedirs(hf_cache_dir, exist_ok=True)

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
openai_client = None
if OPENAI_API_KEY:
    try:
        openai_client = OpenAI(api_key=OPENAI_API_KEY)
    except Exception as e:
        print(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
else:
    print("âš ï¸ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ GPT ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# SDXL ëª¨ë¸ ë³€ìˆ˜ (ì´ˆê¸°í™”ëŠ” init í•¨ìˆ˜ì—ì„œ ìˆ˜í–‰)
T2I_PIPE = None
I2I_PIPE = None
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
MODEL_ID = "stabilityai/stable-diffusion-xl-base-1.0"

def init_sdxl_pipelines():
    """SDXL T2I ë° I2I íŒŒì´í”„ë¼ì¸ì„ ì´ˆê¸°í™”í•˜ê³  ì „ì—­ ë³€ìˆ˜ì— ì €ì¥í•©ë‹ˆë‹¤."""
    global T2I_PIPE, I2I_PIPE
    try:
        print(f"SDXL ëª¨ë¸ ë¡œë”© ì¤‘... (Device: {DEVICE}, Cache: {hf_cache_dir})")
        dtype = torch.float16 if torch.cuda.is_available() else torch.float32

        # T2I (Text-to-Image) íŒŒì´í”„ë¼ì¸
        T2I_PIPE = StableDiffusionXLPipeline.from_pretrained(
            MODEL_ID,
            cache_dir=hf_cache_dir,
            torch_dtype=dtype
        ).to(DEVICE)
        
        # I2I (Image-to-Image) íŒŒì´í”„ë¼ì¸
        I2I_PIPE = StableDiffusionXLImg2ImgPipeline.from_pretrained(
            MODEL_ID,
            cache_dir=hf_cache_dir,
            torch_dtype=dtype
        ).to(DEVICE)
        
        print("SDXL ëª¨ë¸ ë¡œë”© ì™„ë£Œ.")
    except Exception as e:
        print(f"SDXL ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        T2I_PIPE = None
        I2I_PIPE = None

# ============================================================
# ğŸ¯ AI ì¶”ë¡  í•¨ìˆ˜ (í•µì‹¬ ë¡œì§)
# ============================================================

def generate_caption_core(info: dict, tone: str) -> str:
    """GPT-5 Minië¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸êµ¬ ë° í•´ì‹œíƒœê·¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    if not openai_client:
        raise ValueError("OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
    prompt = f"""
ë‹¹ì‹ ì€ í—¬ìŠ¤ì¼€ì–´ ì†Œìƒê³µì¸ì„ ìœ„í•œ ì „ë¬¸ ì¸ìŠ¤íƒ€ê·¸ë¨ ì½˜í…ì¸  í¬ë¦¬ì—ì´í„°ì…ë‹ˆë‹¤.
ì•„ë˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¸ìŠ¤íƒ€ê·¸ë¨ ê²Œì‹œë¬¼ì— ìµœì í™”ëœ ì½˜í…ì¸ ë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”.

ìš”ì²­:
1. ì¸ìŠ¤íƒ€ê·¸ë¨ í™ë³´ ë¬¸êµ¬ 3ê°œ ì‘ì„±
    - ê° ë¬¸êµ¬: í›„í‚¹ â†’ í•µì‹¬ ë©”ì‹œì§€ â†’ CTA
    - ì´ëª¨í‹°ì½˜ ì‚¬ìš©
    - ë¬¸ì²´ ìŠ¤íƒ€ì¼: {tone}
2. í•´ì‹œíƒœê·¸ 15ê°œ ì¶”ì²œ (ì¤‘ë³µ ì œê±°)

[ì •ë³´]
ì„œë¹„ìŠ¤ ì¢…ë¥˜: {info['service_type']}
ì„œë¹„ìŠ¤ëª…: {info['service_name']}
í•µì‹¬ íŠ¹ì§•: {info['features']}
ì§€ì—­: {info['location']}
ì´ë²¤íŠ¸: ì—†ìŒ

ì¶œë ¥ í˜•ì‹:
ë¬¸êµ¬:
1. [ë¬¸êµ¬1]
2. [ë¬¸êµ¬2]
3. [ë¬¸êµ¬3]

í•´ì‹œíƒœê·¸:
#[íƒœê·¸1] #[íƒœê·¸2] ... #[íƒœê·¸N]
"""
    try:
        response = openai_client.responses.create(
            model=MODEL_GPT_MINI,
            input=prompt,
            reasoning={"effort":"minimal"},
            max_output_tokens=512, 
        )
        return response.output_text.strip()
    except Exception as e:
        print(f"GPT-5 Mini í˜¸ì¶œ ì˜¤ë¥˜: {e}")
        raise

def generate_t2i_core(prompt: str, width: int, height: int, steps: int) -> bytes:
    """SDXL T2Ië¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ê³  PNG ë°”ì´íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if not T2I_PIPE:
        raise ValueError("T2I íŒŒì´í”„ë¼ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    negative_prompt = "low quality, blurry, text, watermark, distorted, ugly, tiling, poorly drawn"
    
    result = T2I_PIPE(
        prompt=prompt, 
        negative_prompt=negative_prompt, 
        width=width, 
        height=height, 
        num_inference_steps=steps,
        guidance_scale=7.5 # ê¸°ë³¸ê°’ ì‚¬ìš©
    )
    image = result.images[0]
    
    # BytesIOë¡œ ë³€í™˜ í›„ ë°˜í™˜
    buf = io.BytesIO()
    image.save(buf, format="PNG")
    return buf.getvalue()

def generate_i2i_core(input_image_bytes: bytes, prompt: str, strength: float, width: int, height: int, steps: int) -> bytes:
    """SDXL I2Ië¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ í¸ì§‘/í•©ì„±í•˜ê³  PNG ë°”ì´íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if not I2I_PIPE:
        raise ValueError("I2I íŒŒì´í”„ë¼ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    negative_prompt = "low quality, blurry, text, watermark, distorted, ugly, tiling, poorly drawn"
    
    # 1. ì›ë³¸ ì´ë¯¸ì§€ ë¡œë“œ ë° ë¦¬ì‚¬ì´ì¦ˆ
    input_image = Image.open(io.BytesIO(input_image_bytes)).convert("RGB").resize((width, height))
    
    # 2. I2I íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    result = I2I_PIPE(
        prompt=prompt, 
        image=input_image,
        strength=strength,
        negative_prompt=negative_prompt, 
        num_inference_steps=steps,
        guidance_scale=7.5
    )
    image = result.images[0]
    
    # 3. BytesIOë¡œ ë³€í™˜ í›„ ë°˜í™˜
    buf = io.BytesIO()
    image.save(buf, format="PNG")
    return buf.getvalue()