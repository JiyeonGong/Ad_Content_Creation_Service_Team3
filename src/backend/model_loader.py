# model_loader.py
"""
ëª¨ë¸ ë¡œë” - ì„¤ì • ê¸°ë°˜ ëª¨ë¸ ë¡œë”© ë° ê´€ë¦¬
"""
import os
import traceback
from typing import Optional, Tuple, Any
import torch
from diffusers import (
    DiffusionPipeline,
    StableDiffusionXLPipeline,
    StableDiffusionXLImg2ImgPipeline,
    AutoPipelineForImage2Image,
    FluxTransformer2DModel,
    BitsAndBytesConfig
)

from .model_registry import ModelConfig, get_registry


class ModelLoader:
    """ëª¨ë¸ ë¡œë”© ë° ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, cache_dir: str, use_bfloat16: bool = True):
        self.cache_dir = cache_dir
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

        # FLUXëŠ” bfloat16 ê¶Œì¥ (ai-ad ë°©ì‹)
        if use_bfloat16 and self.device == "cuda":
            self.dtype = torch.bfloat16
        else:
            self.dtype = torch.float16 if self.device == "cuda" else torch.float32

        self.t2i_pipe = None
        self.i2i_pipe = None
        self.current_model_name = None
        self.current_model_config: Optional[ModelConfig] = None

        self.registry = get_registry()

        print(f"ğŸ”§ ModelLoader ì´ˆê¸°í™” (Device: {self.device}, dtype: {self.dtype}, Cache: {cache_dir})")
    
    def is_loaded(self) -> bool:
        """ëª¨ë¸ ë¡œë“œ ì—¬ë¶€ í™•ì¸"""
        return self.t2i_pipe is not None
    
    def get_current_model_info(self) -> dict:
        """í˜„ì¬ ë¡œë“œëœ ëª¨ë¸ ì •ë³´"""
        if not self.current_model_config:
            return {"loaded": False}
        
        return {
            "loaded": True,
            "name": self.current_model_name,
            "id": self.current_model_config.id,
            "type": self.current_model_config.type,
            "device": self.device,
            "description": self.current_model_config.description
        }
    
    def _apply_memory_optimizations(self, pipe, model_type: str, pipe_name: str = "", is_quantized: bool = False):
        """ë©”ëª¨ë¦¬ ìµœì í™” ë° ì†ë„ ìµœì í™” ì ìš©"""
        memory_config = self.registry.get_memory_config()

        # íŒŒì´í”„ë¼ì¸ ì´ë¦„ í‘œì‹œ (T2I/I2I êµ¬ë¶„)
        prefix = f"[{pipe_name}] " if pipe_name else "  "

        # CPU offload ì„¤ì • (ì–‘ìí™” ì‚¬ìš© ì‹œ ìë™ ë¹„í™œì„±í™”)
        if memory_config.get("enable_cpu_offload", False) and not is_quantized:
            try:
                # FLUX ëª¨ë¸ì€ sequential offload ì‚¬ìš© (ë” ê³µê²©ì ì¸ ë©”ëª¨ë¦¬ ì ˆì•½)
                if model_type == "flux":
                    pipe.enable_sequential_cpu_offload()
                    print(f"{prefix}âœ“ Sequential CPU ì˜¤í”„ë¡œë“œ í™œì„±í™” (FLUX ì „ìš©)")
                else:
                    pipe.enable_model_cpu_offload()
                    print(f"{prefix}âœ“ Model CPU ì˜¤í”„ë¡œë“œ í™œì„±í™”")
            except Exception as e:
                print(f"{prefix}âš ï¸ CPU offload ì‹¤íŒ¨: {e}")

        # VAE Tiling (ê³ í•´ìƒë„ ì²˜ë¦¬)
        if hasattr(pipe, 'vae'):
            try:
                pipe.vae.enable_tiling()
                print(f"{prefix}âœ“ VAE Tiling í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½, ì†ë„ ì˜í–¥ ì—†ìŒ)")
            except:
                pass

        # VAE Slicing (ë°°ì¹˜ ì²˜ë¦¬)
        if memory_config.get("enable_vae_slicing", False):
            if hasattr(pipe, 'vae'):
                try:
                    pipe.vae.enable_slicing()
                    print(f"{prefix}âœ“ VAE ìŠ¬ë¼ì´ì‹± í™œì„±í™”")
                except:
                    pass

        # Attention Slicing (ì„ íƒì )
        if memory_config.get("enable_attention_slicing", False):
            try:
                pipe.enable_attention_slicing()
                print(f"{prefix}âœ“ ì–´í…ì…˜ ìŠ¬ë¼ì´ì‹± í™œì„±í™”")
            except:
                pass

        # Flash Attention 2 ì ìš© (ì–‘ìí™”ì™€ í•¨ê»˜ ì‚¬ìš© ì‹œ ì¶”ê°€ ì†ë„ ê°œì„ )
        if memory_config.get("use_flash_attention", False):
            try:
                if hasattr(pipe, 'transformer') and hasattr(pipe.transformer, 'enable_flash_attention_2'):
                    pipe.transformer.enable_flash_attention_2()
                    print(f"{prefix}âœ“ Flash Attention 2 í™œì„±í™”")
                else:
                    print(f"{prefix}â„¹ï¸ Flash Attention 2 ë¯¸ì§€ì› ëª¨ë¸")
            except ImportError:
                print(f"{prefix}âš ï¸ Flash Attention 2 ë¯¸ì„¤ì¹˜, ê¸°ë³¸ attention ì‚¬ìš©")
            except Exception as e:
                print(f"{prefix}âš ï¸ Flash Attention 2 ì ìš© ì‹¤íŒ¨: {e}")

        return pipe
    
    def _load_model_by_type(self, model_config: ModelConfig) -> Tuple[Any, Any]:
        """ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ íŒŒì´í”„ë¼ì¸ ë¡œë“œ"""
        model_id = model_config.id
        model_type = model_config.type.lower()
        memory_config = self.registry.get_memory_config()

        print(f"  ğŸ“¦ íƒ€ì…: {model_type}")

        # ê¸°ë³¸ ë¡œë”© ì˜µì…˜
        load_kwargs = {
            "cache_dir": self.cache_dir,
            "torch_dtype": self.dtype
        }

        # ì‚¬ì „ ì–‘ìí™” ëª¨ë¸ ì²´í¬ (ëª¨ë¸ IDì— "int8", "fp8", "nf4" í¬í•¨ ì‹œ)
        is_prequantized = any(keyword in model_id.lower() for keyword in ["int8", "fp8", "nf4", "gguf"])

        if is_prequantized:
            print("  âœ… ì‚¬ì „ ì–‘ìí™” ëª¨ë¸ ê°ì§€ - ë°”ë¡œ ë¡œë“œ (ì–‘ìí™” ê³¼ì • ìƒëµ)")
            use_quantization = False
        else:
            # ì–‘ìí™” ì„¤ì • (FLUXì—ë§Œ ì ìš©)
            quant_type = memory_config.get("quantization_type", "none").lower()
            use_quantization = quant_type in ["fp8", "nf4"] and model_type == "flux"

            if use_quantization:
                if quant_type == "fp8":
                    print("  ğŸš€ FP8 ì–‘ìí™” ëª¨ë“œ í™œì„±í™” (22GB â†’ 12GB, í’ˆì§ˆ 99%+, 2-2.6ë°° ì†ë„)")
                elif quant_type == "nf4":
                    print("  ğŸš€ NF4 ì–‘ìí™” ëª¨ë“œ í™œì„±í™” (22GB â†’ 12GB, í’ˆì§ˆ 98%, 3-4ë°° ì†ë„)")
            elif memory_config.get("use_8bit", False):
                load_kwargs["load_in_8bit"] = True
                print("  âœ“ 8-bit ì–‘ìí™” ëª¨ë“œ (deprecated)")

        # ëª¨ë¸ íƒ€ì…ë³„ ë¡œë”©
        if model_type == "flux-bnb-4bit":
            # ì‚¬ì „ ì–‘ìí™” 4-bit ëª¨ë¸ (diffusers/FLUX.1-dev-bnb-4bit)
            from diffusers import FluxPipeline
            print("  ğŸ“¥ ì‚¬ì „ ì–‘ìí™” 4-bit ëª¨ë¸ (bitsandbytes) ë¡œë”© ì¤‘...")
            print("  âš ï¸ ì²« ë¡œë“œ ì‹œ ë‹¤ìš´ë¡œë“œì— ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

            t2i = FluxPipeline.from_pretrained(
                model_id,
                torch_dtype=self.dtype,
                cache_dir=self.cache_dir
            )
            t2i = t2i.to(self.device)
            print("  âœ“ ì‚¬ì „ ì–‘ìí™” 4-bit ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

            # GPU ë©”ëª¨ë¦¬ í™•ì¸
            if torch.cuda.is_available():
                allocated = torch.cuda.memory_allocated() / 1024**3
                print(f"  ğŸ“Š GPU ë©”ëª¨ë¦¬: {allocated:.2f} GB")

            # I2I íŒŒì´í”„ë¼ì¸
            try:
                i2i = AutoPipelineForImage2Image.from_pipe(t2i)
            except:
                i2i = t2i
                print("  âš ï¸ I2I íŒŒì´í”„ë¼ì¸ ê³µìœ ")

        elif model_type == "flux-bnb-8bit":
            # ì‚¬ì „ ì–‘ìí™” 8-bit ëª¨ë¸ (diffusers/FLUX.1-dev-bnb-8bit)
            from diffusers import FluxPipeline
            print("  ğŸ“¥ ì‚¬ì „ ì–‘ìí™” 8-bit ëª¨ë¸ (bitsandbytes) ë¡œë”© ì¤‘...")
            print("  âš ï¸ ì²« ë¡œë“œ ì‹œ ë‹¤ìš´ë¡œë“œì— ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

            t2i = FluxPipeline.from_pretrained(
                model_id,
                torch_dtype=self.dtype,
                cache_dir=self.cache_dir
            )
            print("  âœ“ ì‚¬ì „ ì–‘ìí™” 8-bit ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

            # GPU ë©”ëª¨ë¦¬ í™•ì¸
            if torch.cuda.is_available():
                allocated = torch.cuda.memory_allocated() / 1024**3
                print(f"  ğŸ“Š GPU ë©”ëª¨ë¦¬: {allocated:.2f} GB")

            # I2I íŒŒì´í”„ë¼ì¸
            try:
                i2i = AutoPipelineForImage2Image.from_pipe(t2i)
            except:
                i2i = t2i
                print("  âš ï¸ I2I íŒŒì´í”„ë¼ì¸ ê³µìœ ")

        elif model_type == "flux-fp8-pretrained":
            # ì‚¬ì „ ì–‘ìí™” FP8 ëª¨ë¸ (diffusers/FLUX.1-dev-torchao-fp8)
            # torchao ë²„ì „ í˜¸í™˜ ë¬¸ì œë¡œ ì‚¬ìš© ë¶ˆê°€
            from diffusers import FluxPipeline
            print("  ğŸ“¥ ì‚¬ì „ ì–‘ìí™” FP8 ëª¨ë¸ ë¡œë”© ì¤‘...")
            print("  âš ï¸ ì²« ë¡œë“œ ì‹œ ë‹¤ìš´ë¡œë“œì— ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

            t2i = FluxPipeline.from_pretrained(
                model_id,
                torch_dtype=self.dtype,
                use_safetensors=False,
                device_map="cuda:0",
                cache_dir=self.cache_dir
            )
            print("  âœ“ ì‚¬ì „ ì–‘ìí™” FP8 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

            # GPU ë©”ëª¨ë¦¬ í™•ì¸
            if torch.cuda.is_available():
                allocated = torch.cuda.memory_allocated() / 1024**3
                print(f"  ğŸ“Š GPU ë©”ëª¨ë¦¬: {allocated:.2f} GB")

            # I2I íŒŒì´í”„ë¼ì¸
            try:
                i2i = AutoPipelineForImage2Image.from_pipe(t2i)
            except:
                i2i = t2i
                print("  âš ï¸ I2I íŒŒì´í”„ë¼ì¸ ê³µìœ ")

        elif model_type == "flux":
            # FLUX ê³„ì—´: FP8 / NF4 ì–‘ìí™” ì§€ì›
            if use_quantization:
                try:
                    if quant_type == "fp8":
                        # FP8 ì–‘ìí™” (TorchAO)
                        # Transformerë§Œ ì–‘ìí™”, ë‚˜ë¨¸ì§€ëŠ” ì›ë³¸
                        print("  ğŸ“¥ FP8 Transformer ë¡œë”© ì¤‘...")
                        from torchao.quantization import quantize_
                        from torchao.quantization.quant_api import Float8WeightOnlyConfig

                        # Transformer ë¡œë“œ í›„ ì–‘ìí™”
                        transformer = FluxTransformer2DModel.from_pretrained(
                            model_id,
                            subfolder="transformer",
                            torch_dtype=self.dtype,
                            cache_dir=self.cache_dir
                        )

                        # ì–‘ìí™” ì „ ëª¨ë¸ í¬ê¸° í™•ì¸
                        param_size_before = sum(p.numel() * p.element_size() for p in transformer.parameters()) / 1024**3
                        print(f"  ğŸ“Š ì–‘ìí™” ì „ Transformer í¬ê¸°: {param_size_before:.2f} GB")

                        # FP8 ì–‘ìí™” ì ìš© (Float8WeightOnlyConfig ì‚¬ìš©)
                        print("  ğŸ”„ FP8 ì–‘ìí™” ì ìš© ì¤‘...")
                        quantize_(transformer, Float8WeightOnlyConfig())

                        # ì–‘ìí™” í›„ ëª¨ë¸ í¬ê¸° í™•ì¸
                        param_size_after = sum(p.numel() * p.element_size() for p in transformer.parameters()) / 1024**3
                        print(f"  ğŸ“Š ì–‘ìí™” í›„ Transformer í¬ê¸°: {param_size_after:.2f} GB")

                        # ì–‘ìí™” ì„±ê³µ ì—¬ë¶€ í™•ì¸
                        if param_size_after < param_size_before * 0.7:
                            print(f"  âœ“ FP8 ì–‘ìí™” ì„±ê³µ (í¬ê¸° {param_size_before:.2f}GB â†’ {param_size_after:.2f}GB)")
                        else:
                            print(f"  âš ï¸ FP8 ì–‘ìí™” ì‹¤íŒ¨ ë˜ëŠ” ë¯¸ì ìš© (í¬ê¸° ë³€í™” ì—†ìŒ)")
                            raise RuntimeError("FP8 ì–‘ìí™”ê°€ ì ìš©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

                        # ì „ì²´ íŒŒì´í”„ë¼ì¸ êµ¬ì„±
                        print("  ğŸ”§ íŒŒì´í”„ë¼ì¸ êµ¬ì„± ì¤‘...")
                        t2i = DiffusionPipeline.from_pretrained(
                            model_id,
                            transformer=transformer,
                            torch_dtype=self.dtype,
                            cache_dir=self.cache_dir
                        )

                        # GPUë¡œ ì´ë™
                        t2i = t2i.to(self.device)
                        print(f"  âœ“ FP8 ëª¨ë¸ì„ {self.device}ë¡œ ì´ë™")

                        # ìµœì¢… GPU ë©”ëª¨ë¦¬ í™•ì¸
                        if torch.cuda.is_available():
                            allocated = torch.cuda.memory_allocated() / 1024**3
                            print(f"  ğŸ“Š ì „ì²´ GPU ë©”ëª¨ë¦¬: {allocated:.2f} GB")

                    elif quant_type == "nf4":
                        # NF4 ì–‘ìí™” (BitsAndBytes)
                        # âš ï¸ NF4 ì–‘ìí™” ëª¨ë¸ì€ ì €ì¥/ë¡œë“œ ë³µì¡ - ë§¤ë²ˆ ì–‘ìí™” ìˆ˜í–‰
                        print("  ğŸ“¥ NF4 Transformer ë¡œë”© ì¤‘...")
                        nf4_config = BitsAndBytesConfig(
                            load_in_4bit=True,
                            bnb_4bit_quant_type="nf4",
                            bnb_4bit_compute_dtype=self.dtype
                        )

                        # Transformerë§Œ ì–‘ìí™” ë¡œë“œ
                        transformer = FluxTransformer2DModel.from_pretrained(
                            model_id,
                            subfolder="transformer",
                            quantization_config=nf4_config,
                            torch_dtype=self.dtype,
                            cache_dir=self.cache_dir
                        )
                        print("  âœ“ NF4 ì–‘ìí™” ë¡œë“œ ì™„ë£Œ")

                        # ì „ì²´ íŒŒì´í”„ë¼ì¸ êµ¬ì„±
                        print("  ğŸ”§ íŒŒì´í”„ë¼ì¸ êµ¬ì„± ì¤‘...")
                        t2i = DiffusionPipeline.from_pretrained(
                            model_id,
                            transformer=transformer,
                            torch_dtype=self.dtype,
                            cache_dir=self.cache_dir
                        )
                        t2i = t2i.to(self.device)
                        print(f"  âœ“ ì–‘ìí™”ëœ ëª¨ë¸ì„ {self.device}ë¡œ ì´ë™")

                    print(f"  âœ… {quant_type.upper()} ì–‘ìí™” ë¡œë”© ì™„ë£Œ")

                except Exception as e:
                    print(f"  âš ï¸ {quant_type.upper()} ë¡œë”© ì‹¤íŒ¨: {e}")
                    print(f"  ğŸ”„ CPU offload ëª¨ë“œë¡œ í´ë°± ì‹œë„...")
                    use_quantization = False
                    # í´ë°±: CPU offload ëª¨ë“œ (CPU 16GBë¡œëŠ” ë¶„ì‚°ë¡œë”© ë¶ˆê°€)
                    # enable_sequential_cpu_offload ë°©ì‹ ì‚¬ìš©
                    t2i = DiffusionPipeline.from_pretrained(
                        model_id,
                        torch_dtype=self.dtype,
                        cache_dir=self.cache_dir
                    )
                    t2i.enable_sequential_cpu_offload()
                    print(f"  âœ“ Sequential CPU offload ì ìš© (ëŠë¦¬ì§€ë§Œ ë©”ëª¨ë¦¬ ì•ˆì •ì )")
            else:
                # ì¼ë°˜ FLUX ë¡œë”© (ì–‘ìí™” ë¯¸ì‚¬ìš©)
                # device_map="balanced"ë¡œ GPU ìš°ì„ , ë„˜ì¹˜ë©´ CPU ë¶„ì‚°
                load_kwargs["device_map"] = "balanced"
                t2i = DiffusionPipeline.from_pretrained(model_id, **load_kwargs)
                print(f"  âœ“ device_map='balanced' ì ìš© (GPU ìš°ì„ , ë„˜ì¹˜ë©´ CPU ë¶„ì‚°)")

            # I2I íŒŒì´í”„ë¼ì¸ ìƒì„± ì‹œë„
            try:
                i2i = AutoPipelineForImage2Image.from_pipe(t2i)
            except:
                i2i = t2i  # í´ë°±
                print("  âš ï¸ I2I íŒŒì´í”„ë¼ì¸ ê³µìœ ")
        
        elif model_type in ["sdxl", "sd3", "playground"]:
            # SDXL ê³„ì—´
            t2i = StableDiffusionXLPipeline.from_pretrained(model_id, **load_kwargs).to(self.device)
            i2i = StableDiffusionXLImg2ImgPipeline.from_pretrained(model_id, **load_kwargs).to(self.device)
        
        elif model_type == "kandinsky":
            # Kandinsky ê³„ì—´
            from diffusers import AutoPipelineForText2Image
            t2i = AutoPipelineForText2Image.from_pretrained(model_id, **load_kwargs).to(self.device)
            i2i = AutoPipelineForImage2Image.from_pipe(t2i)
        
        else:
            # ê¸°ë³¸ (Auto íŒŒì´í”„ë¼ì¸)
            print(f"  âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” íƒ€ì… '{model_type}', Auto íŒŒì´í”„ë¼ì¸ ì‚¬ìš©")
            t2i = DiffusionPipeline.from_pretrained(model_id, **load_kwargs).to(self.device)
            try:
                i2i = AutoPipelineForImage2Image.from_pipe(t2i)
            except:
                i2i = t2i
        
        # ë©”ëª¨ë¦¬ ìµœì í™” ì ìš© (ì‚¬ì „ ì–‘ìí™” ëª¨ë¸ì€ CPU offload í•˜ë©´ ì•ˆ ë¨)
        if model_type == "flux-fp8-pretrained":
            print("  â„¹ï¸ ì‚¬ì „ ì–‘ìí™” ëª¨ë¸ - CPU offload ë¹„í™œì„±í™”")
        else:
            t2i = self._apply_memory_optimizations(t2i, model_type, "T2I", use_quantization)
            if i2i != t2i:
                i2i = self._apply_memory_optimizations(i2i, model_type, "I2I", use_quantization)

        return t2i, i2i
    
    def load_model(self, model_name: str) -> bool:
        """íŠ¹ì • ëª¨ë¸ ë¡œë“œ"""
        # ì´ë¯¸ ë¡œë“œëœ ê²½ìš° ìŠ¤í‚µ
        if self.is_loaded() and self.current_model_name == model_name:
            print(f"â„¹ï¸ ëª¨ë¸ '{model_name}' ì´ë¯¸ ë¡œë“œë¨ â€” ìŠ¤í‚µ")
            return True

        # ê¸°ì¡´ ëª¨ë¸ í•´ì œ (ë©”ëª¨ë¦¬ í™•ë³´)
        if self.is_loaded():
            print(f"ğŸ§¹ ê¸°ì¡´ ëª¨ë¸ '{self.current_model_name}' í•´ì œ ì¤‘...")
            self.unload_model()

        # ëª¨ë¸ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        model_config = self.registry.get_model(model_name)
        if not model_config:
            print(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸: {model_name}")
            return False

        print(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì‹œì‘: {model_name}")
        print(f"  ID: {model_config.id}")
        
        # ì¸ì¦ í•„ìš” ì—¬ë¶€ ì²´í¬
        if model_config.requires_auth:
            print(f"  âš ï¸ ì¸ì¦ í•„ìš” ëª¨ë¸ì…ë‹ˆë‹¤.")
            print(f"  í•´ê²°: huggingface-cli login")
        
        try:
            self.t2i_pipe, self.i2i_pipe = self._load_model_by_type(model_config)
            self.current_model_name = model_name
            self.current_model_config = model_config
            
            print(f"âœ… ëª¨ë¸ '{model_name}' ë¡œë”© ì„±ê³µ!")
            return True
            
        except Exception as e:
            error_msg = str(e).lower()
            print(f"âŒ ëª¨ë¸ '{model_name}' ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # ì¸ì¦ ì—ëŸ¬ ìƒì„¸ ì•ˆë‚´
            if any(kw in error_msg for kw in ["401", "authentication", "gated", "access"]):
                print(f"\nğŸ” ì¸ì¦ í•„ìš”:")
                print(f"1. https://huggingface.co/{model_config.id} ë°©ë¬¸")
                print(f"2. 'Agree and access repository' í´ë¦­")
                print(f"3. í„°ë¯¸ë„: huggingface-cli login")
            
            # GPU OOM ì—ëŸ¬
            elif "out of memory" in error_msg and self.device == "cuda":
                print(f"\nğŸ’¾ ë©”ëª¨ë¦¬ ë¶€ì¡± ê°ì§€")
                print(f"í•´ê²° ë°©ë²•:")
                print(f"1. model_config.yamlì—ì„œ memory.use_8bit: true ì„¤ì •")
                print(f"2. ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš© (sdxl, playground)")
                print(f"3. CPU ëª¨ë“œë¡œ ì‹¤í–‰")
            
            print(traceback.format_exc())
            return False
    
    def load_with_fallback(self) -> bool:
        """
        Primary ëª¨ë¸ ë¡œë“œ ì‹œë„, ì‹¤íŒ¨ ì‹œ í´ë°± ì²´ì¸ ì‹¤í–‰
        """
        # ì´ë¯¸ ë¡œë“œëœ ê²½ìš° ìŠ¤í‚µ
        if self.is_loaded():
            print(f"â„¹ï¸ ëª¨ë¸ ì´ë¯¸ ë¡œë“œë¨ â€” ìŠ¤í‚µ")
            return True
        
        # Primary ëª¨ë¸ ì‹œë„
        primary = self.registry.get_primary_model()
        print(f"ğŸ¯ Primary ëª¨ë¸ ì‹œë„: {primary}")
        
        if self.load_model(primary):
            return True
        
        # í´ë°± ë¹„í™œì„±í™”ëœ ê²½ìš° ì¢…ë£Œ
        if not self.registry.is_fallback_enabled():
            print("âš ï¸ í´ë°±ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return False
        
        # í´ë°± ì²´ì¸ ì‹¤í–‰
        fallback_chain = self.registry.get_fallback_models()
        print(f"ğŸ”„ í´ë°± ì²´ì¸ ì‹¤í–‰: {fallback_chain}")
        
        for fallback_name in fallback_chain:
            if fallback_name == primary:
                continue  # ì´ë¯¸ ì‹œë„í•œ ëª¨ë¸ ìŠ¤í‚µ
            
            print(f"\nğŸ”„ í´ë°± ì‹œë„: {fallback_name}")
            if self.load_model(fallback_name):
                print(f"âœ… í´ë°± ì„±ê³µ: {fallback_name}")
                return True
        
        # ëª¨ë“  í´ë°± ì‹¤íŒ¨
        print("âŒ ëª¨ë“  ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
        return False
    
    def unload_model(self):
        """ëª¨ë¸ ì–¸ë¡œë“œ (ë©”ëª¨ë¦¬ í•´ì œ)"""
        if self.t2i_pipe:
            del self.t2i_pipe
            self.t2i_pipe = None
        
        if self.i2i_pipe:
            del self.i2i_pipe
            self.i2i_pipe = None
        
        self.current_model_name = None
        self.current_model_config = None
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        print("ğŸ—‘ï¸ ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ")