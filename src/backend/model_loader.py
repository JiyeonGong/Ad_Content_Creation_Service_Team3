# model_loader.py
"""
ëª¨ë¸ ë¡œë” - ì„¤ì • ê¸°ë°˜ ëª¨ë¸ ë¡œë”© ë° ê´€ë¦¬
"""
import os
import traceback
from typing import Optional, Tuple, Any
import torch
from diffusers import (
    DiffusionPipeline,
    StableDiffusionXLPipeline,
    StableDiffusionXLImg2ImgPipeline,
    AutoPipelineForImage2Image
)

from .model_registry import ModelConfig, get_registry


class ModelLoader:
    """ëª¨ë¸ ë¡œë”© ë° ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, cache_dir: str, use_bfloat16: bool = True):
        self.cache_dir = cache_dir
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

        # FLUXëŠ” bfloat16 ê¶Œì¥ (ai-ad ë°©ì‹)
        if use_bfloat16 and self.device == "cuda":
            self.dtype = torch.bfloat16
        else:
            self.dtype = torch.float16 if self.device == "cuda" else torch.float32

        self.t2i_pipe = None
        self.i2i_pipe = None
        self.current_model_name = None
        self.current_model_config: Optional[ModelConfig] = None

        self.registry = get_registry()

        print(f"ğŸ”§ ModelLoader ì´ˆê¸°í™” (Device: {self.device}, dtype: {self.dtype}, Cache: {cache_dir})")
    
    def is_loaded(self) -> bool:
        """ëª¨ë¸ ë¡œë“œ ì—¬ë¶€ í™•ì¸"""
        return self.t2i_pipe is not None
    
    def get_current_model_info(self) -> dict:
        """í˜„ì¬ ë¡œë“œëœ ëª¨ë¸ ì •ë³´"""
        if not self.current_model_config:
            return {"loaded": False}
        
        return {
            "loaded": True,
            "name": self.current_model_name,
            "id": self.current_model_config.id,
            "type": self.current_model_config.type,
            "device": self.device,
            "description": self.current_model_config.description
        }
    
    def _apply_memory_optimizations(self, pipe, model_type: str):
        """ë©”ëª¨ë¦¬ ìµœì í™” ì ìš© (ai-ad ë°©ì‹ ê°•í™”)"""
        memory_config = self.registry.get_memory_config()

        # FLUX ì „ìš©: Sequential CPU offload (ë” ê³µê²©ì ì¸ ë©”ëª¨ë¦¬ ì ˆì•½)
        if model_type == "flux" and memory_config.get("enable_cpu_offload", False):
            try:
                pipe.enable_sequential_cpu_offload()
                print("  âœ“ Sequential CPU ì˜¤í”„ë¡œë“œ í™œì„±í™” (FLUX ì „ìš©, ë©”ëª¨ë¦¬ 70% ì ˆì•½)")
            except Exception as e:
                print(f"  âš ï¸ Sequential CPU offload ì‹¤íŒ¨: {e}")
                try:
                    pipe.enable_model_cpu_offload()
                    print("  âœ“ ì¼ë°˜ CPU ì˜¤í”„ë¡œë“œë¡œ í´ë°±")
                except:
                    pass
        elif memory_config.get("enable_cpu_offload", False):
            try:
                pipe.enable_model_cpu_offload()
                print("  âœ“ CPU ì˜¤í”„ë¡œë“œ í™œì„±í™”")
            except:
                pass

        # VAE Tiling (ê³ í•´ìƒë„ ì²˜ë¦¬)
        if hasattr(pipe, 'vae'):
            try:
                pipe.vae.enable_tiling()
                print("  âœ“ VAE Tiling í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½, ì†ë„ ì˜í–¥ ì—†ìŒ)")
            except:
                pass

        # VAE Slicing (ë°°ì¹˜ ì²˜ë¦¬)
        if memory_config.get("enable_vae_slicing", False):
            if hasattr(pipe, 'vae'):
                try:
                    pipe.vae.enable_slicing()
                    print("  âœ“ VAE ìŠ¬ë¼ì´ì‹± í™œì„±í™”")
                except:
                    pass

        # Attention Slicing (ì„ íƒì )
        if memory_config.get("enable_attention_slicing", False):
            try:
                pipe.enable_attention_slicing()
                print("  âœ“ ì–´í…ì…˜ ìŠ¬ë¼ì´ì‹± í™œì„±í™”")
            except:
                pass

        return pipe
    
    def _load_model_by_type(self, model_config: ModelConfig) -> Tuple[Any, Any]:
        """ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ íŒŒì´í”„ë¼ì¸ ë¡œë“œ"""
        model_id = model_config.id
        model_type = model_config.type.lower()
        
        print(f"  ğŸ“¦ íƒ€ì…: {model_type}")
        
        # 8-bit ë¡œë”© ì˜µì…˜
        load_kwargs = {
            "cache_dir": self.cache_dir,
            "torch_dtype": self.dtype
        }
        
        if self.registry.get_memory_config().get("use_8bit", False):
            load_kwargs["load_in_8bit"] = True
            print("  âœ“ 8-bit ì–‘ìí™” ëª¨ë“œ")
        
        # ëª¨ë¸ íƒ€ì…ë³„ ë¡œë”©
        if model_type == "flux":
            # FLUX ê³„ì—´ (ai-ad ë°©ì‹: CPU offload ì‚¬ìš© ì‹œ .to(device) ìƒëµ)
            t2i = DiffusionPipeline.from_pretrained(model_id, **load_kwargs)

            # CPU offload ë¯¸ì‚¬ìš© ì‹œì—ë§Œ .to(device)
            if not self.registry.get_memory_config().get("enable_cpu_offload", False):
                t2i = t2i.to(self.device)
                print(f"  âœ“ ëª¨ë¸ì„ {self.device}ë¡œ ì´ë™")

            # I2I íŒŒì´í”„ë¼ì¸ ìƒì„± ì‹œë„
            try:
                i2i = AutoPipelineForImage2Image.from_pipe(t2i)
            except:
                i2i = t2i  # í´ë°±
                print("  âš ï¸ I2I íŒŒì´í”„ë¼ì¸ ê³µìœ ")
        
        elif model_type in ["sdxl", "sd3", "playground"]:
            # SDXL ê³„ì—´
            t2i = StableDiffusionXLPipeline.from_pretrained(model_id, **load_kwargs).to(self.device)
            i2i = StableDiffusionXLImg2ImgPipeline.from_pretrained(model_id, **load_kwargs).to(self.device)
        
        elif model_type == "kandinsky":
            # Kandinsky ê³„ì—´
            from diffusers import AutoPipelineForText2Image
            t2i = AutoPipelineForText2Image.from_pretrained(model_id, **load_kwargs).to(self.device)
            i2i = AutoPipelineForImage2Image.from_pipe(t2i)
        
        else:
            # ê¸°ë³¸ (Auto íŒŒì´í”„ë¼ì¸)
            print(f"  âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” íƒ€ì… '{model_type}', Auto íŒŒì´í”„ë¼ì¸ ì‚¬ìš©")
            t2i = DiffusionPipeline.from_pretrained(model_id, **load_kwargs).to(self.device)
            try:
                i2i = AutoPipelineForImage2Image.from_pipe(t2i)
            except:
                i2i = t2i
        
        # ë©”ëª¨ë¦¬ ìµœì í™” ì ìš© (model_type ì „ë‹¬)
        t2i = self._apply_memory_optimizations(t2i, model_type)
        if i2i != t2i:
            i2i = self._apply_memory_optimizations(i2i, model_type)
        
        return t2i, i2i
    
    def load_model(self, model_name: str) -> bool:
        """íŠ¹ì • ëª¨ë¸ ë¡œë“œ"""
        # ì´ë¯¸ ë¡œë“œëœ ê²½ìš° ìŠ¤í‚µ
        if self.is_loaded() and self.current_model_name == model_name:
            print(f"â„¹ï¸ ëª¨ë¸ '{model_name}' ì´ë¯¸ ë¡œë“œë¨ â€” ìŠ¤í‚µ")
            return True
        
        # ëª¨ë¸ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        model_config = self.registry.get_model(model_name)
        if not model_config:
            print(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸: {model_name}")
            return False
        
        print(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì‹œì‘: {model_name}")
        print(f"  ID: {model_config.id}")
        
        # ì¸ì¦ í•„ìš” ì—¬ë¶€ ì²´í¬
        if model_config.requires_auth:
            print(f"  âš ï¸ ì¸ì¦ í•„ìš” ëª¨ë¸ì…ë‹ˆë‹¤.")
            print(f"  í•´ê²°: huggingface-cli login")
        
        try:
            self.t2i_pipe, self.i2i_pipe = self._load_model_by_type(model_config)
            self.current_model_name = model_name
            self.current_model_config = model_config
            
            print(f"âœ… ëª¨ë¸ '{model_name}' ë¡œë”© ì„±ê³µ!")
            return True
            
        except Exception as e:
            error_msg = str(e).lower()
            print(f"âŒ ëª¨ë¸ '{model_name}' ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # ì¸ì¦ ì—ëŸ¬ ìƒì„¸ ì•ˆë‚´
            if any(kw in error_msg for kw in ["401", "authentication", "gated", "access"]):
                print(f"\nğŸ” ì¸ì¦ í•„ìš”:")
                print(f"1. https://huggingface.co/{model_config.id} ë°©ë¬¸")
                print(f"2. 'Agree and access repository' í´ë¦­")
                print(f"3. í„°ë¯¸ë„: huggingface-cli login")
            
            # GPU OOM ì—ëŸ¬
            elif "out of memory" in error_msg and self.device == "cuda":
                print(f"\nğŸ’¾ ë©”ëª¨ë¦¬ ë¶€ì¡± ê°ì§€")
                print(f"í•´ê²° ë°©ë²•:")
                print(f"1. model_config.yamlì—ì„œ memory.use_8bit: true ì„¤ì •")
                print(f"2. ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš© (sdxl, playground)")
                print(f"3. CPU ëª¨ë“œë¡œ ì‹¤í–‰")
            
            print(traceback.format_exc())
            return False
    
    def load_with_fallback(self) -> bool:
        """
        Primary ëª¨ë¸ ë¡œë“œ ì‹œë„, ì‹¤íŒ¨ ì‹œ í´ë°± ì²´ì¸ ì‹¤í–‰
        """
        # ì´ë¯¸ ë¡œë“œëœ ê²½ìš° ìŠ¤í‚µ
        if self.is_loaded():
            print(f"â„¹ï¸ ëª¨ë¸ ì´ë¯¸ ë¡œë“œë¨ â€” ìŠ¤í‚µ")
            return True
        
        # Primary ëª¨ë¸ ì‹œë„
        primary = self.registry.get_primary_model()
        print(f"ğŸ¯ Primary ëª¨ë¸ ì‹œë„: {primary}")
        
        if self.load_model(primary):
            return True
        
        # í´ë°± ë¹„í™œì„±í™”ëœ ê²½ìš° ì¢…ë£Œ
        if not self.registry.is_fallback_enabled():
            print("âš ï¸ í´ë°±ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return False
        
        # í´ë°± ì²´ì¸ ì‹¤í–‰
        fallback_chain = self.registry.get_fallback_models()
        print(f"ğŸ”„ í´ë°± ì²´ì¸ ì‹¤í–‰: {fallback_chain}")
        
        for fallback_name in fallback_chain:
            if fallback_name == primary:
                continue  # ì´ë¯¸ ì‹œë„í•œ ëª¨ë¸ ìŠ¤í‚µ
            
            print(f"\nğŸ”„ í´ë°± ì‹œë„: {fallback_name}")
            if self.load_model(fallback_name):
                print(f"âœ… í´ë°± ì„±ê³µ: {fallback_name}")
                return True
        
        # ëª¨ë“  í´ë°± ì‹¤íŒ¨
        print("âŒ ëª¨ë“  ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
        return False
    
    def unload_model(self):
        """ëª¨ë¸ ì–¸ë¡œë“œ (ë©”ëª¨ë¦¬ í•´ì œ)"""
        if self.t2i_pipe:
            del self.t2i_pipe
            self.t2i_pipe = None
        
        if self.i2i_pipe:
            del self.i2i_pipe
            self.i2i_pipe = None
        
        self.current_model_name = None
        self.current_model_config = None
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        print("ğŸ—‘ï¸ ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ")