# comfyui_workflows.py
"""
ComfyUI ì›Œí¬í”Œë¡œìš° ì •ì˜
ì‹¤ì œ ì›Œí¬í”Œë¡œìš° JSONì€ ComfyUIì—ì„œ ìƒì„± í›„ ì—¬ê¸°ì— í…œí”Œë¦¿ìœ¼ë¡œ ì €ì¥
"""
import yaml
import os
from typing import Dict, Any


def load_image_editing_config() -> Dict[str, Any]:
    """ì´ë¯¸ì§€ í¸ì§‘ ì„¤ì • ë¡œë“œ"""
    config_path = os.path.join(
        os.path.dirname(__file__),
        "..", "..",
        "configs", "image_editing_config.yaml"
    )

    with open(config_path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


def get_pipeline_steps_for_mode(experiment_id: str) -> Dict[str, str]:
    """
    ëª¨ë“œë³„ ë…¸ë“œ ID -> íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ëª… ë§¤í•‘

    Args:
        experiment_id: ëª¨ë“œ ID (portrait_mode, product_mode, hybrid_mode)

    Returns:
        {node_id: step_name} ë”•ì…”ë„ˆë¦¬
    """
    if experiment_id == "portrait_mode":
        return {
            "1": "ğŸ“¥ ì´ë¯¸ì§€ ë¡œë“œ",
            "10": "ğŸ” ì–¼êµ´ ê°ì§€ ëª¨ë¸ ë¡œë“œ",
            "12": "ğŸ‘¤ ì–¼êµ´ ì˜ì—­ ê°ì§€ ì¤‘...",
            "13": "ğŸ­ ì–¼êµ´ ë§ˆìŠ¤í¬ ìƒì„±",
            "14": "ğŸ”„ ë§ˆìŠ¤í¬ ë°˜ì „ (í¸ì§‘ ì˜ì—­ ì„¤ì •)",
            "20": "ğŸ“Š Depth/Canny ë§µ ì¶”ì¶œ ì¤‘...",
            "21": "ğŸ® ControlNet ëª¨ë¸ ë¡œë“œ",
            "22": "ğŸ¨ ControlNet ì ìš©",
            "3": "ğŸ“ CLIP í…ìŠ¤íŠ¸ ì¸ì½”ë”©",
            "4": "ğŸ§  GGUF ëª¨ë¸ ë¡œë“œ",
            "6": "âš™ï¸ Negative í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬",
            "30": "ğŸ² Latent ë…¸ì´ì¦ˆ ìƒì„±",
            "31": "ğŸ–Œï¸ ë§ˆìŠ¤í¬ ì ìš© (í¸ì§‘ ì˜ì—­ ì œí•œ)",
            "40": "ğŸš€ ì´ë¯¸ì§€ ìƒì„± ì¤‘... (KSampler)",
            "41": "ğŸ¬ VAE ë””ì½”ë”©",
            "50": "ğŸ’¾ ê²°ê³¼ ì €ì¥"
        }
    elif experiment_id == "product_mode":
        return {
            "1": "ğŸ“¥ ì´ë¯¸ì§€ ë¡œë“œ",
            "2": "âœ‚ï¸ BEN2 ë°°ê²½ ì œê±° ì¤‘...",
            "3": "ğŸ“ ë°°ê²½ í”„ë¡¬í”„íŠ¸ ì¸ì½”ë”©",
            "4": "ğŸ§  GGUF ëª¨ë¸ ë¡œë“œ (ë°°ê²½ ìƒì„±)",
            "5": "ğŸ² Latent ë…¸ì´ì¦ˆ ìƒì„±",
            "6": "ğŸ¨ ë°°ê²½ ì´ë¯¸ì§€ ìƒì„± ì¤‘...",
            "7": "ğŸ¬ VAE ë””ì½”ë”© (ë°°ê²½)",
            "8": "ğŸ”— ì œí’ˆ + ë°°ê²½ ë ˆì´ì–´ í•©ì„±",
            "9": "ğŸ–¼ï¸ FLUX Fill ì…ë ¥ ì¤€ë¹„",
            "10": "ğŸ“ ë¸”ë Œë”© í”„ë¡¬í”„íŠ¸ ì¸ì½”ë”©",
            "11": "ğŸ§  FLUX Fill ëª¨ë¸ ë¡œë“œ",
            "12": "ğŸ¨ ìì—°ìŠ¤ëŸ¬ìš´ ë¸”ë Œë”© ì¤‘...",
            "13": "ğŸ¬ VAE ë””ì½”ë”© (ìµœì¢…)",
            "50": "ğŸ’¾ ê²°ê³¼ ì €ì¥"
        }
    elif experiment_id == "hybrid_mode":
        return {
            "1": "ğŸ“¥ ì´ë¯¸ì§€ ë¡œë“œ",
            "10": "ğŸ” ì–¼êµ´ ê°ì§€ ëª¨ë¸ ë¡œë“œ",
            "11": "ğŸ‘¤ ì–¼êµ´ ì˜ì—­ ê°ì§€ ì¤‘...",
            "12": "ğŸ­ ì–¼êµ´ ë§ˆìŠ¤í¬ ìƒì„±",
            "20": "ğŸ” ì œí’ˆ ê°ì§€ ëª¨ë¸ ë¡œë“œ",
            "21": "ğŸ“¦ ì œí’ˆ ì˜ì—­ ê°ì§€ ì¤‘...",
            "22": "ğŸ“¦ ì œí’ˆ ë§ˆìŠ¤í¬ ìƒì„±",
            "30": "ğŸ”— ì–¼êµ´+ì œí’ˆ ë§ˆìŠ¤í¬ í•©ì„±",
            "31": "ğŸ”„ ë§ˆìŠ¤í¬ ë°˜ì „ (í¸ì§‘ ì˜ì—­ ì„¤ì •)",
            "40": "ğŸ“Š Canny ë§µ ì¶”ì¶œ ì¤‘...",
            "41": "ğŸ® ControlNet ëª¨ë¸ ë¡œë“œ",
            "42": "ğŸ¨ ControlNet ì ìš©",
            "3": "ğŸ“ CLIP í…ìŠ¤íŠ¸ ì¸ì½”ë”©",
            "4": "ğŸ§  GGUF ëª¨ë¸ ë¡œë“œ",
            "6": "âš™ï¸ Negative í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬",
            "50": "ğŸ² Latent ë…¸ì´ì¦ˆ ìƒì„±",
            "51": "ğŸ–Œï¸ ë§ˆìŠ¤í¬ ì ìš© (í¸ì§‘ ì˜ì—­ ì œí•œ)",
            "60": "ğŸš€ ì´ë¯¸ì§€ ìƒì„± ì¤‘... (KSampler)",
            "61": "ğŸ¬ VAE ë””ì½”ë”©",
            "70": "ğŸ’¾ ê²°ê³¼ ì €ì¥"
        }
    else:
        return {}


def get_workflow_template(experiment_id: str) -> Dict[str, Any]:
    """
    ì‹¤í—˜ IDì— ë”°ë¼ ì›Œí¬í”Œë¡œìš° í…œí”Œë¦¿ ë°˜í™˜

    Args:
        experiment_id: ëª¨ë¸ ID (portrait_mode, product_mode, hybrid_mode, FLUX.1-dev-Q8, FLUX.1-dev-Q4)

    Returns:
        ComfyUI ì›Œí¬í”Œë¡œìš° JSON
    """
    # ìƒˆë¡œìš´ í¸ì§‘ ëª¨ë“œ
    if experiment_id == "portrait_mode":
        return get_portrait_mode_workflow()
    elif experiment_id == "product_mode":
        return get_product_mode_workflow()
    elif experiment_id == "hybrid_mode":
        return get_hybrid_mode_workflow()
    elif experiment_id in ["FLUX.1-dev-Q8", "FLUX.1-dev-Q4"]:
        return get_flux_t2i_workflow()
    else:
        raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ì‹¤í—˜ ID: {experiment_id}")


def get_flux_t2i_workflow() -> Dict[str, Any]:
    """
    FLUX T2I ì›Œí¬í”Œë¡œìš° í…œí”Œë¦¿ (GGUF ëª¨ë¸ ì‚¬ìš©)
    """
    workflow = {
        # ë…¸ë“œ 1: FLUX UNET ë¡œë“œ (GGUF)
        "1": {
            "class_type": "UnetLoaderGGUF",
            "inputs": {
                "unet_name": "flux1-dev-Q8_0.gguf"  # ëŸ°íƒ€ì„ì— ë³€ê²½
            }
        },

        # ë…¸ë“œ 2: Dual CLIP ë¡œë“œ (CLIP-L + T5-XXL)
        "2": {
            "class_type": "DualCLIPLoaderGGUF",
            "inputs": {
                "clip_name1": "clip_l.safetensors",
                "clip_name2": "t5-v1_1-xxl-encoder-Q8_0.gguf",
                "type": "flux"
            }
        },

        # ë…¸ë“œ 3: í”„ë¡¬í”„íŠ¸ ì¸ì½”ë”©
        "3": {
            "class_type": "CLIPTextEncode",
            "inputs": {
                "text": "",  # ëŸ°íƒ€ì„ì— ì„¤ì •
                "clip": ["2", 0]
            }
        },

        # [NEW] ë…¸ë“œ 30: Negative í”„ë¡¬í”„íŠ¸ (FluxëŠ” ë³´í†µ ë¹„ì›Œë‘ )
        "30": {
            "class_type": "CLIPTextEncode",
            "inputs": {
                "text": "",  # í•­ìƒ ë¹„ì›Œë‘ 
                "clip": ["2", 0]
            }
        },

        # ----------------------------------------------------------
        # [í•µì‹¬ ì¶”ê°€] ë…¸ë“œ 35: Flux Guidance
        # ì´ ë…¸ë“œê°€ ìˆì–´ì•¼ í”„ë¡¬í”„íŠ¸ ë§ì„ ì˜ ë“£ìŠµë‹ˆë‹¤.
        # guidance ê°’ì´ ë†’ì„ìˆ˜ë¡(ì˜ˆ: 6.0) í”„ë¡¬í”„íŠ¸ ë°˜ì˜ë¥ ì´ ì˜¬ë¼ê°‘ë‹ˆë‹¤.
        # ----------------------------------------------------------
        "35": {
            "class_type": "FluxGuidance",
            "inputs": {
                "conditioning": ["3", 0], # Positive í”„ë¡¬í”„íŠ¸ë¥¼ ë°›ìŒ
                "guidance": 50 # [ì¤‘ìš”] ê¸°ë³¸ê°’ 3.5 -> 4.5~6.0ìœ¼ë¡œ ì˜¬ë¦¬ì„¸ìš”!
            }
        },

        # ë…¸ë“œ 4: VAE ë¡œë“œ
        "4": {
            "class_type": "VAELoader",
            "inputs": {
                "vae_name": "ae.safetensors"
            }
        },

        # ë…¸ë“œ 5: Empty Latent
        "5": {
            "class_type": "EmptyLatentImage",
            "inputs": {
                "width": 1024,  # ëŸ°íƒ€ì„ì— ì„¤ì •
                "height": 1024,  # ëŸ°íƒ€ì„ì— ì„¤ì •
                "batch_size": 1
            }
        },

        # ë…¸ë“œ 6: KSampler
        "6": {
            "class_type": "KSampler",
            "inputs": {
                "seed": 0,  # ëŸ°íƒ€ì„ì— ì„¤ì •
                "steps": 28,  # ëŸ°íƒ€ì„ì— ì„¤ì •
                "cfg": 1.0,  # ëŸ°íƒ€ì„ì— ì„¤ì •
                "sampler_name": "euler",
                "scheduler": "simple",
                "denoise": 1.0,
                "model": ["1", 0],
                "positive": ["35", 0],
                "negative": ["30", 0],  # FLUXëŠ” negative ë¶ˆí•„ìš”
                "latent_image": ["5", 0]
            }
        },

        # ë…¸ë“œ 7: VAE Decode
        "7": {
            "class_type": "VAEDecode",
            "inputs": {
                "samples": ["6", 0],
                "vae": ["4", 0]
            }
        },

        # ë…¸ë“œ 8: Save Image
        "8": {
            "class_type": "SaveImage",
            "inputs": {
                "filename_prefix": "flux_t2i",
                "images": ["7", 0]
            }
        }
    }

    return workflow


def get_flux_t2i_with_impact_workflow() -> Dict[str, Any]:
    """
    FLUX T2I + Impact Pack (FaceDetailer) ì›Œí¬í”Œë¡œìš°
    """
    workflow = get_flux_t2i_workflow()

    # ê¸°ì¡´ ë…¸ë“œ 8 (SaveImage) ì‚­ì œ
    del workflow["8"]

    # BBox Detector ë…¸ë“œ (ì–¼êµ´ ê°ì§€ìš©)
    workflow["9"] = {
        "class_type": "UltralyticsDetectorProvider",
        "inputs": {
            "model_name": "bbox/face_yolov8m.pt"
        }
    }

    # FaceDetailer ë…¸ë“œ ì¶”ê°€ (ë…¸ë“œ 7 (VAEDecode) ì´í›„)
    workflow["10"] = {
        "class_type": "FaceDetailer",
        "inputs": {
            "image": ["7", 0],  # VAE Decode ì¶œë ¥
            "model": ["1", 0],  # UNET
            "clip": ["2", 0],   # CLIP
            "vae": ["4", 0],    # VAE
            "positive": ["35", 0],  # FluxGuidance ì¶œë ¥ (ì¡°ê±´ë¶€)
            "negative": ["30", 0],  # Negative í”„ë¡¬í”„íŠ¸ (ë¹ˆ í…ìŠ¤íŠ¸)
            "bbox_detector": ["9", 0],  # BBox Detector
            "wildcard": "",
            "cycle": 1,
            "guide_size": 512,
            "guide_size_for": True,
            "max_size": 1024,
            "seed": 0,  # ëŸ°íƒ€ì„ì— ì„¤ì •
            "steps": 3,  # ì–¼êµ´ í›„ì²˜ë¦¬ steps
            "cfg": 8.0,
            "sampler_name": "euler",
            "scheduler": "normal",
            "denoise": 0.5,
            "feather": 5,
            "noise_mask": True,
            "force_inpaint": True,
            "bbox_threshold": 0.5,
            "bbox_dilation": 10,
            "bbox_crop_factor": 3.0,
            "sam_detection_hint": "center-1",
            "sam_dilation": 0,
            "sam_threshold": 0.93,
            "sam_bbox_expansion": 0,
            "sam_mask_hint_threshold": 0.7,
            "sam_mask_hint_use_negative": "False",
            "drop_size": 10
        }
    }

    # Hand Detector ë…¸ë“œ (ì† ê°ì§€ìš©)
    workflow["12"] = {
        "class_type": "UltralyticsDetectorProvider",
        "inputs": {
            "model_name": "bbox/hand_yolov8s.pt"
        }
    }

    # HandDetailer ë…¸ë“œ ì¶”ê°€ (FaceDetailer ì¶œë ¥ ì´í›„)
    workflow["13"] = {
        "class_type": "FaceDetailer",  # HandDetailerëŠ” FaceDetailerì™€ ê°™ì€ ë…¸ë“œ ì‚¬ìš©
        "inputs": {
            "image": ["10", 0],  # FaceDetailer ì¶œë ¥
            "model": ["1", 0],
            "clip": ["2", 0],
            "vae": ["4", 0],
            "positive": ["35", 0],
            "negative": ["30", 0],
            "bbox_detector": ["12", 0],  # Hand Detector
            "wildcard": "",
            "cycle": 1,
            "guide_size": 384,  # ì†ì€ ì‘ìœ¼ë¯€ë¡œ guide_size ì¤„ì„
            "guide_size_for": True,
            "max_size": 1024,
            "seed": 0,
            "steps": 6,
            "cfg": 8.0,
            "sampler_name": "euler",
            "scheduler": "normal",
            "denoise": 0.5,
            "feather": 5,
            "noise_mask": True,
            "force_inpaint": True,
            "bbox_threshold": 0.5,
            "bbox_dilation": 10,
            "bbox_crop_factor": 3.0,
            "sam_detection_hint": "center-1",
            "sam_dilation": 0,
            "sam_threshold": 0.93,
            "sam_bbox_expansion": 0,
            "sam_mask_hint_threshold": 0.7,
            "sam_mask_hint_use_negative": "False",
            "drop_size": 10
        }
    }

    # Save Imageë¥¼ HandDetailer ì¶œë ¥ìœ¼ë¡œ ì—°ê²°
    workflow["11"] = {
        "class_type": "SaveImage",
        "inputs": {
            "filename_prefix": "flux_t2i_impact",
            "images": ["13", 0]  # HandDetailer ì¶œë ¥
        }
    }

    return workflow


def get_flux_i2i_workflow() -> Dict[str, Any]:
    """
    FLUX I2I ì›Œí¬í”Œë¡œìš° í…œí”Œë¦¿ (GGUF)
    """
    workflow = {
        # ë…¸ë“œ 1: FLUX UNET ë¡œë“œ (GGUF)
        "1": {
            "class_type": "UnetLoaderGGUF",
            "inputs": {
                "unet_name": "flux1-dev-Q8_0.gguf"  # ëŸ°íƒ€ì„ì— ë³€ê²½
            }
        },

        # ë…¸ë“œ 2: Dual CLIP ë¡œë“œ (CLIP-L + T5-XXL)
        "2": {
            "class_type": "DualCLIPLoaderGGUF",
            "inputs": {
                "clip_name1": "clip_l.safetensors",
                "clip_name2": "t5-v1_1-xxl-encoder-Q8_0.gguf",
                "type": "flux"
            }
        },

        # ë…¸ë“œ 3: í”„ë¡¬í”„íŠ¸ ì¸ì½”ë”©
        "3": {
            "class_type": "CLIPTextEncode",
            "inputs": {
                "text": "",  # ëŸ°íƒ€ì„ì— ì„¤ì •
                "clip": ["2", 0]
            }
        },

        # ë…¸ë“œ 4: VAE ë¡œë“œ
        "4": {
            "class_type": "VAELoader",
            "inputs": {
                "vae_name": "ae.safetensors"
            }
        },

        # ë…¸ë“œ 5: ì´ë¯¸ì§€ ë¡œë“œ
        "5": {
            "class_type": "LoadImage",
            "inputs": {
                "image": "input.png"  # ëŸ°íƒ€ì„ì— ì„¤ì •
            }
        },

        # ë…¸ë“œ 6: VAE Encode
        "6": {
            "class_type": "VAEEncode",
            "inputs": {
                "pixels": ["5", 0],
                "vae": ["4", 0]
            }
        },

        # ë…¸ë“œ 7: KSampler
        "7": {
            "class_type": "KSampler",
            "inputs": {
                "seed": 0,  # ëŸ°íƒ€ì„ì— ì„¤ì •
                "steps": 28,  # ëŸ°íƒ€ì„ì— ì„¤ì •
                "cfg": 3.5,  # ëŸ°íƒ€ì„ì— ì„¤ì •
                "sampler_name": "euler",
                "scheduler": "normal",
                "denoise": 0.75,  # strength, ëŸ°íƒ€ì„ì— ì„¤ì •
                "model": ["1", 0],
                "positive": ["3", 0],
                "negative": ["3", 0],  # FLUXëŠ” negative ë¶ˆí•„ìš”
                "latent_image": ["6", 0]
            }
        },

        # ë…¸ë“œ 8: VAE Decode
        "8": {
            "class_type": "VAEDecode",
            "inputs": {
                "samples": ["7", 0],
                "vae": ["4", 0]
            }
        },

        # ë…¸ë“œ 9: Save Image
        "9": {
            "class_type": "SaveImage",
            "inputs": {
                "filename_prefix": "flux_i2i",
                "images": ["8", 0]
            }
        }
    }

    return workflow


# ğŸ—‘ï¸ ê¸°ì¡´ ì‹¤í—˜ ì›Œí¬í”Œë¡œìš° ì œê±°ë¨ (ben2_flux_fill, ben2_qwen_image)
# ìƒˆë¡œìš´ 3ê°€ì§€ ëª¨ë“œë¡œ ëŒ€ì²´: portrait_mode, product_mode, hybrid_mode


def update_flux_t2i_workflow(
    workflow: Dict[str, Any],
    model_name: str,
    prompt: str,
    width: int,
    height: int,
    steps: int,
    guidance_scale: float,
    seed: int = None
) -> Dict[str, Any]:
    """FLUX T2I ì›Œí¬í”Œë¡œìš° íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ (GGUF)"""
    import random
    from .model_registry import get_model_config

    if seed is None:
        seed = random.randint(0, 2**32 - 1)

    # configì—ì„œ model_id ê°€ì ¸ì˜¤ê¸°
    model_config = get_model_config()
    model_id = model_config["models"][model_name]["id"]

    # UNET GGUF íŒŒì¼ ì„¤ì • (ë…¸ë“œ 1)
    workflow["1"]["inputs"]["unet_name"] = model_id

    # í”„ë¡¬í”„íŠ¸ ì„¤ì • (ë…¸ë“œ 3)
    workflow["3"]["inputs"]["text"] = prompt

    # ì´ë¯¸ì§€ í¬ê¸° ì„¤ì • (ë…¸ë“œ 5: EmptyLatentImage)
    workflow["5"]["inputs"]["width"] = width
    workflow["5"]["inputs"]["height"] = height

    # ------------------------------------------------------------
    # [í•µì‹¬ ìˆ˜ì •] Guidance ê°’ì„ ì˜¬ë°”ë¥¸ ë…¸ë“œ(35ë²ˆ)ì— ì—°ê²°!
    # ------------------------------------------------------------
    
    # 1. KSampler (ë…¸ë“œ 6)
    workflow["6"]["inputs"]["seed"] = seed
    workflow["6"]["inputs"]["steps"] = steps
    workflow["6"]["inputs"]["cfg"] = 1.0  # [ì¤‘ìš”] CFGëŠ” 1.0 ê³ ì •! (ì›¹ ê°’ ë¬´ì‹œ)

    # 2. FluxGuidance (ë…¸ë“œ 35)
    if "35" in workflow:
        workflow["35"]["inputs"]["guidance"] = guidance_scale # ì›¹ ìŠ¬ë¼ì´ë” ê°’ì„ ì—¬ê¸°ì—!
    else:
        print("âš ï¸ ê²½ê³ : FluxGuidance(35) ë…¸ë“œê°€ í…œí”Œë¦¿ì— ì—†ìŠµë‹ˆë‹¤!")

    return workflow


def update_flux_i2i_workflow(
    workflow: Dict[str, Any],
    model_name: str,
    prompt: str,
    strength: float,
    steps: int,
    guidance_scale: float,
    seed: int = None
) -> Dict[str, Any]:
    """FLUX I2I ì›Œí¬í”Œë¡œìš° íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ (GGUF)"""
    import random
    from .model_registry import get_model_config

    if seed is None:
        seed = random.randint(0, 2**32 - 1)

    # configì—ì„œ model_id ê°€ì ¸ì˜¤ê¸°
    model_config = get_model_config()
    model_id = model_config["models"][model_name]["id"]

    # UNET GGUF íŒŒì¼ ì„¤ì • (ë…¸ë“œ 1)
    workflow["1"]["inputs"]["unet_name"] = model_id

    # í”„ë¡¬í”„íŠ¸ ì„¤ì • (ë…¸ë“œ 3)
    workflow["3"]["inputs"]["text"] = prompt

    # ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„° ì„¤ì • (ë…¸ë“œ 7: KSampler)
    workflow["7"]["inputs"]["seed"] = seed
    workflow["7"]["inputs"]["steps"] = steps
    workflow["7"]["inputs"]["cfg"] = guidance_scale
    workflow["7"]["inputs"]["denoise"] = strength

    return workflow


def update_workflow_inputs(
    workflow: Dict[str, Any],
    experiment_id: str,
    prompt: str,
    negative_prompt: str = "",
    steps: int = None,
    guidance_scale: float = None,
    strength: float = None,
    seed: int = None,
    # ìƒˆë¡œìš´ ëª¨ë“œ íŒŒë¼ë¯¸í„°
    controlnet_type: str = "depth",
    controlnet_strength: float = 0.7,
    denoise_strength: float = 1.0,
    blending_strength: float = 0.35,
    background_prompt: str = None
) -> Dict[str, Any]:
    """
    ì›Œí¬í”Œë¡œìš° í…œí”Œë¦¿ì— ì‚¬ìš©ì ì…ë ¥ ë°˜ì˜

    Args:
        workflow: ì›Œí¬í”Œë¡œìš° í…œí”Œë¦¿
        experiment_id: ì‹¤í—˜ ID (portrait_mode, product_mode, hybrid_mode)
        prompt: í¸ì§‘ í”„ë¡¬í”„íŠ¸ (positive)
        negative_prompt: ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸
        steps: ì¶”ë¡  ë‹¨ê³„
        guidance_scale: Guidance scale
        strength: ë³€í™” ê°•ë„ (deprecated, denoise_strength ì‚¬ìš©)
        seed: ëœë¤ ì‹œë“œ
        controlnet_type: ControlNet íƒ€ì… ("depth" ë˜ëŠ” "canny")
        controlnet_strength: ControlNet ê°•ë„ (0.0~1.0)
        denoise_strength: ë³€ê²½ ê°•ë„ (0.0~1.0)
        blending_strength: ë¸”ë Œë”© ê°•ë„ (Product ëª¨ë“œ)
        background_prompt: ë°°ê²½ í”„ë¡¬í”„íŠ¸ (Product ëª¨ë“œ)

    Returns:
        ì—…ë°ì´íŠ¸ëœ ì›Œí¬í”Œë¡œìš°
    """
    config = load_image_editing_config()

    # ëª¨ë“œ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    mode_config = None
    for mode_id, mode_data in config.get("editing_modes", {}).items():
        if mode_data["id"] == experiment_id:
            mode_config = mode_data
            break

    if not mode_config:
        raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë“œ ID: {experiment_id}")

    # ê¸°ë³¸ê°’ ì„¤ì •
    params = mode_config.get("params", {})
    steps = steps or params.get("default_steps", 28)
    guidance_scale = guidance_scale or params.get("guidance_scale", 3.5)

    # ëœë¤ ì‹œë“œ ìƒì„±
    if seed is None:
        import random
        seed = random.randint(0, 2**32 - 1)

    # ============================================================
    # Portrait Mode ì›Œí¬í”Œë¡œìš° ì—…ë°ì´íŠ¸
    # ============================================================
    if experiment_id == "portrait_mode":
        # í”„ë¡¬í”„íŠ¸ ì„¤ì • (ë…¸ë“œ 5)
        if "5" in workflow:
            workflow["5"]["inputs"]["text"] = prompt

        # Negative í”„ë¡¬í”„íŠ¸ (ë…¸ë“œ 6)
        if "6" in workflow:
            workflow["6"]["inputs"]["text"] = negative_prompt

        # FluxGuidance (ë…¸ë“œ 7)
        if "7" in workflow:
            workflow["7"]["inputs"]["guidance"] = guidance_scale

        # ControlNet íƒ€ì… ë³€ê²½ (ë…¸ë“œ 20)
        if "20" in workflow and controlnet_type == "canny":
            workflow["20"] = {
                "class_type": "CannyEdgePreprocessor",
                "inputs": {
                    "image": ["1", 0],
                    "low_threshold": 100,
                    "high_threshold": 200,
                    "resolution": 1024
                }
            }

        # ControlNet ê°•ë„ (ë…¸ë“œ 22)
        if "22" in workflow:
            workflow["22"]["inputs"]["strength"] = controlnet_strength

        # KSampler (ë…¸ë“œ 40)
        if "40" in workflow:
            workflow["40"]["inputs"]["seed"] = seed
            workflow["40"]["inputs"]["steps"] = steps
            workflow["40"]["inputs"]["denoise"] = denoise_strength

    # ============================================================
    # Product Mode ì›Œí¬í”Œë¡œìš° ì—…ë°ì´íŠ¸
    # ============================================================
    elif experiment_id == "product_mode":
        # ë°°ê²½ í”„ë¡¬í”„íŠ¸ ì„¤ì • (ë…¸ë“œ 13)
        bg_prompt = background_prompt or prompt
        if "13" in workflow:
            workflow["13"]["inputs"]["text"] = bg_prompt

        # Negative í”„ë¡¬í”„íŠ¸ (ë…¸ë“œ 14)
        if "14" in workflow:
            workflow["14"]["inputs"]["text"] = negative_prompt

        # FluxGuidance (ë…¸ë“œ 15)
        if "15" in workflow:
            workflow["15"]["inputs"]["guidance"] = guidance_scale

        # ë°°ê²½ ìƒì„± KSampler (ë…¸ë“œ 17)
        if "17" in workflow:
            workflow["17"]["inputs"]["seed"] = seed
            workflow["17"]["inputs"]["steps"] = steps

        # ë¸”ë Œë”© KSampler (ë…¸ë“œ 42)
        if "42" in workflow:
            workflow["42"]["inputs"]["seed"] = seed
            workflow["42"]["inputs"]["denoise"] = blending_strength

    # ============================================================
    # Hybrid Mode ì›Œí¬í”Œë¡œìš° ì—…ë°ì´íŠ¸
    # ============================================================
    elif experiment_id == "hybrid_mode":
        # í”„ë¡¬í”„íŠ¸ ì„¤ì • (ë…¸ë“œ 5)
        if "5" in workflow:
            workflow["5"]["inputs"]["text"] = prompt

        # Negative í”„ë¡¬í”„íŠ¸ (ë…¸ë“œ 6)
        if "6" in workflow:
            workflow["6"]["inputs"]["text"] = negative_prompt

        # FluxGuidance (ë…¸ë“œ 7)
        if "7" in workflow:
            workflow["7"]["inputs"]["guidance"] = guidance_scale

        # ControlNet íƒ€ì… ë³€ê²½ (ë…¸ë“œ 20 - ê¸°ë³¸ì€ Canny)
        if "20" in workflow and controlnet_type == "depth":
            workflow["20"] = {
                "class_type": "DepthAnythingPreprocessor",
                "inputs": {
                    "image": ["1", 0],
                    "ckpt_name": "depth_anything_v2_vitl.pth",
                    "resolution": 1024
                }
            }

        # ControlNet ê°•ë„ (ë…¸ë“œ 22)
        if "22" in workflow:
            workflow["22"]["inputs"]["strength"] = controlnet_strength

        # KSampler (ë…¸ë“œ 40)
        if "40" in workflow:
            workflow["40"]["inputs"]["seed"] = seed
            workflow["40"]["inputs"]["steps"] = steps
            workflow["40"]["inputs"]["denoise"] = denoise_strength

    return workflow


# ğŸ—‘ï¸ í”„ë¦¬ë¡œë“œ ê¸°ëŠ¥ ì œê±°ë¨ (ëª¨ë¸ ìë™ ë¡œë”© ì œê±°ë¡œ ë¶ˆí•„ìš”)




def get_workflow_input_image_node_id(experiment_id: str) -> str:
    """
    ì›Œí¬í”Œë¡œìš°ì˜ ì…ë ¥ ì´ë¯¸ì§€ ë…¸ë“œ ID ë°˜í™˜

    Args:
        experiment_id: ì‹¤í—˜ ID

    Returns:
        ë…¸ë“œ ID (ì˜ˆ: "1")
    """
    # ëª¨ë“  ì›Œí¬í”Œë¡œìš°ì—ì„œ ë…¸ë“œ 1ì´ LoadImage
    return "1"


# ============================================================
# ìƒˆë¡œìš´ í¸ì§‘ ëª¨ë“œ ì›Œí¬í”Œë¡œìš° (v3.0)
# ============================================================

def get_portrait_mode_workflow() -> Dict[str, Any]:
    """
    ğŸŸ¢ Portrait Mode ì›Œí¬í”Œë¡œìš°
    
    íŒŒì´í”„ë¼ì¸:
    1. ì–¼êµ´ ê°ì§€ (Face Detector)
    2. ë§ˆìŠ¤í¬ ë°˜ì „ (ì–¼êµ´ ì œì™¸)
    3. ControlNet ê°€ì´ë“œ ì¶”ì¶œ (Depth/Canny)
    4. Masked I2I ìƒì„± (ì˜·/ë°°ê²½ë§Œ ë³€ê²½)
    """
    workflow = {
        # ë…¸ë“œ 1: ì…ë ¥ ì´ë¯¸ì§€ ë¡œë“œ
        "1": {
            "class_type": "LoadImage",
            "inputs": {
                "image": "input.png"  # ëŸ°íƒ€ì„ì— ë³€ê²½
            }
        },

        # ë…¸ë“œ 2: FLUX UNET ë¡œë“œ
        "2": {
            "class_type": "UnetLoaderGGUF",
            "inputs": {
                "unet_name": "flux1-dev-Q8_0.gguf"
            }
        },

        # ë…¸ë“œ 3: Dual CLIP ë¡œë“œ
        "3": {
            "class_type": "DualCLIPLoaderGGUF",
            "inputs": {
                "clip_name1": "clip_l.safetensors",
                "clip_name2": "t5-v1_1-xxl-encoder-Q8_0.gguf",
                "type": "flux"
            }
        },

        # ë…¸ë“œ 4: VAE ë¡œë“œ
        "4": {
            "class_type": "VAELoader",
            "inputs": {
                "vae_name": "ae.safetensors"
            }
        },

        # ë…¸ë“œ 5: í”„ë¡¬í”„íŠ¸ ì¸ì½”ë”©
        "5": {
            "class_type": "CLIPTextEncode",
            "inputs": {
                "text": "",  # ëŸ°íƒ€ì„ì— ì„¤ì •
                "clip": ["3", 0]
            }
        },

        # ë…¸ë“œ 6: Negative í”„ë¡¬í”„íŠ¸
        "6": {
            "class_type": "CLIPTextEncode",
            "inputs": {
                "text": "",
                "clip": ["3", 0]
            }
        },

        # ë…¸ë“œ 7: FluxGuidance
        "7": {
            "class_type": "FluxGuidance",
            "inputs": {
                "conditioning": ["5", 0],
                "guidance": 3.5  # ëŸ°íƒ€ì„ì— ì„¤ì •
            }
        },

        # ë…¸ë“œ 10: Face Detector
        "10": {
            "class_type": "UltralyticsDetectorProvider",
            "inputs": {
                "model_name": "bbox/face_yolov8m.pt"
            }
        },

        # ë…¸ë“œ 11: SEGS from Detection (ì–¼êµ´ ì˜ì—­ ì„¸ê·¸ë¨¼íŠ¸)
        "11": {
            "class_type": "SAMLoader",
            "inputs": {
                "model_name": "sam_vit_b_01ec64.pth"
            }
        },

        # ë…¸ë“œ 12: BboxDetectorSEGS (ì–¼êµ´ ê°ì§€ ì‹¤í–‰)
        "12": {
            "class_type": "BboxDetectorSEGS",
            "inputs": {
                "image": ["1", 0],
                "bbox_detector": ["10", 0],
                "threshold": 0.5,
                "dilation": 10,
                "crop_factor": 3.0,
                "drop_size": 10,
                "labels": "all"
            }
        },

        # ë…¸ë“œ 13: SEGS to Mask (ì–¼êµ´ ë§ˆìŠ¤í¬ ìƒì„±)
        "13": {
            "class_type": "SegsToCombinedMask",
            "inputs": {
                "segs": ["12", 0]
            }
        },

        # ë…¸ë“œ 14: Invert Mask (ì–¼êµ´ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ì˜ì—­)
        "14": {
            "class_type": "InvertMask",
            "inputs": {
                "mask": ["13", 0]
            }
        },

        # ë…¸ë“œ 20: ControlNet Preprocessor (Depth ë˜ëŠ” Canny)
        "20": {
            "class_type": "DepthAnythingPreprocessor",
            "inputs": {
                "image": ["1", 0],
                "ckpt_name": "depth_anything_vitl14.pth",
                "resolution": 1024
            }
        },

        # ë…¸ë“œ 21: ControlNet ë¡œë“œ
        "21": {
            "class_type": "ControlNetLoader",
            "inputs": {
                "control_net_name": "InstantX-FLUX.1-dev-Controlnet-Union.safetensors"
            }
        },

        # ë…¸ë“œ 22: Apply ControlNet
        "22": {
            "class_type": "ControlNetApplyAdvanced",
            "inputs": {
                "positive": ["7", 0],
                "negative": ["6", 0],
                "control_net": ["21", 0],
                "image": ["20", 0],
                "strength": 0.7,  # ëŸ°íƒ€ì„ì— ì„¤ì •
                "start_percent": 0.0,
                "end_percent": 1.0
            }
        },

        # ë…¸ë“œ 30: VAE Encode (ì…ë ¥ ì´ë¯¸ì§€)
        "30": {
            "class_type": "VAEEncode",
            "inputs": {
                "pixels": ["1", 0],
                "vae": ["4", 0]
            }
        },

        # ë…¸ë“œ 31: Set Latent Noise Mask (ë§ˆìŠ¤í¬ ì ìš©)
        "31": {
            "class_type": "SetLatentNoiseMask",
            "inputs": {
                "samples": ["30", 0],
                "mask": ["14", 0]  # ë°˜ì „ëœ ë§ˆìŠ¤í¬ (ì–¼êµ´ ì œì™¸)
            }
        },

        # ë…¸ë“œ 40: KSampler (Masked I2I)
        "40": {
            "class_type": "KSampler",
            "inputs": {
                "seed": 0,  # ëŸ°íƒ€ì„ì— ì„¤ì •
                "steps": 28,  # ëŸ°íƒ€ì„ì— ì„¤ì •
                "cfg": 1.0,
                "sampler_name": "euler",
                "scheduler": "simple",
                "denoise": 1.0,  # ëŸ°íƒ€ì„ì— ì„¤ì •
                "model": ["2", 0],
                "positive": ["22", 0],  # ControlNet ì ìš©ëœ ì¡°ê±´
                "negative": ["6", 0],
                "latent_image": ["31", 0]  # ë§ˆìŠ¤í¬ ì ìš©ëœ latent
            }
        },

        # ë…¸ë“œ 41: VAE Decode
        "41": {
            "class_type": "VAEDecode",
            "inputs": {
                "samples": ["40", 0],
                "vae": ["4", 0]
            }
        },

        # ë…¸ë“œ 50: Save Image
        "50": {
            "class_type": "SaveImage",
            "inputs": {
                "filename_prefix": "portrait_mode",
                "images": ["41", 0]
            }
        }
    }

    return workflow


def get_product_mode_workflow() -> Dict[str, Any]:
    """
    ğŸ”µ Product Mode ì›Œí¬í”Œë¡œìš°
    
    íŒŒì´í”„ë¼ì¸:
    1. BEN2ë¡œ ì œí’ˆ ëˆ„ë¼ ë”°ê¸°
    2. Flux Dev T2Ië¡œ ë°°ê²½ ìƒì„±
    3. ë ˆì´ì–´ í•©ì„± (ë°°ê²½ + ì œí’ˆ)
    4. Flux Fillë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ìœµí•©
    """
    workflow = {
        # ë…¸ë“œ 1: ì…ë ¥ ì´ë¯¸ì§€ ë¡œë“œ
        "1": {
            "class_type": "LoadImage",
            "inputs": {
                "image": "input.png"
            }
        },

        # ë…¸ë“œ 2: RMBG ë°°ê²½ ì œê±° (BEN2 ëª¨ë¸ ì‚¬ìš©)
        "2": {
            "class_type": "RMBG",
            "inputs": {
                "image": ["1", 0],
                "model": "BEN2",
                "sensitivity": 1.0,
                "process_res": 1024,
                "background": "Alpha"
            }
        },

        # ë…¸ë“œ 10: FLUX UNET ë¡œë“œ (T2Iìš©)
        "10": {
            "class_type": "UnetLoaderGGUF",
            "inputs": {
                "unet_name": "flux1-dev-Q8_0.gguf"
            }
        },

        # ë…¸ë“œ 11: Dual CLIP ë¡œë“œ
        "11": {
            "class_type": "DualCLIPLoaderGGUF",
            "inputs": {
                "clip_name1": "clip_l.safetensors",
                "clip_name2": "t5-v1_1-xxl-encoder-Q8_0.gguf",
                "type": "flux"
            }
        },

        # ë…¸ë“œ 12: VAE ë¡œë“œ
        "12": {
            "class_type": "VAELoader",
            "inputs": {
                "vae_name": "ae.safetensors"
            }
        },

        # ë…¸ë“œ 13: ë°°ê²½ í”„ë¡¬í”„íŠ¸ ì¸ì½”ë”©
        "13": {
            "class_type": "CLIPTextEncode",
            "inputs": {
                "text": "",  # ëŸ°íƒ€ì„ì— ì„¤ì • (ë°°ê²½ í”„ë¡¬í”„íŠ¸)
                "clip": ["11", 0]
            }
        },

        # ë…¸ë“œ 14: Negative í”„ë¡¬í”„íŠ¸
        "14": {
            "class_type": "CLIPTextEncode",
            "inputs": {
                "text": "",
                "clip": ["11", 0]
            }
        },

        # ë…¸ë“œ 15: FluxGuidance
        "15": {
            "class_type": "FluxGuidance",
            "inputs": {
                "conditioning": ["13", 0],
                "guidance": 5.0  # ëŸ°íƒ€ì„ì— ì„¤ì •
            }
        },

        # ë…¸ë“œ 16: Empty Latent (ë°°ê²½ ìƒì„±ìš©)
        "16": {
            "class_type": "EmptyLatentImage",
            "inputs": {
                "width": 1024,  # ëŸ°íƒ€ì„ì— ì„¤ì •
                "height": 1024,
                "batch_size": 1
            }
        },

        # ë…¸ë“œ 17: KSampler (ë°°ê²½ ìƒì„±)
        "17": {
            "class_type": "KSampler",
            "inputs": {
                "seed": 0,  # ëŸ°íƒ€ì„ì— ì„¤ì •
                "steps": 28,
                "cfg": 1.0,
                "sampler_name": "euler",
                "scheduler": "simple",
                "denoise": 1.0,
                "model": ["10", 0],
                "positive": ["15", 0],
                "negative": ["14", 0],
                "latent_image": ["16", 0]
            }
        },

        # ë…¸ë“œ 18: VAE Decode (ë°°ê²½ ì´ë¯¸ì§€)
        "18": {
            "class_type": "VAEDecode",
            "inputs": {
                "samples": ["17", 0],
                "vae": ["12", 0]
            }
        },

        # ë…¸ë“œ 20: ImageCompositeM (ë ˆì´ì–´ í•©ì„±)
        "20": {
            "class_type": "ImageCompositeAbsolute",
            "inputs": {
                "image_base": ["18", 0],  # ë°°ê²½
                "image_overlay": ["2", 0],  # BEN2 ëˆ„ë¼ ì´ë¯¸ì§€
                "x": 0,
                "y": 0
            }
        },

        # ë…¸ë“œ 30: FLUX Fill UNET ë¡œë“œ
        "30": {
            "class_type": "UnetLoaderGGUF",
            "inputs": {
                "unet_name": "FLUX.1-Fill-dev-Q8_0.gguf"
            }
        },

        # ë…¸ë“œ 31: BEN2 ë§ˆìŠ¤í¬ ì¶”ì¶œ
        "31": {
            "class_type": "ImageToMask",
            "inputs": {
                "image": ["2", 0],
                "channel": "alpha"
            }
        },

        # ë…¸ë“œ 32: Invert Mask (ì œí’ˆ ì™¸ê³½ë§Œ)
        "32": {
            "class_type": "InvertMask",
            "inputs": {
                "mask": ["31", 0]
            }
        },

        # ë…¸ë“œ 33: Dilate Mask (ì™¸ê³½ í™•ì¥)
        "33": {
            "class_type": "GrowMask",
            "inputs": {
                "mask": ["31", 0],
                "expand": 10,
                "tapered_corners": True
            }
        },

        # ë…¸ë“œ 40: VAE Encode (í•©ì„± ì´ë¯¸ì§€)
        "40": {
            "class_type": "VAEEncode",
            "inputs": {
                "pixels": ["20", 0],
                "vae": ["12", 0]
            }
        },

        # ë…¸ë“œ 41: Set Latent Noise Mask (ì™¸ê³½ë§Œ ë¸”ë Œë”©)
        "41": {
            "class_type": "SetLatentNoiseMask",
            "inputs": {
                "samples": ["40", 0],
                "mask": ["33", 0]
            }
        },

        # ë…¸ë“œ 42: KSampler (Blending)
        "42": {
            "class_type": "KSampler",
            "inputs": {
                "seed": 0,
                "steps": 28,
                "cfg": 1.0,
                "sampler_name": "euler",
                "scheduler": "simple",
                "denoise": 0.35,  # ëŸ°íƒ€ì„ì— ì„¤ì • (ë¸”ë Œë”© ê°•ë„)
                "model": ["30", 0],  # Flux Fill
                "positive": ["15", 0],  # ë°°ê²½ í”„ë¡¬í”„íŠ¸ ì¬ì‚¬ìš©
                "negative": ["14", 0],
                "latent_image": ["41", 0]
            }
        },

        # ë…¸ë“œ 43: VAE Decode
        "43": {
            "class_type": "VAEDecode",
            "inputs": {
                "samples": ["42", 0],
                "vae": ["12", 0]
            }
        },

        # ë…¸ë“œ 50: Save Image
        "50": {
            "class_type": "SaveImage",
            "inputs": {
                "filename_prefix": "product_mode",
                "images": ["43", 0]
            }
        }
    }

    return workflow


def get_hybrid_mode_workflow() -> Dict[str, Any]:
    """
    ğŸŸ£ Hybrid Mode ì›Œí¬í”Œë¡œìš°
    
    íŒŒì´í”„ë¼ì¸:
    1. ì–¼êµ´ ê°ì§€ (Face Detector)
    2. ì œí’ˆ ê°ì§€ (BEN2)
    3. ë©€í‹° ë§ˆìŠ¤í¬ í•©ì„± (ì–¼êµ´ + ì œí’ˆ)
    4. ë§ˆìŠ¤í¬ ë°˜ì „ (ì˜·/ë°°ê²½ë§Œ ìˆ˜ì •)
    5. ControlNet (Canny) + Masked I2I
    """
    workflow = {
        # ë…¸ë“œ 1: ì…ë ¥ ì´ë¯¸ì§€ ë¡œë“œ
        "1": {
            "class_type": "LoadImage",
            "inputs": {
                "image": "input.png"
            }
        },

        # ë…¸ë“œ 2: FLUX UNET ë¡œë“œ
        "2": {
            "class_type": "UnetLoaderGGUF",
            "inputs": {
                "unet_name": "flux1-dev-Q8_0.gguf"
            }
        },

        # ë…¸ë“œ 3: Dual CLIP ë¡œë“œ
        "3": {
            "class_type": "DualCLIPLoaderGGUF",
            "inputs": {
                "clip_name1": "clip_l.safetensors",
                "clip_name2": "t5-v1_1-xxl-encoder-Q8_0.gguf",
                "type": "flux"
            }
        },

        # ë…¸ë“œ 4: VAE ë¡œë“œ
        "4": {
            "class_type": "VAELoader",
            "inputs": {
                "vae_name": "ae.safetensors"
            }
        },

        # ë…¸ë“œ 5: í”„ë¡¬í”„íŠ¸ ì¸ì½”ë”©
        "5": {
            "class_type": "CLIPTextEncode",
            "inputs": {
                "text": "",  # ëŸ°íƒ€ì„ì— ì„¤ì •
                "clip": ["3", 0]
            }
        },

        # ë…¸ë“œ 6: Negative í”„ë¡¬í”„íŠ¸
        "6": {
            "class_type": "CLIPTextEncode",
            "inputs": {
                "text": "",
                "clip": ["3", 0]
            }
        },

        # ë…¸ë“œ 7: FluxGuidance
        "7": {
            "class_type": "FluxGuidance",
            "inputs": {
                "conditioning": ["5", 0],
                "guidance": 3.5
            }
        },

        # ë…¸ë“œ 10: Face Detector
        "10": {
            "class_type": "UltralyticsDetectorProvider",
            "inputs": {
                "model_name": "bbox/face_yolov8m.pt"
            }
        },

        # ë…¸ë“œ 11: BboxDetectorSEGS (ì–¼êµ´)
        "11": {
            "class_type": "BboxDetectorSEGS",
            "inputs": {
                "image": ["1", 0],
                "bbox_detector": ["10", 0],
                "threshold": 0.5,
                "dilation": 10,
                "crop_factor": 3.0,
                "drop_size": 10
            }
        },

        # ë…¸ë“œ 12: SEGS to Mask (ì–¼êµ´ ë§ˆìŠ¤í¬)
        "12": {
            "class_type": "SegsToCombinedMask",
            "inputs": {
                "segs": ["11", 0]
            }
        },

        # ë…¸ë“œ 15: BEN2 (ì œí’ˆ ëˆ„ë¼)
        "15": {
            "class_type": "BEN2",
            "inputs": {
                "image": ["1", 0]
            }
        },

        # ë…¸ë“œ 16: ImageToMask (ì œí’ˆ ë§ˆìŠ¤í¬)
        "16": {
            "class_type": "ImageToMask",
            "inputs": {
                "image": ["15", 0],
                "channel": "alpha"
            }
        },

        # ë…¸ë“œ 17: MaskComposite (ì–¼êµ´ + ì œí’ˆ ë§ˆìŠ¤í¬ í•©ì„±)
        "17": {
            "class_type": "MaskComposite",
            "inputs": {
                "destination": ["12", 0],  # ì–¼êµ´ ë§ˆìŠ¤í¬
                "source": ["16", 0],  # ì œí’ˆ ë§ˆìŠ¤í¬
                "x": 0,
                "y": 0,
                "operation": "add"  # í•©ì§‘í•©
            }
        },

        # ë…¸ë“œ 18: Invert Mask (ì–¼êµ´+ì œí’ˆ ì œì™¸í•œ ë‚˜ë¨¸ì§€)
        "18": {
            "class_type": "InvertMask",
            "inputs": {
                "mask": ["17", 0]
            }
        },

        # ë…¸ë“œ 20: ControlNet Preprocessor (Canny)
        "20": {
            "class_type": "CannyEdgePreprocessor",
            "inputs": {
                "image": ["1", 0],
                "low_threshold": 100,
                "high_threshold": 200,
                "resolution": 1024
            }
        },

        # ë…¸ë“œ 21: ControlNet ë¡œë“œ
        "21": {
            "class_type": "ControlNetLoader",
            "inputs": {
                "control_net_name": "InstantX-FLUX.1-dev-Controlnet-Union.safetensors"
            }
        },

        # ë…¸ë“œ 22: Apply ControlNet
        "22": {
            "class_type": "ControlNetApplyAdvanced",
            "inputs": {
                "positive": ["7", 0],
                "negative": ["6", 0],
                "control_net": ["21", 0],
                "image": ["20", 0],
                "strength": 0.8,  # ëŸ°íƒ€ì„ì— ì„¤ì •
                "start_percent": 0.0,
                "end_percent": 1.0
            }
        },

        # ë…¸ë“œ 30: VAE Encode
        "30": {
            "class_type": "VAEEncode",
            "inputs": {
                "pixels": ["1", 0],
                "vae": ["4", 0]
            }
        },

        # ë…¸ë“œ 31: Set Latent Noise Mask
        "31": {
            "class_type": "SetLatentNoiseMask",
            "inputs": {
                "samples": ["30", 0],
                "mask": ["18", 0]  # ì–¼êµ´+ì œí’ˆ ì œì™¸
            }
        },

        # ë…¸ë“œ 40: KSampler
        "40": {
            "class_type": "KSampler",
            "inputs": {
                "seed": 0,
                "steps": 28,
                "cfg": 1.0,
                "sampler_name": "euler",
                "scheduler": "simple",
                "denoise": 0.9,  # ëŸ°íƒ€ì„ì— ì„¤ì •
                "model": ["2", 0],
                "positive": ["22", 0],
                "negative": ["6", 0],
                "latent_image": ["31", 0]
            }
        },

        # ë…¸ë“œ 41: VAE Decode
        "41": {
            "class_type": "VAEDecode",
            "inputs": {
                "samples": ["40", 0],
                "vae": ["4", 0]
            }
        },

        # ë…¸ë“œ 50: Save Image
        "50": {
            "class_type": "SaveImage",
            "inputs": {
                "filename_prefix": "hybrid_mode",
                "images": ["41", 0]
            }
        }
    }

    return workflow
