# comfyui_workflows.py
"""
ComfyUI ì›Œí¬í”Œë¡œìš° ì •ì˜
ì‹¤ì œ ì›Œí¬í”Œë¡œìš° JSONì€ ComfyUIì—ì„œ ìƒì„± í›„ ì—¬ê¸°ì— í…œí”Œë¦¿ìœ¼ë¡œ ì €ì¥
"""
import yaml
import os
from typing import Dict, Any


def load_image_editing_config() -> Dict[str, Any]:
    """ì´ë¯¸ì§€ í¸ì§‘ ì„¤ì • ë¡œë“œ"""
    config_path = os.path.join(
        os.path.dirname(__file__),
        "..", "..",
        "configs", "image_editing_config.yaml"
    )

    with open(config_path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


def get_workflow_template(experiment_id: str) -> Dict[str, Any]:
    """
    ì‹¤í—˜ IDì— ë”°ë¼ ì›Œí¬í”Œë¡œìš° í…œí”Œë¦¿ ë°˜í™˜

    Args:
        experiment_id: ëª¨ë¸ ID (portrait_mode, product_mode, hybrid_mode, FLUX.1-dev-Q8, FLUX.1-dev-Q4)

    Returns:
        ComfyUI ì›Œí¬í”Œë¡œìš° JSON
    """
    # ìƒˆë¡œìš´ í¸ì§‘ ëª¨ë“œ
    if experiment_id == "portrait_mode":
        return get_portrait_mode_workflow()
    elif experiment_id == "product_mode":
        return get_product_mode_workflow()
    elif experiment_id == "hybrid_mode":
        return get_hybrid_mode_workflow()

    # ê¸°ì¡´ ì‹¤í—˜ (í•˜ìœ„ í˜¸í™˜ì„±)
    elif experiment_id == "ben2_flux_fill":
        return get_ben2_flux_fill_workflow()
    elif experiment_id == "ben2_qwen_image":
        return get_ben2_qwen_workflow()
    elif experiment_id in ["FLUX.1-dev-Q8", "FLUX.1-dev-Q4"]:
        return get_flux_t2i_workflow()
    else:
        raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ì‹¤í—˜ ID: {experiment_id}")


def get_flux_t2i_workflow() -> Dict[str, Any]:
    """
    FLUX T2I ì›Œí¬í”Œë¡œìš° í…œí”Œë¦¿ (GGUF ëª¨ë¸ ì‚¬ìš©)
    """
    workflow = {
        # ë…¸ë“œ 1: FLUX UNET ë¡œë“œ (GGUF)
        "1": {
            "class_type": "UnetLoaderGGUF",
            "inputs": {
                "unet_name": "flux1-dev-Q8_0.gguf"  # ëŸ°íƒ€ì„ì— ë³€ê²½
            }
        },

        # ë…¸ë“œ 2: Dual CLIP ë¡œë“œ (CLIP-L + T5-XXL)
        "2": {
            "class_type": "DualCLIPLoaderGGUF",
            "inputs": {
                "clip_name1": "clip_l.safetensors",
                "clip_name2": "t5-v1_1-xxl-encoder-Q8_0.gguf",
                "type": "flux"
            }
        },

        # ë…¸ë“œ 3: í”„ë¡¬í”„íŠ¸ ì¸ì½”ë”©
        "3": {
            "class_type": "CLIPTextEncode",
            "inputs": {
                "text": "",  # ëŸ°íƒ€ì„ì— ì„¤ì •
                "clip": ["2", 0]
            }
        },

        # [NEW] ë…¸ë“œ 30: Negative í”„ë¡¬í”„íŠ¸ (FluxëŠ” ë³´í†µ ë¹„ì›Œë‘ )
        "30": {
            "class_type": "CLIPTextEncode",
            "inputs": {
                "text": "",  # í•­ìƒ ë¹„ì›Œë‘ 
                "clip": ["2", 0]
            }
        },

        # ----------------------------------------------------------
        # [í•µì‹¬ ì¶”ê°€] ë…¸ë“œ 35: Flux Guidance
        # ì´ ë…¸ë“œê°€ ìˆì–´ì•¼ í”„ë¡¬í”„íŠ¸ ë§ì„ ì˜ ë“£ìŠµë‹ˆë‹¤.
        # guidance ê°’ì´ ë†’ì„ìˆ˜ë¡(ì˜ˆ: 6.0) í”„ë¡¬í”„íŠ¸ ë°˜ì˜ë¥ ì´ ì˜¬ë¼ê°‘ë‹ˆë‹¤.
        # ----------------------------------------------------------
        "35": {
            "class_type": "FluxGuidance",
            "inputs": {
                "conditioning": ["3", 0], # Positive í”„ë¡¬í”„íŠ¸ë¥¼ ë°›ìŒ
                "guidance": 50 # [ì¤‘ìš”] ê¸°ë³¸ê°’ 3.5 -> 4.5~6.0ìœ¼ë¡œ ì˜¬ë¦¬ì„¸ìš”!
            }
        },

        # ë…¸ë“œ 4: VAE ë¡œë“œ
        "4": {
            "class_type": "VAELoader",
            "inputs": {
                "vae_name": "ae.safetensors"
            }
        },

        # ë…¸ë“œ 5: Empty Latent
        "5": {
            "class_type": "EmptyLatentImage",
            "inputs": {
                "width": 1024,  # ëŸ°íƒ€ì„ì— ì„¤ì •
                "height": 1024,  # ëŸ°íƒ€ì„ì— ì„¤ì •
                "batch_size": 1
            }
        },

        # ë…¸ë“œ 6: KSampler
        "6": {
            "class_type": "KSampler",
            "inputs": {
                "seed": 0,  # ëŸ°íƒ€ì„ì— ì„¤ì •
                "steps": 28,  # ëŸ°íƒ€ì„ì— ì„¤ì •
                "cfg": 1.0,  # ëŸ°íƒ€ì„ì— ì„¤ì •
                "sampler_name": "euler",
                "scheduler": "simple",
                "denoise": 1.0,
                "model": ["1", 0],
                "positive": ["35", 0],
                "negative": ["30", 0],  # FLUXëŠ” negative ë¶ˆí•„ìš”
                "latent_image": ["5", 0]
            }
        },

        # ë…¸ë“œ 7: VAE Decode
        "7": {
            "class_type": "VAEDecode",
            "inputs": {
                "samples": ["6", 0],
                "vae": ["4", 0]
            }
        },

        # ë…¸ë“œ 8: Save Image
        "8": {
            "class_type": "SaveImage",
            "inputs": {
                "filename_prefix": "flux_t2i",
                "images": ["7", 0]
            }
        }
    }

    return workflow


def get_flux_t2i_with_impact_workflow() -> Dict[str, Any]:
    """
    FLUX T2I + Impact Pack (FaceDetailer) ì›Œí¬í”Œë¡œìš°
    """
    workflow = get_flux_t2i_workflow()

    # ê¸°ì¡´ ë…¸ë“œ 8 (SaveImage) ì‚­ì œ
    del workflow["8"]

    # BBox Detector ë…¸ë“œ (ì–¼êµ´ ê°ì§€ìš©)
    workflow["9"] = {
        "class_type": "UltralyticsDetectorProvider",
        "inputs": {
            "model_name": "bbox/face_yolov8m.pt"
        }
    }

    # FaceDetailer ë…¸ë“œ ì¶”ê°€ (ë…¸ë“œ 7 (VAEDecode) ì´í›„)
    workflow["10"] = {
        "class_type": "FaceDetailer",
        "inputs": {
            "image": ["7", 0],  # VAE Decode ì¶œë ¥
            "model": ["1", 0],  # UNET
            "clip": ["2", 0],   # CLIP
            "vae": ["4", 0],    # VAE
            "positive": ["35", 0],  # FluxGuidance ì¶œë ¥ (ì¡°ê±´ë¶€)
            "negative": ["30", 0],  # Negative í”„ë¡¬í”„íŠ¸ (ë¹ˆ í…ìŠ¤íŠ¸)
            "bbox_detector": ["9", 0],  # BBox Detector
            "wildcard": "",
            "cycle": 1,
            "guide_size": 512,
            "guide_size_for": True,
            "max_size": 1024,
            "seed": 0,  # ëŸ°íƒ€ì„ì— ì„¤ì •
            "steps": 3,  # ì–¼êµ´ í›„ì²˜ë¦¬ steps
            "cfg": 8.0,
            "sampler_name": "euler",
            "scheduler": "normal",
            "denoise": 0.5,
            "feather": 5,
            "noise_mask": True,
            "force_inpaint": True,
            "bbox_threshold": 0.5,
            "bbox_dilation": 10,
            "bbox_crop_factor": 3.0,
            "sam_detection_hint": "center-1",
            "sam_dilation": 0,
            "sam_threshold": 0.93,
            "sam_bbox_expansion": 0,
            "sam_mask_hint_threshold": 0.7,
            "sam_mask_hint_use_negative": "False",
            "drop_size": 10
        }
    }

    # Hand Detector ë…¸ë“œ (ì† ê°ì§€ìš©)
    workflow["12"] = {
        "class_type": "UltralyticsDetectorProvider",
        "inputs": {
            "model_name": "bbox/hand_yolov8s.pt"
        }
    }

    # HandDetailer ë…¸ë“œ ì¶”ê°€ (FaceDetailer ì¶œë ¥ ì´í›„)
    workflow["13"] = {
        "class_type": "FaceDetailer",  # HandDetailerëŠ” FaceDetailerì™€ ê°™ì€ ë…¸ë“œ ì‚¬ìš©
        "inputs": {
            "image": ["10", 0],  # FaceDetailer ì¶œë ¥
            "model": ["1", 0],
            "clip": ["2", 0],
            "vae": ["4", 0],
            "positive": ["35", 0],
            "negative": ["30", 0],
            "bbox_detector": ["12", 0],  # Hand Detector
            "wildcard": "",
            "cycle": 1,
            "guide_size": 384,  # ì†ì€ ì‘ìœ¼ë¯€ë¡œ guide_size ì¤„ì„
            "guide_size_for": True,
            "max_size": 1024,
            "seed": 0,
            "steps": 6,
            "cfg": 8.0,
            "sampler_name": "euler",
            "scheduler": "normal",
            "denoise": 0.5,
            "feather": 5,
            "noise_mask": True,
            "force_inpaint": True,
            "bbox_threshold": 0.5,
            "bbox_dilation": 10,
            "bbox_crop_factor": 3.0,
            "sam_detection_hint": "center-1",
            "sam_dilation": 0,
            "sam_threshold": 0.93,
            "sam_bbox_expansion": 0,
            "sam_mask_hint_threshold": 0.7,
            "sam_mask_hint_use_negative": "False",
            "drop_size": 10
        }
    }

    # Save Imageë¥¼ HandDetailer ì¶œë ¥ìœ¼ë¡œ ì—°ê²°
    workflow["11"] = {
        "class_type": "SaveImage",
        "inputs": {
            "filename_prefix": "flux_t2i_impact",
            "images": ["13", 0]  # HandDetailer ì¶œë ¥
        }
    }

    return workflow


def get_flux_i2i_workflow() -> Dict[str, Any]:
    """
    FLUX I2I ì›Œí¬í”Œë¡œìš° í…œí”Œë¦¿ (GGUF)
    """
    workflow = {
        # ë…¸ë“œ 1: FLUX UNET ë¡œë“œ (GGUF)
        "1": {
            "class_type": "UnetLoaderGGUF",
            "inputs": {
                "unet_name": "flux1-dev-Q8_0.gguf"  # ëŸ°íƒ€ì„ì— ë³€ê²½
            }
        },

        # ë…¸ë“œ 2: Dual CLIP ë¡œë“œ (CLIP-L + T5-XXL)
        "2": {
            "class_type": "DualCLIPLoaderGGUF",
            "inputs": {
                "clip_name1": "clip_l.safetensors",
                "clip_name2": "t5-v1_1-xxl-encoder-Q8_0.gguf",
                "type": "flux"
            }
        },

        # ë…¸ë“œ 3: í”„ë¡¬í”„íŠ¸ ì¸ì½”ë”©
        "3": {
            "class_type": "CLIPTextEncode",
            "inputs": {
                "text": "",  # ëŸ°íƒ€ì„ì— ì„¤ì •
                "clip": ["2", 0]
            }
        },

        # ë…¸ë“œ 4: VAE ë¡œë“œ
        "4": {
            "class_type": "VAELoader",
            "inputs": {
                "vae_name": "ae.safetensors"
            }
        },

        # ë…¸ë“œ 5: ì´ë¯¸ì§€ ë¡œë“œ
        "5": {
            "class_type": "LoadImage",
            "inputs": {
                "image": "input.png"  # ëŸ°íƒ€ì„ì— ì„¤ì •
            }
        },

        # ë…¸ë“œ 6: VAE Encode
        "6": {
            "class_type": "VAEEncode",
            "inputs": {
                "pixels": ["5", 0],
                "vae": ["4", 0]
            }
        },

        # ë…¸ë“œ 7: KSampler
        "7": {
            "class_type": "KSampler",
            "inputs": {
                "seed": 0,  # ëŸ°íƒ€ì„ì— ì„¤ì •
                "steps": 28,  # ëŸ°íƒ€ì„ì— ì„¤ì •
                "cfg": 3.5,  # ëŸ°íƒ€ì„ì— ì„¤ì •
                "sampler_name": "euler",
                "scheduler": "normal",
                "denoise": 0.75,  # strength, ëŸ°íƒ€ì„ì— ì„¤ì •
                "model": ["1", 0],
                "positive": ["3", 0],
                "negative": ["3", 0],  # FLUXëŠ” negative ë¶ˆí•„ìš”
                "latent_image": ["6", 0]
            }
        },

        # ë…¸ë“œ 8: VAE Decode
        "8": {
            "class_type": "VAEDecode",
            "inputs": {
                "samples": ["7", 0],
                "vae": ["4", 0]
            }
        },

        # ë…¸ë“œ 9: Save Image
        "9": {
            "class_type": "SaveImage",
            "inputs": {
                "filename_prefix": "flux_i2i",
                "images": ["8", 0]
            }
        }
    }

    return workflow


def get_ben2_flux_fill_workflow() -> Dict[str, Any]:
    """
    BEN2 + FLUX.1-Fill ì›Œí¬í”Œë¡œìš° í…œí”Œë¦¿ (ì˜¬ë°”ë¥¸ Inpainting êµ¬ì¡°)

    ì›Œí¬í”Œë¡œìš°:
    1. ì›ë³¸ ì´ë¯¸ì§€ ë¡œë“œ
    2. BEN2ë¡œ ë°°ê²½ ì œê±°í•˜ì—¬ ë§ˆìŠ¤í¬ ìƒì„±
    3. ë§ˆìŠ¤í¬ë¥¼ ì‚¬ìš©í•´ InpaintModelConditioning ì ìš©
    4. FLUX.1-Fillë¡œ ë°°ê²½ ì±„ìš°ê¸°
    """
    workflow = {
        # ë…¸ë“œ 1: ì›ë³¸ ì´ë¯¸ì§€ ë¡œë“œ
        "1": {
            "class_type": "LoadImage",
            "inputs": {
                "image": "input.png"  # ì—…ë¡œë“œëœ ì´ë¯¸ì§€ ì´ë¦„ (ëŸ°íƒ€ì„ì— ì„¤ì •)
            }
        },

        # ë…¸ë“œ 2: BEN2 ë°°ê²½ ì œê±° (ì¶œë ¥: IMAGE, MASK, MASK_IMAGE)
        "2": {
            "class_type": "RMBG",
            "inputs": {
                "image": ["1", 0],
                "model": "BEN2",
                "sensitivity": 1.0,
                "process_res": 1024,
                "mask_blur": 0,
                "mask_offset": 0,
                "background": "Alpha",
                "invert_output": True  # True: ë°°ê²½ë§Œ í¸ì§‘, False: ì‚¬ëŒë§Œ í¸ì§‘
            }
        },

        # ë…¸ë“œ 3: FLUX.1-Fill UNET ë¡œë“œ (GGUF)
        "3": {
            "class_type": "UnetLoaderGGUF",
            "inputs": {
                "unet_name": "FLUX.1-Fill-dev-Q8_0.gguf"
            }
        },

        # ë…¸ë“œ 4: CLIP ë¡œë“œ (DualCLIPLoaderGGUF)
        "4": {
            "class_type": "DualCLIPLoaderGGUF",
            "inputs": {
                "clip_name1": "t5-v1_1-xxl-encoder-Q8_0.gguf",
                "clip_name2": "clip_l.safetensors",
                "type": "flux"
            }
        },

        # ë…¸ë“œ 5: VAE ë¡œë“œ
        "5": {
            "class_type": "VAELoader",
            "inputs": {
                "vae_name": "ae.safetensors"
            }
        },

        # ë…¸ë“œ 6: í”„ë¡¬í”„íŠ¸ ì¸ì½”ë”© (FLUXìš©)
        "6": {
            "class_type": "CLIPTextEncodeFlux",
            "inputs": {
                "clip": ["4", 0],
                "clip_l": "",  # ëŸ°íƒ€ì„ì— ì„¤ì •
                "t5xxl": "",   # ëŸ°íƒ€ì„ì— ì„¤ì •
                "guidance": 3.5  # FLUX.1-Fill Inpainting: 3.5~7.5 ê¶Œì¥
            }
        },

        # ë…¸ë“œ 7: FluxGuidance
        "7": {
            "class_type": "FluxGuidance",
            "inputs": {
                "conditioning": ["6", 0],
                "guidance": 3.5  # FLUX.1-Fill Inpainting: 3.5~7.5 (ëŸ°íƒ€ì„ì— ì„¤ì •)
            }
        },

        # ë…¸ë“œ 8: VAEEncodeForInpaint (ì›ë³¸ ì´ë¯¸ì§€ + ë§ˆìŠ¤í¬)
        "8": {
            "class_type": "VAEEncodeForInpaint",
            "inputs": {
                "pixels": ["1", 0],  # ì›ë³¸ ì´ë¯¸ì§€
                "vae": ["5", 0],
                "mask": ["2", 1],  # BEN2ì˜ ë§ˆìŠ¤í¬ ì¶œë ¥
                "grow_mask_by": 6
            }
        },

        # ë…¸ë“œ 9: InpaintModelConditioning
        "9": {
            "class_type": "InpaintModelConditioning",
            "inputs": {
                "positive": ["7", 0],  # FluxGuidanceì˜ conditioning
                "negative": ["7", 0],  # FLUXëŠ” negativeë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì§€ë§Œ í•„ìˆ˜ ì…ë ¥
                "pixels": ["1", 0],    # ì›ë³¸ ì´ë¯¸ì§€
                "vae": ["5", 0],
                "mask": ["2", 1],      # BEN2ì˜ ë§ˆìŠ¤í¬ ì¶œë ¥
                "noise_mask": True     # Boolean - ë…¸ì´ì¦ˆ ë§ˆìŠ¤í¬ ì‚¬ìš© ì—¬ë¶€
            }
        },

        # ë…¸ë“œ 10: KSampler
        "10": {
            "class_type": "KSampler",
            "inputs": {
                "seed": 0,  # ëŸ°íƒ€ì„ì— ì„¤ì •
                "steps": 28,
                "cfg": 1.0,  # FluxGuidanceë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ 1.0
                "sampler_name": "euler",
                "scheduler": "simple",
                "denoise": 1.0,  # Inpaintingì€ ë§ˆìŠ¤í¬ ì˜ì—­ì„ ì™„ì „íˆ ìƒˆë¡œ ìƒì„±í•´ì•¼ í•¨
                "model": ["3", 0],
                "positive": ["9", 0],
                "negative": ["9", 1],
                "latent_image": ["8", 0]
            }
        },

        # ë…¸ë“œ 11: VAE Decode
        "11": {
            "class_type": "VAEDecode",
            "inputs": {
                "samples": ["10", 0],
                "vae": ["5", 0]
            }
        },

        # ë…¸ë“œ 12: SaveImage
        "12": {
            "class_type": "SaveImage",
            "inputs": {
                "filename_prefix": "ben2_flux_fill",
                "images": ["11", 0]
            }
        }
    }

    return workflow



def get_ben2_qwen_workflow() -> Dict[str, Any]:
    """
    BEN2 + Qwen-Image-Edit (GGUF) ì›Œí¬í”Œë¡œìš° í…œí”Œë¦¿
    
    ì›Œí¬í”Œë¡œìš°:
    1. ì›ë³¸ ì´ë¯¸ì§€ ë¡œë“œ
    2. BEN2ë¡œ ë°°ê²½ ì œê±°
    3. Qwen-Image-Edit (GGUF) + CLIP + VAE ë¡œë“œ
    4. TextEncodeQwenImageEditPlus (Positive/Negative)
    5. KSampler (Editing)
    """
    workflow = {
        # ë…¸ë“œ 1: ì´ë¯¸ì§€ ë¡œë“œ
        "1": {
            "class_type": "LoadImage",
            "inputs": {
                "image": "input.png"
            }
        },

        # ë…¸ë“œ 2: BEN2 ë°°ê²½ ì œê±°
        "2": {
            "class_type": "RMBG",
            "inputs": {
                "image": ["1", 0],
                "model": "BEN2",
                "sensitivity": 1.0,
                "process_res": 1024,
                "mask_blur": 0,
                "mask_offset": 0,
                "background": "Alpha",
                "invert_output": True
            }
        },

        # ë…¸ë“œ 3: ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ (Qwenì€ ì¼ì •í•œ í”½ì…€ ìˆ˜ í•„ìš”)
        "3": {
            "class_type": "ImageScaleToTotalPixels",
            "inputs": {
                "image": ["2", 0],  # BEN2 ì¶œë ¥
                "upscale_method": "bicubic",
                "megapixels": 1.0  # 1 ë©”ê°€í”½ì…€ (1024x1024)
            }
        },

        # ë…¸ë“œ 11: Qwen-Image-Edit UNET ë¡œë“œ (GGUF)
        "11": {
            "class_type": "UnetLoaderGGUF",
            "inputs": {
                "unet_name": "qwen-image-edit/Qwen-Image-Edit-2509-Q8_0.gguf"
            }
        },

        # ë…¸ë“œ 4: Dual CLIP ë¡œë“œ (Qwenìš©)
        "4": {
            "class_type": "DualCLIPLoaderGGUF",
            "inputs": {
                "clip_name1": "clip_l.safetensors",
                "clip_name2": "t5-v1_1-xxl-encoder-Q8_0.gguf",
                "type": "flux"
            }
        },

        # ë…¸ë“œ 5: VAE ë¡œë“œ
        "5": {
            "class_type": "VAELoader",
            "inputs": {
                "vae_name": "ae.safetensors"
            }
        },

        # ë…¸ë“œ 6: Qwen Positive Conditioning (TextEncodeQwenImageEditPlus)
        "6": {
            "class_type": "TextEncodeQwenImageEditPlus_lrzjason",
            "inputs": {
                "clip": ["4", 0],
                "prompt": "",  # ëŸ°íƒ€ì„ì— ì„¤ì •
                "vae": ["5", 0],
                "image1": ["3", 0],  # ë¦¬ì‚¬ì´ì¦ˆëœ ì´ë¯¸ì§€
                "enable_resize": True,
                "enable_vl_resize": True,
                "skip_first_image_resize": False,
                "upscale_method": "bicubic",
                "crop": "center",
                "instruction": "Describe the key features of the input image (color, shape, size, texture, objects, background), then explain how the user's text instruction should alter or modify the image. Generate a new image that meets the user's requirements while maintaining consistency with the original input where appropriate."
            }
        },

        # ë…¸ë“œ 7: Negative Conditioning (í…ìŠ¤íŠ¸ë§Œ, ì´ë¯¸ì§€ ì—†ìŒ)
        "7": {
            "class_type": "CLIPTextEncode",
            "inputs": {
                "clip": ["4", 0],
                "text": "text, watermark, low quality, blurry"
            }
        },

        # ë…¸ë“œ 8: KSampler
        "8": {
            "class_type": "KSampler",
            "inputs": {
                "seed": 0,
                "steps": 28,
                "cfg": 3.5,
                "sampler_name": "euler",
                "scheduler": "simple",
                "denoise": 0.8,  # Strength
                "model": ["11", 0],  # Qwen UNET
                "positive": ["6", 0],  # Qwen Positive Conditioning
                "negative": ["7", 0],  # Simple Negative Conditioning
                "latent_image": ["6", 6]  # Latent from TextEncodeQwenImageEditPlus
            }
        },

        # ë…¸ë“œ 9: VAE Decode
        "9": {
            "class_type": "VAEDecode",
            "inputs": {
                "samples": ["8", 0],
                "vae": ["5", 0]
            }
        },

        # ë…¸ë“œ 10: ì´ë¯¸ì§€ ì €ì¥
        "10": {
            "class_type": "SaveImage",
            "inputs": {
                "filename_prefix": "qwen_output",
                "images": ["9", 0]
            }
        }
    }

    return workflow


def update_flux_t2i_workflow(
    workflow: Dict[str, Any],
    model_name: str,
    prompt: str,
    width: int,
    height: int,
    steps: int,
    guidance_scale: float,
    seed: int = None
) -> Dict[str, Any]:
    """FLUX T2I ì›Œí¬í”Œë¡œìš° íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ (GGUF)"""
    import random
    from .model_registry import get_model_config

    if seed is None:
        seed = random.randint(0, 2**32 - 1)

    # configì—ì„œ model_id ê°€ì ¸ì˜¤ê¸°
    model_config = get_model_config()
    model_id = model_config["models"][model_name]["id"]

    # UNET GGUF íŒŒì¼ ì„¤ì • (ë…¸ë“œ 1)
    workflow["1"]["inputs"]["unet_name"] = model_id

    # í”„ë¡¬í”„íŠ¸ ì„¤ì • (ë…¸ë“œ 3)
    workflow["3"]["inputs"]["text"] = prompt

    # ì´ë¯¸ì§€ í¬ê¸° ì„¤ì • (ë…¸ë“œ 5: EmptyLatentImage)
    workflow["5"]["inputs"]["width"] = width
    workflow["5"]["inputs"]["height"] = height

    # ------------------------------------------------------------
    # [í•µì‹¬ ìˆ˜ì •] Guidance ê°’ì„ ì˜¬ë°”ë¥¸ ë…¸ë“œ(35ë²ˆ)ì— ì—°ê²°!
    # ------------------------------------------------------------
    
    # 1. KSampler (ë…¸ë“œ 6)
    workflow["6"]["inputs"]["seed"] = seed
    workflow["6"]["inputs"]["steps"] = steps
    workflow["6"]["inputs"]["cfg"] = 1.0  # [ì¤‘ìš”] CFGëŠ” 1.0 ê³ ì •! (ì›¹ ê°’ ë¬´ì‹œ)

    # 2. FluxGuidance (ë…¸ë“œ 35)
    if "35" in workflow:
        workflow["35"]["inputs"]["guidance"] = guidance_scale # ì›¹ ìŠ¬ë¼ì´ë” ê°’ì„ ì—¬ê¸°ì—!
    else:
        print("âš ï¸ ê²½ê³ : FluxGuidance(35) ë…¸ë“œê°€ í…œí”Œë¦¿ì— ì—†ìŠµë‹ˆë‹¤!")

    return workflow


def update_flux_i2i_workflow(
    workflow: Dict[str, Any],
    model_name: str,
    prompt: str,
    strength: float,
    steps: int,
    guidance_scale: float,
    seed: int = None
) -> Dict[str, Any]:
    """FLUX I2I ì›Œí¬í”Œë¡œìš° íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ (GGUF)"""
    import random
    from .model_registry import get_model_config

    if seed is None:
        seed = random.randint(0, 2**32 - 1)

    # configì—ì„œ model_id ê°€ì ¸ì˜¤ê¸°
    model_config = get_model_config()
    model_id = model_config["models"][model_name]["id"]

    # UNET GGUF íŒŒì¼ ì„¤ì • (ë…¸ë“œ 1)
    workflow["1"]["inputs"]["unet_name"] = model_id

    # í”„ë¡¬í”„íŠ¸ ì„¤ì • (ë…¸ë“œ 3)
    workflow["3"]["inputs"]["text"] = prompt

    # ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„° ì„¤ì • (ë…¸ë“œ 7: KSampler)
    workflow["7"]["inputs"]["seed"] = seed
    workflow["7"]["inputs"]["steps"] = steps
    workflow["7"]["inputs"]["cfg"] = guidance_scale
    workflow["7"]["inputs"]["denoise"] = strength

    return workflow


def update_workflow_inputs(
    workflow: Dict[str, Any],
    experiment_id: str,
    prompt: str,
    negative_prompt: str = "",
    steps: int = None,
    guidance_scale: float = None,
    strength: float = None,
    seed: int = None
) -> Dict[str, Any]:
    """
    ì›Œí¬í”Œë¡œìš° í…œí”Œë¦¿ì— ì‚¬ìš©ì ì…ë ¥ ë°˜ì˜

    Args:
        workflow: ì›Œí¬í”Œë¡œìš° í…œí”Œë¦¿
        experiment_id: ì‹¤í—˜ ID
        prompt: í¸ì§‘ í”„ë¡¬í”„íŠ¸ (positive)
        negative_prompt: ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸
        steps: ì¶”ë¡  ë‹¨ê³„
        guidance_scale: Guidance scale
        strength: ë³€í™” ê°•ë„
        seed: ëœë¤ ì‹œë“œ

    Returns:
        ì—…ë°ì´íŠ¸ëœ ì›Œí¬í”Œë¡œìš°
    """
    config = load_image_editing_config()

    # ì‹¤í—˜ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    experiment = None
    for exp in config.get("experiments", []):
        if exp["id"] == experiment_id:
            experiment = exp
            break

    if not experiment:
        raise ValueError(f"ì‹¤í—˜ IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {experiment_id}")

    # ê¸°ë³¸ê°’ ì„¤ì •
    params = experiment.get("params", {})
    steps = steps or params.get("default_steps", 28)
    guidance_scale = guidance_scale or params.get("guidance_scale", 3.5)
    strength = strength or params.get("strength", 0.8)

    # ëœë¤ ì‹œë“œ ìƒì„±
    if seed is None:
        import random
        seed = random.randint(0, 2**32 - 1)

    # ì›Œí¬í”Œë¡œìš° ì—…ë°ì´íŠ¸
    if experiment_id == "ben2_flux_fill":
        # í”„ë¡¬í”„íŠ¸ ì„¤ì • (ë…¸ë“œ 6 - CLIPTextEncodeFlux)
        if "6" in workflow:
            workflow["6"]["inputs"]["clip_l"] = prompt
            workflow["6"]["inputs"]["t5xxl"] = prompt

        # ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸ ë…¸ë“œ ì¶”ê°€ (ë…¸ë“œ 6_neg)
        if negative_prompt.strip():
            # ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸ìš© ë³„ë„ ë…¸ë“œ ìƒì„±
            workflow["6_neg"] = {
                "class_type": "CLIPTextEncodeFlux",
                "inputs": {
                    "clip": ["4", 0],
                    "clip_l": negative_prompt,
                    "t5xxl": negative_prompt,
                    "guidance": guidance_scale
                }
            }
            # InpaintModelConditioningì˜ negativeë¥¼ 6_negë¡œ ë³€ê²½
            if "9" in workflow:
                workflow["9"]["inputs"]["negative"] = ["6_neg", 0]

        # FluxGuidance ì„¤ì • (ë…¸ë“œ 7)
        if "7" in workflow:
            workflow["7"]["inputs"]["guidance"] = guidance_scale

        # KSampler ì„¤ì • (ë…¸ë“œ 10)
        if "10" in workflow:
            workflow["10"]["inputs"]["seed"] = seed
            workflow["10"]["inputs"]["steps"] = steps

    elif experiment_id == "ben2_qwen_image":
        # í”„ë¡¬í”„íŠ¸ ì„¤ì • (ë…¸ë“œ 6: Qwen Positive Conditioning)
        if "6" in workflow:
            workflow["6"]["inputs"]["prompt"] = prompt

        # ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸ (ë…¸ë“œ 7: CLIPTextEncode)
        if "7" in workflow and negative_prompt:
            workflow["7"]["inputs"]["text"] = negative_prompt

        # KSampler ì„¤ì • (ë…¸ë“œ 8)
        if "8" in workflow:
            workflow["8"]["inputs"]["seed"] = seed
            workflow["8"]["inputs"]["steps"] = steps
            workflow["8"]["inputs"]["cfg"] = guidance_scale
            workflow["8"]["inputs"]["denoise"] = strength

    return workflow


# í”„ë¦¬ë¡œë“œ ê¸°ëŠ¥ ì œê±°ë¨
def _removed_get_preload_workflow(experiment_id: str) -> Dict[str, Any]:
    """
    ëª¨ë¸ í”„ë¦¬ë¡œë”©ìš© ì›Œí¬í”Œë¡œìš° ìƒì„±
    
    ì „ëµ:
    1. ì „ì²´ ì›Œí¬í”Œë¡œìš° ìœ ì§€ (ë…¸ë“œ ì‚­ì œí•˜ë©´ ì—ëŸ¬ ë°œìƒ)
    2. Steps=1ë¡œ ìµœì†Œí™” (ì†ë„ í™•ë³´)
    3. SaveImage -> PreviewImage êµì²´ (ë””ìŠ¤í¬ ì €ì¥ ë°©ì§€)
    
    Args:
        experiment_id: ëª¨ë¸ ID
        
    Returns:
        í”„ë¦¬ë¡œë”©ìš© ì›Œí¬í”Œë¡œìš° JSON
    """
    import copy
    workflow = copy.deepcopy(get_workflow_template(experiment_id))
    
    if experiment_id == "ben2_flux_fill":
        # 1. íŒŒë¼ë¯¸í„° ìµœì†Œí™”
        if "10" in workflow:  # KSampler (ë…¸ë“œ 10)
            workflow["10"]["inputs"]["steps"] = 1
            workflow["10"]["inputs"]["seed"] = 0
        
        if "6" in workflow:  # Prompt
            workflow["6"]["inputs"]["clip_l"] = "preload"
            workflow["6"]["inputs"]["t5xxl"] = "preload"
            
        # 2. SaveImage(12) -> PreviewImage êµì²´
        if "12" in workflow:
            workflow["12"] = {
                "class_type": "PreviewImage",
                "inputs": {
                    "images": ["11", 0]
                }
            }
            
    elif experiment_id == "ben2_qwen_image":
        # 1. íŒŒë¼ë¯¸í„° ìµœì†Œí™”
        if "8" in workflow:  # KSampler
            workflow["8"]["inputs"]["steps"] = 1
            workflow["8"]["inputs"]["seed"] = 0
            
        if "6" in workflow:  # Prompt
            workflow["6"]["inputs"]["prompt"] = "preload"
            
        # 2. SaveImage(10) -> PreviewImage êµì²´
        if "10" in workflow:
            workflow["10"] = {
                "class_type": "PreviewImage",
                "inputs": {
                    "images": ["9", 0]
                }
            }
                
    elif experiment_id in ["FLUX.1-dev-Q8", "FLUX.1-dev-Q4"]:
        # 1. íŒŒë¼ë¯¸í„° ìµœì†Œí™”
        if "4" in workflow:  # KSampler
            workflow["4"]["inputs"]["steps"] = 1
            workflow["4"]["inputs"]["seed"] = 0
            
        if "2" in workflow:  # Prompt
            workflow["2"]["inputs"]["text"] = "preload"
            
        # 2. SaveImage(6) -> PreviewImage êµì²´
        if "6" in workflow:
            workflow["6"] = {
                "class_type": "PreviewImage",
                "inputs": {
                    "images": ["5", 0]
                }
            }
            
    return workflow




def get_workflow_input_image_node_id(experiment_id: str) -> str:
    """
    ì›Œí¬í”Œë¡œìš°ì˜ ì…ë ¥ ì´ë¯¸ì§€ ë…¸ë“œ ID ë°˜í™˜

    Args:
        experiment_id: ì‹¤í—˜ ID

    Returns:
        ë…¸ë“œ ID (ì˜ˆ: "1")
    """
    # ëª¨ë“  ì›Œí¬í”Œë¡œìš°ì—ì„œ ë…¸ë“œ 1ì´ LoadImage
    return "1"


# ============================================================
# ìƒˆë¡œìš´ í¸ì§‘ ëª¨ë“œ ì›Œí¬í”Œë¡œìš° (v3.0)
# ============================================================

def get_portrait_mode_workflow() -> Dict[str, Any]:
    """
    ğŸŸ¢ Portrait Mode ì›Œí¬í”Œë¡œìš°
    
    íŒŒì´í”„ë¼ì¸:
    1. ì–¼êµ´ ê°ì§€ (Face Detector)
    2. ë§ˆìŠ¤í¬ ë°˜ì „ (ì–¼êµ´ ì œì™¸)
    3. ControlNet ê°€ì´ë“œ ì¶”ì¶œ (Depth/Canny)
    4. Masked I2I ìƒì„± (ì˜·/ë°°ê²½ë§Œ ë³€ê²½)
    """
    workflow = {
        # ë…¸ë“œ 1: ì…ë ¥ ì´ë¯¸ì§€ ë¡œë“œ
        "1": {
            "class_type": "LoadImage",
            "inputs": {
                "image": "input.png"  # ëŸ°íƒ€ì„ì— ë³€ê²½
            }
        },

        # ë…¸ë“œ 2: FLUX UNET ë¡œë“œ
        "2": {
            "class_type": "UnetLoaderGGUF",
            "inputs": {
                "unet_name": "flux1-dev-Q8_0.gguf"
            }
        },

        # ë…¸ë“œ 3: Dual CLIP ë¡œë“œ
        "3": {
            "class_type": "DualCLIPLoaderGGUF",
            "inputs": {
                "clip_name1": "clip_l.safetensors",
                "clip_name2": "t5-v1_1-xxl-encoder-Q8_0.gguf",
                "type": "flux"
            }
        },

        # ë…¸ë“œ 4: VAE ë¡œë“œ
        "4": {
            "class_type": "VAELoader",
            "inputs": {
                "vae_name": "ae.safetensors"
            }
        },

        # ë…¸ë“œ 5: í”„ë¡¬í”„íŠ¸ ì¸ì½”ë”©
        "5": {
            "class_type": "CLIPTextEncode",
            "inputs": {
                "text": "",  # ëŸ°íƒ€ì„ì— ì„¤ì •
                "clip": ["3", 0]
            }
        },

        # ë…¸ë“œ 6: Negative í”„ë¡¬í”„íŠ¸
        "6": {
            "class_type": "CLIPTextEncode",
            "inputs": {
                "text": "",
                "clip": ["3", 0]
            }
        },

        # ë…¸ë“œ 7: FluxGuidance
        "7": {
            "class_type": "FluxGuidance",
            "inputs": {
                "conditioning": ["5", 0],
                "guidance": 3.5  # ëŸ°íƒ€ì„ì— ì„¤ì •
            }
        },

        # ë…¸ë“œ 10: Face Detector
        "10": {
            "class_type": "UltralyticsDetectorProvider",
            "inputs": {
                "model_name": "bbox/face_yolov8m.pt"
            }
        },

        # ë…¸ë“œ 11: SEGS from Detection (ì–¼êµ´ ì˜ì—­ ì„¸ê·¸ë¨¼íŠ¸)
        "11": {
            "class_type": "SAMLoader",
            "inputs": {
                "model_name": "sam_vit_b_01ec64.pth"
            }
        },

        # ë…¸ë“œ 12: BboxDetectorSEGS (ì–¼êµ´ ê°ì§€ ì‹¤í–‰)
        "12": {
            "class_type": "BboxDetectorSEGS",
            "inputs": {
                "image": ["1", 0],
                "bbox_detector": ["10", 0],
                "threshold": 0.5,
                "dilation": 10,
                "crop_factor": 3.0,
                "drop_size": 10
            }
        },

        # ë…¸ë“œ 13: SEGS to Mask (ì–¼êµ´ ë§ˆìŠ¤í¬ ìƒì„±)
        "13": {
            "class_type": "SegsToCombinedMask",
            "inputs": {
                "segs": ["12", 0]
            }
        },

        # ë…¸ë“œ 14: Invert Mask (ì–¼êµ´ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ì˜ì—­)
        "14": {
            "class_type": "InvertMask",
            "inputs": {
                "mask": ["13", 0]
            }
        },

        # ë…¸ë“œ 20: ControlNet Preprocessor (Depth ë˜ëŠ” Canny)
        "20": {
            "class_type": "DepthAnythingPreprocessor",
            "inputs": {
                "image": ["1", 0],
                "ckpt_name": "depth_anything_v2_vitl.pth",
                "resolution": 1024
            }
        },

        # ë…¸ë“œ 21: ControlNet ë¡œë“œ
        "21": {
            "class_type": "ControlNetLoader",
            "inputs": {
                "control_net_name": "InstantX-FLUX.1-dev-Controlnet-Union.safetensors"
            }
        },

        # ë…¸ë“œ 22: Apply ControlNet
        "22": {
            "class_type": "ControlNetApplyAdvanced",
            "inputs": {
                "positive": ["7", 0],
                "negative": ["6", 0],
                "control_net": ["21", 0],
                "image": ["20", 0],
                "strength": 0.7,  # ëŸ°íƒ€ì„ì— ì„¤ì •
                "start_percent": 0.0,
                "end_percent": 1.0
            }
        },

        # ë…¸ë“œ 30: VAE Encode (ì…ë ¥ ì´ë¯¸ì§€)
        "30": {
            "class_type": "VAEEncode",
            "inputs": {
                "pixels": ["1", 0],
                "vae": ["4", 0]
            }
        },

        # ë…¸ë“œ 31: Set Latent Noise Mask (ë§ˆìŠ¤í¬ ì ìš©)
        "31": {
            "class_type": "SetLatentNoiseMask",
            "inputs": {
                "samples": ["30", 0],
                "mask": ["14", 0]  # ë°˜ì „ëœ ë§ˆìŠ¤í¬ (ì–¼êµ´ ì œì™¸)
            }
        },

        # ë…¸ë“œ 40: KSampler (Masked I2I)
        "40": {
            "class_type": "KSampler",
            "inputs": {
                "seed": 0,  # ëŸ°íƒ€ì„ì— ì„¤ì •
                "steps": 28,  # ëŸ°íƒ€ì„ì— ì„¤ì •
                "cfg": 1.0,
                "sampler_name": "euler",
                "scheduler": "simple",
                "denoise": 1.0,  # ëŸ°íƒ€ì„ì— ì„¤ì •
                "model": ["2", 0],
                "positive": ["22", 0],  # ControlNet ì ìš©ëœ ì¡°ê±´
                "negative": ["6", 0],
                "latent_image": ["31", 0]  # ë§ˆìŠ¤í¬ ì ìš©ëœ latent
            }
        },

        # ë…¸ë“œ 41: VAE Decode
        "41": {
            "class_type": "VAEDecode",
            "inputs": {
                "samples": ["40", 0],
                "vae": ["4", 0]
            }
        },

        # ë…¸ë“œ 50: Save Image
        "50": {
            "class_type": "SaveImage",
            "inputs": {
                "filename_prefix": "portrait_mode",
                "images": ["41", 0]
            }
        }
    }

    return workflow


def get_product_mode_workflow() -> Dict[str, Any]:
    """
    ğŸ”µ Product Mode ì›Œí¬í”Œë¡œìš°
    
    íŒŒì´í”„ë¼ì¸:
    1. BEN2ë¡œ ì œí’ˆ ëˆ„ë¼ ë”°ê¸°
    2. Flux Dev T2Ië¡œ ë°°ê²½ ìƒì„±
    3. ë ˆì´ì–´ í•©ì„± (ë°°ê²½ + ì œí’ˆ)
    4. Flux Fillë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ìœµí•©
    """
    workflow = {
        # ë…¸ë“œ 1: ì…ë ¥ ì´ë¯¸ì§€ ë¡œë“œ
        "1": {
            "class_type": "LoadImage",
            "inputs": {
                "image": "input.png"
            }
        },

        # ë…¸ë“œ 2: BEN2 ë°°ê²½ ì œê±°
        "2": {
            "class_type": "BEN2",
            "inputs": {
                "image": ["1", 0]
            }
        },

        # ë…¸ë“œ 10: FLUX UNET ë¡œë“œ (T2Iìš©)
        "10": {
            "class_type": "UnetLoaderGGUF",
            "inputs": {
                "unet_name": "flux1-dev-Q8_0.gguf"
            }
        },

        # ë…¸ë“œ 11: Dual CLIP ë¡œë“œ
        "11": {
            "class_type": "DualCLIPLoaderGGUF",
            "inputs": {
                "clip_name1": "clip_l.safetensors",
                "clip_name2": "t5-v1_1-xxl-encoder-Q8_0.gguf",
                "type": "flux"
            }
        },

        # ë…¸ë“œ 12: VAE ë¡œë“œ
        "12": {
            "class_type": "VAELoader",
            "inputs": {
                "vae_name": "ae.safetensors"
            }
        },

        # ë…¸ë“œ 13: ë°°ê²½ í”„ë¡¬í”„íŠ¸ ì¸ì½”ë”©
        "13": {
            "class_type": "CLIPTextEncode",
            "inputs": {
                "text": "",  # ëŸ°íƒ€ì„ì— ì„¤ì • (ë°°ê²½ í”„ë¡¬í”„íŠ¸)
                "clip": ["11", 0]
            }
        },

        # ë…¸ë“œ 14: Negative í”„ë¡¬í”„íŠ¸
        "14": {
            "class_type": "CLIPTextEncode",
            "inputs": {
                "text": "",
                "clip": ["11", 0]
            }
        },

        # ë…¸ë“œ 15: FluxGuidance
        "15": {
            "class_type": "FluxGuidance",
            "inputs": {
                "conditioning": ["13", 0],
                "guidance": 5.0  # ëŸ°íƒ€ì„ì— ì„¤ì •
            }
        },

        # ë…¸ë“œ 16: Empty Latent (ë°°ê²½ ìƒì„±ìš©)
        "16": {
            "class_type": "EmptyLatentImage",
            "inputs": {
                "width": 1024,  # ëŸ°íƒ€ì„ì— ì„¤ì •
                "height": 1024,
                "batch_size": 1
            }
        },

        # ë…¸ë“œ 17: KSampler (ë°°ê²½ ìƒì„±)
        "17": {
            "class_type": "KSampler",
            "inputs": {
                "seed": 0,  # ëŸ°íƒ€ì„ì— ì„¤ì •
                "steps": 28,
                "cfg": 1.0,
                "sampler_name": "euler",
                "scheduler": "simple",
                "denoise": 1.0,
                "model": ["10", 0],
                "positive": ["15", 0],
                "negative": ["14", 0],
                "latent_image": ["16", 0]
            }
        },

        # ë…¸ë“œ 18: VAE Decode (ë°°ê²½ ì´ë¯¸ì§€)
        "18": {
            "class_type": "VAEDecode",
            "inputs": {
                "samples": ["17", 0],
                "vae": ["12", 0]
            }
        },

        # ë…¸ë“œ 20: ImageCompositeM (ë ˆì´ì–´ í•©ì„±)
        "20": {
            "class_type": "ImageCompositeAbsolute",
            "inputs": {
                "image_base": ["18", 0],  # ë°°ê²½
                "image_overlay": ["2", 0],  # BEN2 ëˆ„ë¼ ì´ë¯¸ì§€
                "x": 0,
                "y": 0
            }
        },

        # ë…¸ë“œ 30: FLUX Fill UNET ë¡œë“œ
        "30": {
            "class_type": "UnetLoaderGGUF",
            "inputs": {
                "unet_name": "FLUX.1-Fill-dev-Q8_0.gguf"
            }
        },

        # ë…¸ë“œ 31: BEN2 ë§ˆìŠ¤í¬ ì¶”ì¶œ
        "31": {
            "class_type": "ImageToMask",
            "inputs": {
                "image": ["2", 0],
                "channel": "alpha"
            }
        },

        # ë…¸ë“œ 32: Invert Mask (ì œí’ˆ ì™¸ê³½ë§Œ)
        "32": {
            "class_type": "InvertMask",
            "inputs": {
                "mask": ["31", 0]
            }
        },

        # ë…¸ë“œ 33: Dilate Mask (ì™¸ê³½ í™•ì¥)
        "33": {
            "class_type": "GrowMask",
            "inputs": {
                "mask": ["31", 0],
                "expand": 10,
                "tapered_corners": True
            }
        },

        # ë…¸ë“œ 40: VAE Encode (í•©ì„± ì´ë¯¸ì§€)
        "40": {
            "class_type": "VAEEncode",
            "inputs": {
                "pixels": ["20", 0],
                "vae": ["12", 0]
            }
        },

        # ë…¸ë“œ 41: Set Latent Noise Mask (ì™¸ê³½ë§Œ ë¸”ë Œë”©)
        "41": {
            "class_type": "SetLatentNoiseMask",
            "inputs": {
                "samples": ["40", 0],
                "mask": ["33", 0]
            }
        },

        # ë…¸ë“œ 42: KSampler (Blending)
        "42": {
            "class_type": "KSampler",
            "inputs": {
                "seed": 0,
                "steps": 28,
                "cfg": 1.0,
                "sampler_name": "euler",
                "scheduler": "simple",
                "denoise": 0.35,  # ëŸ°íƒ€ì„ì— ì„¤ì • (ë¸”ë Œë”© ê°•ë„)
                "model": ["30", 0],  # Flux Fill
                "positive": ["15", 0],  # ë°°ê²½ í”„ë¡¬í”„íŠ¸ ì¬ì‚¬ìš©
                "negative": ["14", 0],
                "latent_image": ["41", 0]
            }
        },

        # ë…¸ë“œ 43: VAE Decode
        "43": {
            "class_type": "VAEDecode",
            "inputs": {
                "samples": ["42", 0],
                "vae": ["12", 0]
            }
        },

        # ë…¸ë“œ 50: Save Image
        "50": {
            "class_type": "SaveImage",
            "inputs": {
                "filename_prefix": "product_mode",
                "images": ["43", 0]
            }
        }
    }

    return workflow


def get_hybrid_mode_workflow() -> Dict[str, Any]:
    """
    ğŸŸ£ Hybrid Mode ì›Œí¬í”Œë¡œìš°
    
    íŒŒì´í”„ë¼ì¸:
    1. ì–¼êµ´ ê°ì§€ (Face Detector)
    2. ì œí’ˆ ê°ì§€ (BEN2)
    3. ë©€í‹° ë§ˆìŠ¤í¬ í•©ì„± (ì–¼êµ´ + ì œí’ˆ)
    4. ë§ˆìŠ¤í¬ ë°˜ì „ (ì˜·/ë°°ê²½ë§Œ ìˆ˜ì •)
    5. ControlNet (Canny) + Masked I2I
    """
    workflow = {
        # ë…¸ë“œ 1: ì…ë ¥ ì´ë¯¸ì§€ ë¡œë“œ
        "1": {
            "class_type": "LoadImage",
            "inputs": {
                "image": "input.png"
            }
        },

        # ë…¸ë“œ 2: FLUX UNET ë¡œë“œ
        "2": {
            "class_type": "UnetLoaderGGUF",
            "inputs": {
                "unet_name": "flux1-dev-Q8_0.gguf"
            }
        },

        # ë…¸ë“œ 3: Dual CLIP ë¡œë“œ
        "3": {
            "class_type": "DualCLIPLoaderGGUF",
            "inputs": {
                "clip_name1": "clip_l.safetensors",
                "clip_name2": "t5-v1_1-xxl-encoder-Q8_0.gguf",
                "type": "flux"
            }
        },

        # ë…¸ë“œ 4: VAE ë¡œë“œ
        "4": {
            "class_type": "VAELoader",
            "inputs": {
                "vae_name": "ae.safetensors"
            }
        },

        # ë…¸ë“œ 5: í”„ë¡¬í”„íŠ¸ ì¸ì½”ë”©
        "5": {
            "class_type": "CLIPTextEncode",
            "inputs": {
                "text": "",  # ëŸ°íƒ€ì„ì— ì„¤ì •
                "clip": ["3", 0]
            }
        },

        # ë…¸ë“œ 6: Negative í”„ë¡¬í”„íŠ¸
        "6": {
            "class_type": "CLIPTextEncode",
            "inputs": {
                "text": "",
                "clip": ["3", 0]
            }
        },

        # ë…¸ë“œ 7: FluxGuidance
        "7": {
            "class_type": "FluxGuidance",
            "inputs": {
                "conditioning": ["5", 0],
                "guidance": 3.5
            }
        },

        # ë…¸ë“œ 10: Face Detector
        "10": {
            "class_type": "UltralyticsDetectorProvider",
            "inputs": {
                "model_name": "bbox/face_yolov8m.pt"
            }
        },

        # ë…¸ë“œ 11: BboxDetectorSEGS (ì–¼êµ´)
        "11": {
            "class_type": "BboxDetectorSEGS",
            "inputs": {
                "image": ["1", 0],
                "bbox_detector": ["10", 0],
                "threshold": 0.5,
                "dilation": 10,
                "crop_factor": 3.0,
                "drop_size": 10
            }
        },

        # ë…¸ë“œ 12: SEGS to Mask (ì–¼êµ´ ë§ˆìŠ¤í¬)
        "12": {
            "class_type": "SegsToCombinedMask",
            "inputs": {
                "segs": ["11", 0]
            }
        },

        # ë…¸ë“œ 15: BEN2 (ì œí’ˆ ëˆ„ë¼)
        "15": {
            "class_type": "BEN2",
            "inputs": {
                "image": ["1", 0]
            }
        },

        # ë…¸ë“œ 16: ImageToMask (ì œí’ˆ ë§ˆìŠ¤í¬)
        "16": {
            "class_type": "ImageToMask",
            "inputs": {
                "image": ["15", 0],
                "channel": "alpha"
            }
        },

        # ë…¸ë“œ 17: MaskComposite (ì–¼êµ´ + ì œí’ˆ ë§ˆìŠ¤í¬ í•©ì„±)
        "17": {
            "class_type": "MaskComposite",
            "inputs": {
                "destination": ["12", 0],  # ì–¼êµ´ ë§ˆìŠ¤í¬
                "source": ["16", 0],  # ì œí’ˆ ë§ˆìŠ¤í¬
                "x": 0,
                "y": 0,
                "operation": "add"  # í•©ì§‘í•©
            }
        },

        # ë…¸ë“œ 18: Invert Mask (ì–¼êµ´+ì œí’ˆ ì œì™¸í•œ ë‚˜ë¨¸ì§€)
        "18": {
            "class_type": "InvertMask",
            "inputs": {
                "mask": ["17", 0]
            }
        },

        # ë…¸ë“œ 20: ControlNet Preprocessor (Canny)
        "20": {
            "class_type": "CannyEdgePreprocessor",
            "inputs": {
                "image": ["1", 0],
                "low_threshold": 100,
                "high_threshold": 200,
                "resolution": 1024
            }
        },

        # ë…¸ë“œ 21: ControlNet ë¡œë“œ
        "21": {
            "class_type": "ControlNetLoader",
            "inputs": {
                "control_net_name": "InstantX-FLUX.1-dev-Controlnet-Union.safetensors"
            }
        },

        # ë…¸ë“œ 22: Apply ControlNet
        "22": {
            "class_type": "ControlNetApplyAdvanced",
            "inputs": {
                "positive": ["7", 0],
                "negative": ["6", 0],
                "control_net": ["21", 0],
                "image": ["20", 0],
                "strength": 0.8,  # ëŸ°íƒ€ì„ì— ì„¤ì •
                "start_percent": 0.0,
                "end_percent": 1.0
            }
        },

        # ë…¸ë“œ 30: VAE Encode
        "30": {
            "class_type": "VAEEncode",
            "inputs": {
                "pixels": ["1", 0],
                "vae": ["4", 0]
            }
        },

        # ë…¸ë“œ 31: Set Latent Noise Mask
        "31": {
            "class_type": "SetLatentNoiseMask",
            "inputs": {
                "samples": ["30", 0],
                "mask": ["18", 0]  # ì–¼êµ´+ì œí’ˆ ì œì™¸
            }
        },

        # ë…¸ë“œ 40: KSampler
        "40": {
            "class_type": "KSampler",
            "inputs": {
                "seed": 0,
                "steps": 28,
                "cfg": 1.0,
                "sampler_name": "euler",
                "scheduler": "simple",
                "denoise": 0.9,  # ëŸ°íƒ€ì„ì— ì„¤ì •
                "model": ["2", 0],
                "positive": ["22", 0],
                "negative": ["6", 0],
                "latent_image": ["31", 0]
            }
        },

        # ë…¸ë“œ 41: VAE Decode
        "41": {
            "class_type": "VAEDecode",
            "inputs": {
                "samples": ["40", 0],
                "vae": ["4", 0]
            }
        },

        # ë…¸ë“œ 50: Save Image
        "50": {
            "class_type": "SaveImage",
            "inputs": {
                "filename_prefix": "hybrid_mode",
                "images": ["41", 0]
            }
        }
    }

    return workflow
