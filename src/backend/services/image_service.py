# import io
# from PIL import Image
# from services.pipeline_loader import T2I_PIPE, I2I_PIPE, CURRENT_MODEL
# from services.openai_service import optimize_prompt
# from core.utils import get_model_params

# def generate_t2i_core(prompt: str, width: int, height: int, steps: int) -> bytes:
#     try:
#         print("ğŸš€ generate_t2i_core ì‹¤í–‰ë¨")

#         if T2I_PIPE is None:
#             print("âŒ T2I PIPELINE ë¯¸ì´ˆê¸°í™”")
#             raise RuntimeError("T2I PIPELINE ë¯¸ì´ˆê¸°í™”")

#         optimized_prompt = optimize_prompt(prompt)
#         params = get_model_params(CURRENT_MODEL)

#         gen_params = {
#             "prompt": optimized_prompt,
#             "width": width,
#             "height": height,
#             "num_inference_steps": steps if steps > 1 else params["default_steps"]
#         }

#         if params["use_negative_prompt"]:
#             gen_params["negative_prompt"] = params["negative_prompt"]

#         if params["guidance_scale"]:
#             gen_params["guidance_scale"] = params["guidance_scale"]

#         print("ğŸ”§ íŒŒë¼ë¯¸í„°:", gen_params)

#         # --- pipeline ì‹¤í–‰ ---
#         result = T2I_PIPE(**gen_params)
#         print("ğŸ“Œ pipeline ê²°ê³¼:", result)

#         # --- ì´ë¯¸ì§€ ê²€ì‚¬ ---
#         if not result or not hasattr(result, "images") or len(result.images) == 0:
#             print("âŒ Pipeline ê²°ê³¼ì— ì´ë¯¸ì§€ ì—†ìŒ")
#             raise ValueError("ì´ë¯¸ì§€ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

#         buf = io.BytesIO()
#         result.images[0].save(buf, format="PNG")
#         print("âœ… ì´ë¯¸ì§€ ë³€í™˜ ì„±ê³µ")
#         return buf.getvalue()

#     except Exception as e:
#         print("ğŸ”¥ generate_t2i_core ë‚´ë¶€ ì—ëŸ¬ ë°œìƒ!")
#         import traceback
#         traceback.print_exc()      # <<< ì—¬ê¸°ì„œ ì „ì²´ Traceback ì¶œë ¥ë¨!
#         raise

# def generate_i2i_core(input_image_bytes: bytes, prompt: str, strength: float, width: int, height: int, steps: int) -> bytes:
#     if I2I_PIPE is None:
#         raise RuntimeError("I2I PIPELINE ë¯¸ì´ˆê¸°í™”")
#     optimized_prompt = optimize_prompt(prompt)
#     input_image = Image.open(io.BytesIO(input_image_bytes)).convert("RGB").resize((width, height))
#     params = get_model_params(CURRENT_MODEL)
#     gen_params = {
#         "prompt": optimized_prompt,
#         "image": input_image,
#         "strength": strength,
#         "num_inference_steps": steps if steps>1 else params["default_steps"]
#     }
#     if params["use_negative_prompt"]:
#         gen_params["negative_prompt"] = params["negative_prompt"]
#     if params["guidance_scale"]:
#         gen_params["guidance_scale"] = params["guidance_scale"]
#     result = I2I_PIPE(**gen_params)
#     buf = io.BytesIO()
#     result.images[0].save(buf, format="PNG")
#     return buf.getvalue()








import io, base64
from services.pipeline_loader import T2I_PIPE, I2I_PIPE, CURRENT_MODEL

def generate_t2i_core(prompt: str, width: int, height: int, steps: int) -> bytes:
    try:
        print("ğŸš€ generate_t2i_core ì‹¤í–‰ë¨")
        if T2I_PIPE is None:
            raise RuntimeError("T2I PIPELINE ë¯¸ì´ˆê¸°í™”")

        # ê¸°ë³¸ pipeline íŒŒë¼ë¯¸í„° ì„¤ì •
        params = {
            "prompt": prompt,
            "width": width,
            "height": height,
            "num_inference_steps": steps if steps>1 else 50  # ê¸°ë³¸ 50 ìŠ¤í…
        }

        result = T2I_PIPE(**params)

        # ì´ë¯¸ì§€ ìœ íš¨ì„± í™•ì¸
        if not result or not hasattr(result, "images") or len(result.images)==0:
            raise ValueError("ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: ê²°ê³¼ ì´ë¯¸ì§€ ì—†ìŒ")

        buf = io.BytesIO()
        result.images[0].save(buf, format="PNG")
        print("âœ… ì´ë¯¸ì§€ ìƒì„± ì„±ê³µ")
        return buf.getvalue()

    except Exception as e:
        print("ğŸ”¥ generate_t2i_core ë‚´ë¶€ ì—ëŸ¬ ë°œìƒ!")
        import traceback
        traceback.print_exc()
        raise