"""
RAG ì²´ì¸ (Retrieval-Augmented Generation)

ê¸°ëŠ¥:
1. ê²€ìƒ‰ ì‹œìŠ¤í…œ í†µí•© (retriever.py)
2. Qwen3-30B-A3B-2507 ìƒì„± (Ollama GPU)
3. ëŒ€í™” ë©”ëª¨ë¦¬ ê´€ë¦¬ (ìµœê·¼ 5í„´)
4. ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì‘ë‹µ ìƒì„±

ì‚¬ìš©ë²•:
    from rag_chatbot.rag_chain import RAGChain

    rag = RAGChain()
    response = rag.chat("ìš´ë™ ë°©ë²•ì„ ì•Œë ¤ì¤˜")
    print(response)
"""

from typing import List, Dict, Any, Optional
from pathlib import Path
import json

from langchain_community.llms import Ollama
from langchain.memory import ConversationBufferWindowMemory
from langchain.prompts import PromptTemplate

from .retriever import Retriever


class RAGChain:
    """RAG ì²´ì¸"""

    def __init__(
        self,
        model_name: str = "qwen3-30b-a3b-2507:latest",
        ollama_base_url: str = "http://localhost:11434",
        milvus_host: str = "localhost",
        milvus_port: str = "19530",
        collection_name: str = "healthcare_docs",
        memory_k: int = 5,
        temperature: float = 0.7,
        max_tokens: int = 2048
    ):
        """
        Args:
            model_name: Ollama ëª¨ë¸ ì´ë¦„
            ollama_base_url: Ollama ì„œë²„ URL
            milvus_host: Milvus í˜¸ìŠ¤íŠ¸
            milvus_port: Milvus í¬íŠ¸
            collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
            memory_k: ëŒ€í™” ë©”ëª¨ë¦¬ í„´ ìˆ˜ (ê¸°ë³¸ê°’: 5)
            temperature: ìƒì„± temperature (ê¸°ë³¸ê°’: 0.7)
            max_tokens: ìµœëŒ€ í† í° ìˆ˜ (ê¸°ë³¸ê°’: 2048)
        """
        self.model_name = model_name
        self.ollama_base_url = ollama_base_url
        self.memory_k = memory_k
        self.temperature = temperature
        self.max_tokens = max_tokens

        # Retriever ì´ˆê¸°í™”
        print("ğŸ”§ Retriever ì´ˆê¸°í™” ì¤‘...")
        self.retriever = Retriever(
            milvus_host=milvus_host,
            milvus_port=milvus_port,
            collection_name=collection_name
        )
        print("âœ… Retriever ì´ˆê¸°í™” ì™„ë£Œ")

        # Ollama LLM ì´ˆê¸°í™”
        print(f"ğŸ”§ Ollama LLM ì´ˆê¸°í™” ì¤‘... (model: {model_name})")
        self.llm = Ollama(
            model=model_name,
            base_url=ollama_base_url,
            temperature=temperature,
            num_predict=max_tokens
        )
        print("âœ… Ollama LLM ì´ˆê¸°í™” ì™„ë£Œ")

        # ëŒ€í™” ë©”ëª¨ë¦¬ (ìµœê·¼ 5í„´)
        self.memory = ConversationBufferWindowMemory(
            k=memory_k,
            memory_key="chat_history",
            return_messages=True
        )

        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.prompt_template = PromptTemplate(
            input_variables=["context", "chat_history", "question"],
            template="""ë‹¹ì‹ ì€ í—¬ìŠ¤ì¼€ì–´ ì „ë¬¸ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

## ê²€ìƒ‰ëœ ë¬¸ì„œ:
{context}

## ëŒ€í™” ê¸°ë¡:
{chat_history}

## ì‚¬ìš©ì ì§ˆë¬¸:
{question}

## ë‹µë³€ ì§€ì¹¨:
1. ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ í™œìš©í•˜ì„¸ìš”.
2. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¼ë°˜ì ì¸ í—¬ìŠ¤ì¼€ì–´ ì§€ì‹ìœ¼ë¡œ ë³´ì™„í•˜ì„¸ìš”.
3. ì¹œì ˆí•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ì„¸ìš”.
4. ìš´ë™ì´ë‚˜ ì‹ë‹¨ ê´€ë ¨ ì§ˆë¬¸ì—ëŠ” êµ¬ì²´ì ì¸ ì˜ˆì‹œë¥¼ ì œê³µí•˜ì„¸ìš”.
5. ê´€ë ¨ ì´ë¯¸ì§€ê°€ ìˆë‹¤ë©´ ì–¸ê¸‰í•˜ì„¸ìš”.

ë‹µë³€:"""
        )

    def format_context(self, search_results: List[Dict[str, Any]]) -> str:
        """
        ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ë¡œ í¬ë§·íŒ…

        Args:
            search_results: Retriever.search() ê²°ê³¼

        Returns:
            í¬ë§·íŒ…ëœ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´
        """
        if not search_results:
            return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

        context_parts = []
        for i, result in enumerate(search_results, 1):
            source_type = result["source_type"]
            source_path = result["source_path"]
            text = result["text"]
            score = result["score"]

            # ë¬¸ì„œ ì •ë³´
            context_part = f"[ë¬¸ì„œ {i}] (ì ìˆ˜: {score:.3f}, ì†ŒìŠ¤: {source_type})\n"
            context_part += f"íŒŒì¼: {source_path}\n"
            context_part += f"ë‚´ìš©: {text}\n"

            # ì´ë¯¸ì§€ ê²½ë¡œ (ìˆëŠ” ê²½ìš°)
            if result.get("image_path"):
                context_part += f"ì´ë¯¸ì§€: {result['image_path']}\n"

            context_parts.append(context_part)

        return "\n".join(context_parts)

    def format_chat_history(self) -> str:
        """
        ëŒ€í™” ê¸°ë¡ì„ ë¬¸ìì—´ë¡œ í¬ë§·íŒ…

        Returns:
            í¬ë§·íŒ…ëœ ëŒ€í™” ê¸°ë¡ ë¬¸ìì—´
        """
        messages = self.memory.load_memory_variables({})
        if not messages or "chat_history" not in messages:
            return "ì—†ìŒ"

        chat_history = messages["chat_history"]
        if not chat_history:
            return "ì—†ìŒ"

        history_parts = []
        for msg in chat_history:
            if msg.type == "human":
                history_parts.append(f"ì‚¬ìš©ì: {msg.content}")
            elif msg.type == "ai":
                history_parts.append(f"ìƒë‹´ì‚¬: {msg.content}")

        return "\n".join(history_parts)

    def chat(
        self,
        question: str,
        top_k: int = 20,
        rerank_top_k: int = 5,
        source_type_filter: Optional[str] = None,
        return_sources: bool = False
    ) -> str:
        """
        RAG ê¸°ë°˜ ì±„íŒ…

        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            top_k: Dense ê²€ìƒ‰ ê°œìˆ˜ (ê¸°ë³¸ê°’: 20)
            rerank_top_k: ì¬ìˆœìœ„ í›„ ê°œìˆ˜ (ê¸°ë³¸ê°’: 5)
            source_type_filter: ì†ŒìŠ¤ íƒ€ì… í•„í„° (pdf, image, json)
            return_sources: ì†ŒìŠ¤ ë¬¸ì„œë„ í•¨ê»˜ ë°˜í™˜í• ì§€ (ê¸°ë³¸ê°’: False)

        Returns:
            return_sources=False: ì‘ë‹µ ë¬¸ìì—´
            return_sources=True: {"answer": "ì‘ë‹µ", "sources": [...]}
        """
        print("\n" + "="*60)
        print("RAG ì²´ì¸ ì‹¤í–‰")
        print("="*60)
        print(f"ì§ˆë¬¸: {question}")

        try:
            # 1. ë¬¸ì„œ ê²€ìƒ‰
            print("\nğŸ“š ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")
            search_results = self.retriever.search(
                query=question,
                top_k=top_k,
                rerank_top_k=rerank_top_k,
                source_type_filter=source_type_filter
            )

            # 2. ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
            context = self.format_context(search_results)

            # 3. ëŒ€í™” ê¸°ë¡ í¬ë§·íŒ…
            chat_history = self.format_chat_history()

            # 4. í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self.prompt_template.format(
                context=context,
                chat_history=chat_history,
                question=question
            )

            # 5. LLM ìƒì„±
            print("\nğŸ¤– Qwen3-30B-A3B-2507 ì‘ë‹µ ìƒì„± ì¤‘...")
            response = self.llm.invoke(prompt)

            # 6. ë©”ëª¨ë¦¬ ì €ì¥
            self.memory.save_context(
                {"input": question},
                {"output": response}
            )

            print("âœ… ì‘ë‹µ ìƒì„± ì™„ë£Œ")

            # 7. ê²°ê³¼ ë°˜í™˜
            if return_sources:
                return {
                    "answer": response,
                    "sources": search_results
                }
            else:
                return response

        except Exception as e:
            error_msg = f"âŒ RAG ì²´ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
            print(error_msg)
            return error_msg

    def chat_with_image(
        self,
        question: str,
        image_path: Optional[str] = None,
        top_k: int = 20,
        rerank_top_k: int = 5
    ) -> str:
        """
        ì´ë¯¸ì§€ í¬í•¨ ì±„íŒ…

        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            image_path: ì‚¬ìš©ìê°€ ì—…ë¡œë“œí•œ ì´ë¯¸ì§€ ê²½ë¡œ (ì„ íƒ)
            top_k: Dense ê²€ìƒ‰ ê°œìˆ˜
            rerank_top_k: ì¬ìˆœìœ„ í›„ ê°œìˆ˜

        Returns:
            ì‘ë‹µ ë¬¸ìì—´
        """
        # ì´ë¯¸ì§€ ê²½ë¡œê°€ ì œê³µëœ ê²½ìš° ì§ˆë¬¸ì— ì¶”ê°€
        if image_path:
            question_with_image = f"{question}\n[ì‚¬ìš©ì ì´ë¯¸ì§€: {image_path}]"
        else:
            question_with_image = question

        return self.chat(
            question=question_with_image,
            top_k=top_k,
            rerank_top_k=rerank_top_k
        )

    def clear_memory(self):
        """ëŒ€í™” ë©”ëª¨ë¦¬ ì´ˆê¸°í™”"""
        print("ğŸ”„ ëŒ€í™” ë©”ëª¨ë¦¬ ì´ˆê¸°í™” ì¤‘...")
        self.memory.clear()
        print("âœ… ëŒ€í™” ë©”ëª¨ë¦¬ ì´ˆê¸°í™” ì™„ë£Œ")

    def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        print("\nğŸ”„ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
        self.retriever.close()
        print("âœ… ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")


if __name__ == "__main__":
    """
    í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì˜ˆì‹œ
    """
    import sys

    print("="*60)
    print("RAG Chain í…ŒìŠ¤íŠ¸")
    print("="*60)

    # RAG ì²´ì¸ ì´ˆê¸°í™”
    rag = RAGChain()

    # ëŒ€í™” ì‹œì‘
    print("\nğŸ’¬ ëŒ€í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. (ì¢…ë£Œ: 'quit' ë˜ëŠ” 'exit')")
    print("ë©”ëª¨ë¦¬ ì´ˆê¸°í™”: 'clear'")
    print("ì†ŒìŠ¤ ë¬¸ì„œ í¬í•¨ ì‘ë‹µ: '/sources <ì§ˆë¬¸>'")
    print("="*60)

    while True:
        try:
            # ì‚¬ìš©ì ì…ë ¥
            user_input = input("\nì‚¬ìš©ì: ").strip()

            if not user_input:
                continue

            # ì¢…ë£Œ
            if user_input.lower() in ["quit", "exit", "ì¢…ë£Œ"]:
                print("\nğŸ‘‹ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break

            # ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
            if user_input.lower() == "clear":
                rag.clear_memory()
                continue

            # ì†ŒìŠ¤ ë¬¸ì„œ í¬í•¨ ì‘ë‹µ
            if user_input.startswith("/sources "):
                question = user_input[9:].strip()
                result = rag.chat(question, return_sources=True)

                print(f"\nìƒë‹´ì‚¬: {result['answer']}")

                print("\n" + "="*60)
                print("ì°¸ê³  ë¬¸ì„œ")
                print("="*60)
                for i, source in enumerate(result["sources"], 1):
                    print(f"\n[{i}] {source['source_type']} - ì ìˆ˜: {source['score']:.3f}")
                    print(f"íŒŒì¼: {source['source_path']}")
                    print(f"ë‚´ìš©: {source['text'][:150]}...")
                    if source.get("image_path"):
                        print(f"ì´ë¯¸ì§€: {source['image_path']}")
                continue

            # ì¼ë°˜ ì±„íŒ…
            response = rag.chat(user_input)
            print(f"\nìƒë‹´ì‚¬: {response}")

        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # ì •ë¦¬
    rag.close()
