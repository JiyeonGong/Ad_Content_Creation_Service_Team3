"""
ê²€ìƒ‰ ì‹œìŠ¤í…œ (Retriever)

ê¸°ëŠ¥:
1. Milvus Dense ê²€ìƒ‰ (top-20)
2. BGE-M3 ColBERT ì¬ìˆœìœ„ (top-5)
3. ì´ë¯¸ì§€ ê²½ë¡œ í¬í•¨ ë°˜í™˜

ì‚¬ìš©ë²•:
    from rag_chatbot.retriever import Retriever

    retriever = Retriever()
    results = retriever.search("ìš´ë™ ë°©ë²• ì•Œë ¤ì¤˜")

    for result in results:
        print(result["text"])
        if result.get("image_path"):
            print(f"ì´ë¯¸ì§€: {result['image_path']}")
"""

from typing import List, Dict, Any, Optional
import torch
from pathlib import Path
from sentence_transformers import SentenceTransformer
from FlagEmbedding import BGEM3FlagModel

from .vector_store import MilvusVectorStore


class Retriever:
    """ê²€ìƒ‰ ë° ì¬ìˆœìœ„ ì‹œìŠ¤í…œ"""

    def __init__(
        self,
        milvus_host: str = "localhost",
        milvus_port: str = "19530",
        collection_name: str = "healthcare_docs",
        embed_model_name: str = "BAAI/bge-m3",
        device: Optional[str] = None
    ):
        """
        Args:
            milvus_host: Milvus í˜¸ìŠ¤íŠ¸
            milvus_port: Milvus í¬íŠ¸
            collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
            embed_model_name: BGE-M3 ëª¨ë¸ ê²½ë¡œ
            device: ë””ë°”ì´ìŠ¤ (Noneì´ë©´ ìë™ ì„ íƒ)
        """
        self.milvus_host = milvus_host
        self.milvus_port = milvus_port
        self.collection_name = collection_name
        self.embed_model_name = embed_model_name

        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if device is None:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device

        # Milvus ì—°ê²°
        self.vector_store = MilvusVectorStore(
            host=milvus_host,
            port=milvus_port
        )
        self.vector_store.connect()

        # BGE-M3 ëª¨ë¸ (lazy loading)
        self.model = None

    def load_model(self):
        """BGE-M3 ëª¨ë¸ ë¡œë”©"""
        if self.model is None:
            print(f"ğŸ“¥ BGE-M3 ëª¨ë¸ ë¡œë”© ì¤‘... (device: {self.device})")
            self.model = BGEM3FlagModel(
                self.embed_model_name,
                use_fp16=True if self.device == "cuda" else False
            )
            print("âœ… BGE-M3 ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

    def unload_model(self):
        """BGE-M3 ëª¨ë¸ ì–¸ë¡œë”©"""
        if self.model is not None:
            print("ğŸ”„ BGE-M3 ëª¨ë¸ ì–¸ë¡œë”© ì¤‘...")
            del self.model
            self.model = None

            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            print("âœ… BGE-M3 ëª¨ë¸ ì–¸ë¡œë”© ì™„ë£Œ")

    def search(
        self,
        query: str,
        top_k: int = 20,
        rerank_top_k: int = 5,
        source_type_filter: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        ê²€ìƒ‰ ë° ì¬ìˆœìœ„

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: Dense ê²€ìƒ‰ ìƒìœ„ kê°œ (ê¸°ë³¸ê°’: 20)
            rerank_top_k: ì¬ìˆœìœ„ í›„ ìƒìœ„ kê°œ (ê¸°ë³¸ê°’: 5)
            source_type_filter: ì†ŒìŠ¤ íƒ€ì… í•„í„° (pdf, image, json)

        Returns:
            ì¬ìˆœìœ„ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (ê° ë¬¸ì„œëŠ” dict)
            [
                {
                    "text": "ë¬¸ì„œ í…ìŠ¤íŠ¸",
                    "source_type": "pdf|image|json",
                    "source_path": "íŒŒì¼ ê²½ë¡œ",
                    "metadata": {...},
                    "score": 0.95,  # ì¬ìˆœìœ„ ì ìˆ˜
                    "image_path": "ì´ë¯¸ì§€ ê²½ë¡œ" (ìˆëŠ” ê²½ìš°)
                },
                ...
            ]
        """
        print(f"\n{'='*60}")
        print(f"ê²€ìƒ‰ ì¿¼ë¦¬: {query}")
        print(f"{'='*60}")

        try:
            # 1. BGE-M3 ëª¨ë¸ ë¡œë”©
            self.load_model()

            # 2. ì¿¼ë¦¬ ì„ë² ë”© (Denseë§Œ)
            print(f"\nğŸ” ì¿¼ë¦¬ ì„ë² ë”© ì¤‘...")
            query_embedding_dict = self.model.encode(
                [query],
                batch_size=1,
                max_length=8192
            )
            query_vector = query_embedding_dict["dense_vecs"][0]

            # 3. Milvus Dense ê²€ìƒ‰ (top-20)
            print(f"ğŸ” Milvus Dense ê²€ìƒ‰ ì¤‘... (top-{top_k})")

            # ì†ŒìŠ¤ íƒ€ì… í•„í„° í‘œí˜„ì‹
            expr = None
            if source_type_filter:
                expr = f'source_type == "{source_type_filter}"'

            search_results = self.vector_store.search(
                query_vector=query_vector.tolist(),
                top_k=top_k,
                expr=expr
            )

            if not search_results:
                print("âš ï¸  ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return []

            print(f"âœ… {len(search_results)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ")

            # 4. BGE-M3 ColBERT ì¬ìˆœìœ„ (top-5)
            print(f"\nğŸ”„ BGE-M3 ColBERT ì¬ìˆœìœ„ ì¤‘... (top-{rerank_top_k})")

            # ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            docs_text = [doc["text"] for doc in search_results]

            # ColBERT ì¬ìˆœìœ„ (BGE-M3ì˜ compute_score ì‚¬ìš©)
            rerank_scores = []
            for doc_text in docs_text:
                # BGE-M3 ColBERT ì ìˆ˜ ê³„ì‚°
                score_dict = self.model.compute_score(
                    [[query, doc_text]],
                    weights_for_different_modes=[0.0, 1.0, 0.0]  # ColBERTë§Œ ì‚¬ìš©
                )
                score = score_dict["colbert"][0]
                rerank_scores.append(score)

            # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
            scored_docs = list(zip(search_results, rerank_scores))
            scored_docs.sort(key=lambda x: x[1], reverse=True)

            # ìƒìœ„ rerank_top_kê°œ ì„ íƒ
            top_docs = scored_docs[:rerank_top_k]

            # ê²°ê³¼ í¬ë§·íŒ…
            results = []
            for doc, score in top_docs:
                result = {
                    "text": doc["text"],
                    "source_type": doc["source_type"],
                    "source_path": doc["source_path"],
                    "metadata": doc["metadata"],
                    "score": float(score)
                }

                # ì´ë¯¸ì§€ ê²½ë¡œ ì¶”ê°€ (ì´ë¯¸ì§€ ì†ŒìŠ¤ íƒ€ì…ì´ê±°ë‚˜ ë©”íƒ€ë°ì´í„°ì— ìˆëŠ” ê²½ìš°)
                if doc["source_type"] == "image":
                    result["image_path"] = doc["source_path"]
                elif "image_path" in doc["metadata"]:
                    result["image_path"] = doc["metadata"]["image_path"]

                results.append(result)

            print(f"âœ… ì¬ìˆœìœ„ ì™„ë£Œ ({len(results)}ê°œ ë¬¸ì„œ)")

            # 5. ê²°ê³¼ ì¶œë ¥
            print(f"\n{'='*60}")
            print("ê²€ìƒ‰ ê²°ê³¼")
            print(f"{'='*60}")
            for i, result in enumerate(results, 1):
                print(f"\n[{i}] ì ìˆ˜: {result['score']:.4f}")
                print(f"ì†ŒìŠ¤: {result['source_type']} - {result['source_path']}")
                print(f"í…ìŠ¤íŠ¸: {result['text'][:100]}...")
                if result.get("image_path"):
                    print(f"ì´ë¯¸ì§€: {result['image_path']}")

            return results

        finally:
            # 6. ëª¨ë¸ ì–¸ë¡œë”© (í•­ìƒ ì‹¤í–‰)
            self.unload_model()

    def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.unload_model()
        self.vector_store.close()


if __name__ == "__main__":
    """
    í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì˜ˆì‹œ
    """
    import sys

    # ê²€ìƒ‰ ì¿¼ë¦¬
    if len(sys.argv) > 1:
        query = " ".join(sys.argv[1:])
    else:
        query = "ìš´ë™ ë°©ë²•ì„ ì•Œë ¤ì¤˜"

    print("="*60)
    print("RAG Retriever í…ŒìŠ¤íŠ¸")
    print("="*60)
    print(f"ì¿¼ë¦¬: {query}")

    # Retriever ì´ˆê¸°í™”
    retriever = Retriever()

    # ê²€ìƒ‰
    results = retriever.search(
        query=query,
        top_k=20,
        rerank_top_k=5
    )

    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*60)
    print("ìµœì¢… ê²°ê³¼")
    print("="*60)
    for i, result in enumerate(results, 1):
        print(f"\n[{i}] {result['source_type']} - ì ìˆ˜: {result['score']:.4f}")
        print(f"íŒŒì¼: {result['source_path']}")
        print(f"ë‚´ìš©: {result['text'][:200]}...")
        if result.get("image_path"):
            print(f"ì´ë¯¸ì§€: {result['image_path']}")

    # ì •ë¦¬
    retriever.close()
