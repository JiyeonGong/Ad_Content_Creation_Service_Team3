# # C:\Users\devuser\Codeit\Ad_Content_Creation_Service_Team3\src\healthcare\backend.py
# #========================================
# # í”„ë¡ íŠ¸ì—”ë“œ/ë²¡ì—”ë“œ ë¶„ë¦¬ + ìºì‹œ ê²½ë¡œ ì§€ì • ë²„ì „
# #========================================

# import os
# import re
# from fastapi import FastAPI, UploadFile, File, Form
# from fastapi.middleware.cors import CORSMiddleware
# from openai import OpenAI
# from diffusers import StableDiffusionXLPipeline, StableDiffusionXLImg2ImgPipeline
# import torch
# from io import BytesIO
# from PIL import Image

# # ===============================
# # ğŸŒ í™˜ê²½ ë³€ìˆ˜ ë° í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
# # ===============================
# OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
# MODEL_GPT_MINI = "gpt-5-mini"
# openai_client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None

# # ===============================
# # ğŸ’¾ ìºì‹œ ê²½ë¡œ ì„¤ì • (í”„ë¡œì íŠ¸ êµ¬ì¡° ê¸°ì¤€)
# # ===============================
# project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
# cache_root = os.path.join(project_root, "cache")
# os.makedirs(cache_root, exist_ok=True)

# hf_cache_dir = os.path.join(cache_root, "hf_models")
# os.makedirs(hf_cache_dir, exist_ok=True)

# print(f"[INFO] Hugging Face ëª¨ë¸ ìºì‹œ ê²½ë¡œ: {hf_cache_dir}")

# # ===============================
# # FastAPI ì•± & CORS
# # ===============================
# app = FastAPI(title="Healthcare AI Content API")
# app.add_middleware(
#     CORSMiddleware,
#     allow_origins=["*"],  # ì‹¤ì œ ë°°í¬ ì‹œ ë„ë©”ì¸ ì œí•œ ê°€ëŠ¥
#     allow_methods=["*"],
#     allow_headers=["*"],
# )

# # ===============================
# # GPT-5 Mini ë¬¸êµ¬ ìƒì„±
# # ===============================
# def parse_output(output):
#     captions, hashtags = [], ""
#     try:
#         m = re.search(r"ë¬¸êµ¬:(.*?)í•´ì‹œíƒœê·¸:(.*)", output, re.S)
#         if m:
#             caption_text = m.group(1).strip()
#             hashtags = m.group(2).strip()
#             captions = [line.split(".", 1)[1].strip() if "." in line else line.strip()
#                         for line in caption_text.split("\n") if line.strip()]
#         else:
#             captions = [output]
#     except Exception:
#         captions = [output]
#     return captions, hashtags

# @app.post("/generate_captions")
# def generate_captions(
#     service_name: str = Form(...),
#     features: str = Form(...),
#     tone: str = Form("ì¹œê·¼í•˜ê³  ë™ê¸°ë¶€ì—¬")
# ):
#     if not openai_client:
#         return {"error": "OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}

#     prompt = f"""
# ë‹¹ì‹ ì€ í—¬ìŠ¤ì¼€ì–´ ì†Œìƒê³µì¸ì„ ìœ„í•œ ì „ë¬¸ ì¸ìŠ¤íƒ€ê·¸ë¨ ì½˜í…ì¸  í¬ë¦¬ì—ì´í„°ì…ë‹ˆë‹¤.
# ì•„ë˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¸ìŠ¤íƒ€ê·¸ë¨ ê²Œì‹œë¬¼ì— ìµœì í™”ëœ ì½˜í…ì¸ ë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”.

# ìš”ì²­:
# 1. ì¸ìŠ¤íƒ€ê·¸ë¨ í™ë³´ ë¬¸êµ¬ 3ê°œ ì‘ì„±
#     - ê° ë¬¸êµ¬: í›„í‚¹ â†’ í•µì‹¬ ë©”ì‹œì§€ â†’ CTA
#     - ì´ëª¨í‹°ì½˜ ì‚¬ìš©
#     - ë¬¸ì²´ ìŠ¤íƒ€ì¼: {tone}

# [ì •ë³´]
# ì„œë¹„ìŠ¤ ì¢…ë¥˜: í—¬ìŠ¤/í”¼íŠ¸ë‹ˆìŠ¤
# ì„œë¹„ìŠ¤ëª…: {service_name}
# í•µì‹¬ íŠ¹ì§•: {features}
# ì§€ì—­: ì „êµ­/ì˜¨ë¼ì¸
# ì´ë²¤íŠ¸: ì—†ìŒ

# ì¶œë ¥ í˜•ì‹:
# ë¬¸êµ¬:
# 1. [ë¬¸êµ¬1]
# 2. [ë¬¸êµ¬2]
# 3. [ë¬¸êµ¬3]

# í•´ì‹œíƒœê·¸:
# #[íƒœê·¸1] #[íƒœê·¸2] ... #[íƒœê·¸N]
# """
#     response = openai_client.responses.create(
#         model=MODEL_GPT_MINI,
#         input=prompt,
#         reasoning={"effort": "minimal"},
#         max_output_tokens=512
#     )
#     captions, hashtags = parse_output(response.output_text.strip())
#     return {"captions": captions, "hashtags": hashtags}

# # ===============================
# # SDXL T2I
# # ===============================
# pipe_t2i = None
# def init_sdxl_t2i():
#     global pipe_t2i
#     if pipe_t2i is None:
#         pipe_t2i = StableDiffusionXLPipeline.from_pretrained(
#             "stabilityai/stable-diffusion-xl-base-1.0",
#             cache_dir=hf_cache_dir,  # âœ… HF ëª¨ë¸ ìºì‹œ ê²½ë¡œ ì§€ì •
#             torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
#         ).to("cuda" if torch.cuda.is_available() else "cpu")
#     return pipe_t2i

# @app.post("/generate_image")
# def generate_image(
#     prompt: str = Form(...),
#     width: int = Form(1024),
#     height: int = Form(1024),
#     steps: int = Form(30)
# ):
#     pipe = init_sdxl_t2i()
#     negative_prompt = "low quality, blurry, text, watermark, distorted"
#     result = pipe(
#         prompt=prompt,
#         negative_prompt=negative_prompt,
#         width=width,
#         height=height,
#         num_inference_steps=steps
#     )
#     buf = BytesIO()
#     result.images[0].save(buf, format="PNG")
#     buf.seek(0)
#     return {"image_bytes": buf.getvalue()}

# # ===============================
# # SDXL I2I
# # ===============================
# pipe_i2i = None
# def init_sdxl_i2i():
#     global pipe_i2i
#     if pipe_i2i is None:
#         pipe_i2i = StableDiffusionXLImg2ImgPipeline.from_pretrained(
#             "stabilityai/stable-diffusion-xl-base-1.0",
#             cache_dir=hf_cache_dir,  # âœ… HF ëª¨ë¸ ìºì‹œ ê²½ë¡œ ì§€ì •
#             torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
#         ).to("cuda" if torch.cuda.is_available() else "cpu")
#     return pipe_i2i

# @app.post("/edit_image")
# def edit_image(
#     image: UploadFile = File(...),
#     prompt: str = Form(...),
#     strength: float = Form(0.75),
#     width: int = Form(1024),
#     height: int = Form(1024),
#     steps: int = Form(30)
# ):
#     pipe = init_sdxl_i2i()
#     input_image = Image.open(BytesIO(image.file.read())).convert("RGB").resize((width, height))
#     negative_prompt = "low quality, blurry, text, watermark, distorted"

#     result = pipe(
#         prompt=prompt,
#         image=input_image,
#         strength=strength,
#         negative_prompt=negative_prompt,
#         num_inference_steps=steps
#     )
#     buf = BytesIO()
#     result.images[0].save(buf, format="PNG")
#     buf.seek(0)
#     return {"image_bytes": buf.getvalue()}







# # C:\Users\devuser\Codeit\Ad_Content_Creation_Service_Team3\src\healthcare\backend.py
# #========================================
# # í”„ë¡ íŠ¸ì—”ë“œ/ë²¡ì—”ë“œ ë¶„ë¦¬ + ìºì‹œ ê²½ë¡œ ì§€ì • ë²„ì „ + streamlit.py ê°œì„  ì‚¬í•­ ë°˜ì˜
# #========================================

# import os
# from openai import OpenAI
# from diffusers import StableDiffusionXLPipeline, StableDiffusionXLImg2ImgPipeline
# import torch
# from io import BytesIO
# from PIL import Image

# # ====================================================
# # ğŸŒ± í™˜ê²½ ë³€ìˆ˜
# # ====================================================
# OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
# MODEL_GPT_MINI = "gpt-5-mini"

# # OpenAI í´ë¼ì´ì–¸íŠ¸
# openai_client = None
# if OPENAI_API_KEY:
#     try:
#         openai_client = OpenAI(api_key=OPENAI_API_KEY)
#     except Exception as e:
#         print(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")

# # ====================================================
# # ğŸ—„ ìºì‹œ ê²½ë¡œ
# # ====================================================
# cache_root = os.path.join(os.path.abspath(os.path.dirname(__file__)), "cache")
# hf_cache_dir = os.path.join(cache_root, "hf_models")
# os.makedirs(hf_cache_dir, exist_ok=True)

# # ====================================================
# # ğŸ“ GPT-5 Mini í™ë³´ ë¬¸êµ¬ + í•´ì‹œíƒœê·¸ ìƒì„±
# # ====================================================
# def generate_caption_and_hashtags(client, model, tone, info, hashtag_count=15):
#     prompt = f"""
# ë‹¹ì‹ ì€ í—¬ìŠ¤ì¼€ì–´ ì†Œìƒê³µì¸ì„ ìœ„í•œ ì „ë¬¸ ì¸ìŠ¤íƒ€ê·¸ë¨ ì½˜í…ì¸  í¬ë¦¬ì—ì´í„°ì…ë‹ˆë‹¤.
# ì•„ë˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¸ìŠ¤íƒ€ê·¸ë¨ ê²Œì‹œë¬¼ì— ìµœì í™”ëœ ì½˜í…ì¸ ë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”.

# ìš”ì²­:
# 1. ì¸ìŠ¤íƒ€ê·¸ë¨ í™ë³´ ë¬¸êµ¬ 3ê°œ ì‘ì„±
#     - ê° ë¬¸êµ¬: í›„í‚¹ â†’ í•µì‹¬ ë©”ì‹œì§€ â†’ CTA
#     - ì´ëª¨í‹°ì½˜ ì‚¬ìš©
#     - ë¬¸ì²´ ìŠ¤íƒ€ì¼: {tone}
# 2. í•´ì‹œíƒœê·¸ {hashtag_count}ê°œ ì¶”ì²œ (ì¤‘ë³µ ì œê±°)

# [ì •ë³´]
# ì„œë¹„ìŠ¤ ì¢…ë¥˜: {info['service_type']}
# ì„œë¹„ìŠ¤ëª…: {info['service_name']}
# í•µì‹¬ íŠ¹ì§•: {info['features']}
# ì§€ì—­: {info['location']}
# ì´ë²¤íŠ¸: {info['event_info']}

# ì¶œë ¥ í˜•ì‹:
# ë¬¸êµ¬:
# 1. [ë¬¸êµ¬1]
# 2. [ë¬¸êµ¬2]
# 3. [ë¬¸êµ¬3]

# í•´ì‹œíƒœê·¸:
# #[íƒœê·¸1] #[íƒœê·¸2] ... #[íƒœê·¸N]
# """
#     try:
#         response = client.responses.create(
#             model=model,
#             input=prompt,
#             reasoning={"effort":"minimal"},
#             max_output_tokens=512
#         )
#         return response.output_text.strip()
#     except Exception as e:
#         return f"ë¬¸êµ¬:\n1. [API ì˜¤ë¥˜]\ní•´ì‹œíƒœê·¸:\n#[APIì˜¤ë¥˜]"

# # ====================================================
# # ğŸ–¼ SDXL ì´ë¯¸ì§€ ìƒì„±
# # ====================================================
# def init_local_sdxl_t2i(model_id="stabilityai/stable-diffusion-xl-base-1.0"):
#     pipe = StableDiffusionXLPipeline.from_pretrained(
#         model_id,
#         cache_dir=hf_cache_dir,
#         torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
#     )
#     return pipe.to("cuda" if torch.cuda.is_available() else "cpu")

# def init_local_sdxl_i2i(model_id="stabilityai/stable-diffusion-xl-base-1.0"):
#     pipe = StableDiffusionXLImg2ImgPipeline.from_pretrained(
#         model_id,
#         cache_dir=hf_cache_dir,
#         torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
#     )
#     return pipe.to("cuda" if torch.cuda.is_available() else "cpu")

# def generate_image_local(pipe, prompt, width=1024, height=1024, steps=30):
#     negative_prompt = "low quality, blurry, text, watermark, distorted"
#     result = pipe(prompt=prompt, negative_prompt=negative_prompt, width=width, height=height, num_inference_steps=steps)
#     image = result.images[0]
#     buf = BytesIO()
#     image.save(buf, format="PNG")
#     buf.seek(0)
#     return buf

# def generate_image_i2i_local(pipe, input_image_bytes, prompt, strength=0.75, width=1024, height=1024, steps=30):
#     negative_prompt = "low quality, blurry, text, watermark, distorted"
#     input_image = Image.open(BytesIO(input_image_bytes)).convert("RGB").resize((width, height))
#     result = pipe(prompt=prompt, image=input_image, strength=strength, negative_prompt=negative_prompt, num_inference_steps=steps)
#     image = result.images[0]
#     buf = BytesIO()
#     image.save(buf, format="PNG")
#     buf.seek(0)
#     return buf

# def caption_to_image_prompt(caption, style="Instagram banner"):
#     return f"{caption}, {style}, vibrant, professional, motivational"








# C:\Users\devuser\Codeit\Ad_Content_Creation_Service_Team3\src\healthcare\backend.py
# ============================================================
# âš™ï¸ FastAPI ë°±ì—”ë“œ API ì„œë²„ (AI ì¶”ë¡  ë¡œì§ ë‹´ë‹¹)
# - GPT-5 Mini í˜¸ì¶œ (OpenAI API)
# - SDXL T2I/I2I ë¡œì»¬ ì¶”ë¡  (Diffusers)
# - ì´ë¯¸ì§€ ë°ì´í„°ëŠ” Base64ë¡œ ì¸ì½”ë”©/ë””ì½”ë”©í•˜ì—¬ ì „ì†¡
# ============================================================

import os
import io
import base64
from typing import Dict, Any, List

from fastapi import FastAPI, HTTPException, Body
from pydantic import BaseModel, Field

# AI ëª¨ë¸ ë° ì´ë¯¸ì§€ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from openai import OpenAI
from diffusers import StableDiffusionXLPipeline, StableDiffusionXLImg2ImgPipeline
import torch
from PIL import Image
from dotenv import load_dotenv

# ============================================================
# ğŸŒ í™˜ê²½ ì„¤ì • ë° í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
# ============================================================

# .env íŒŒì¼ ë¡œë”© (í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€)
load_dotenv(os.path.join(os.path.dirname(__file__), "..", "..", ".env"))

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
MODEL_GPT_MINI = "gpt-5-mini"

# Hugging Face ëª¨ë¸ ìºì‹œ ê²½ë¡œ ì„¤ì •
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
hf_cache_dir = os.path.join(project_root, "cache", "hf_models")
os.makedirs(hf_cache_dir, exist_ok=True)

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
openai_client = None
if OPENAI_API_KEY:
    try:
        openai_client = OpenAI(api_key=OPENAI_API_KEY)
    except Exception as e:
        print(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
else:
    print("âš ï¸ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ GPT ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# SDXL ëª¨ë¸ ì´ˆê¸°í™” (FastAPI ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰)
T2I_PIPE = None
I2I_PIPE = None
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
MODEL_ID = "stabilityai/stable-diffusion-xl-base-1.0"

def init_sdxl_pipelines():
    """SDXL T2I ë° I2I íŒŒì´í”„ë¼ì¸ì„ ì´ˆê¸°í™”í•˜ê³  ì „ì—­ ë³€ìˆ˜ì— ì €ì¥í•©ë‹ˆë‹¤."""
    global T2I_PIPE, I2I_PIPE
    try:
        print(f"SDXL ëª¨ë¸ ë¡œë”© ì¤‘... (Device: {DEVICE}, Cache: {hf_cache_dir})")
        dtype = torch.float16 if torch.cuda.is_available() else torch.float32

        # T2I (Text-to-Image) íŒŒì´í”„ë¼ì¸
        T2I_PIPE = StableDiffusionXLPipeline.from_pretrained(
            MODEL_ID,
            cache_dir=hf_cache_dir,
            torch_dtype=dtype
        ).to(DEVICE)
        
        # I2I (Image-to-Image) íŒŒì´í”„ë¼ì¸
        I2I_PIPE = StableDiffusionXLImg2ImgPipeline.from_pretrained(
            MODEL_ID,
            cache_dir=hf_cache_dir,
            torch_dtype=dtype
        ).to(DEVICE)
        
        print("SDXL ëª¨ë¸ ë¡œë”© ì™„ë£Œ.")
    except Exception as e:
        print(f"SDXL ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        T2I_PIPE = None
        I2I_PIPE = None

# FastAPI ì•± ìƒì„± ë° ëª¨ë¸ ë¡œë”©
app = FastAPI(title="í—¬ìŠ¤ì¼€ì–´ AI ì½˜í…ì¸  API")

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ SDXL ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    init_sdxl_pipelines()

# ============================================================
# ğŸ“œ Pydantic ìŠ¤í‚¤ë§ˆ ì •ì˜ (Request/Response ë°ì´í„° êµ¬ì¡°)
# ============================================================

class CaptionRequest(BaseModel):
    """ë¬¸êµ¬ ìƒì„± ìš”ì²­ ë°ì´í„° ëª¨ë¸"""
    service_type: str = Field(..., description="ì„œë¹„ìŠ¤ ì¢…ë¥˜ (ì˜ˆ: í—¬ìŠ¤ì¥)")
    service_name: str = Field(..., description="ì œí’ˆ/í´ë˜ìŠ¤ ì´ë¦„")
    features: str = Field(..., description="í•µì‹¬ íŠ¹ì§• ë° ì¥ì ")
    location: str = Field(..., description="ì§€ì—­")
    tone: str = Field(..., description="ë¬¸ì²´ ìŠ¤íƒ€ì¼")

class CaptionResponse(BaseModel):
    """ë¬¸êµ¬ ìƒì„± ì‘ë‹µ ë°ì´í„° ëª¨ë¸"""
    output_text: str = Field(..., description="GPT-5 Miniê°€ ìƒì„±í•œ ì›ë³¸ í…ìŠ¤íŠ¸")

class T2IRequest(BaseModel):
    """T2I ì´ë¯¸ì§€ ìƒì„± ìš”ì²­ ë°ì´í„° ëª¨ë¸"""
    prompt: str = Field(..., description="ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸")
    width: int = 1024
    height: int = 1024
    steps: int = 30

class T2IResponse(BaseModel):
    """T2I ì´ë¯¸ì§€ ìƒì„± ì‘ë‹µ ë°ì´í„° ëª¨ë¸"""
    image_base64: str = Field(..., description="ìƒì„±ëœ ì´ë¯¸ì§€ì˜ Base64 ì¸ì½”ë”© ë¬¸ìì—´ (PNG í˜•ì‹)")

class I2IRequest(BaseModel):
    """I2I ì´ë¯¸ì§€ í¸ì§‘/í•©ì„± ìš”ì²­ ë°ì´í„° ëª¨ë¸"""
    input_image_base64: str = Field(..., description="ì›ë³¸ ì´ë¯¸ì§€ì˜ Base64 ì¸ì½”ë”© ë¬¸ìì—´ (PNG/JPG)")
    prompt: str = Field(..., description="ì´ë¯¸ì§€ í¸ì§‘ í”„ë¡¬í”„íŠ¸")
    strength: float = 0.75
    width: int = 1024
    height: int = 1024
    steps: int = 30

# ============================================================
# ğŸ¯ AI ì¶”ë¡  í•¨ìˆ˜ (ê¸°ì¡´ Streamlit ë¡œì§ì„ API í•¨ìˆ˜ë¡œ ë³€í™˜)
# ============================================================

def generate_caption_core(client, model, tone, info, hashtag_count=15) -> str:
    """GPT-5 Minië¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸êµ¬ ë° í•´ì‹œíƒœê·¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    if not client:
        raise ValueError("OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
    prompt = f"""
ë‹¹ì‹ ì€ í—¬ìŠ¤ì¼€ì–´ ì†Œìƒê³µì¸ì„ ìœ„í•œ ì „ë¬¸ ì¸ìŠ¤íƒ€ê·¸ë¨ ì½˜í…ì¸  í¬ë¦¬ì—ì´í„°ì…ë‹ˆë‹¤.
ì•„ë˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¸ìŠ¤íƒ€ê·¸ë¨ ê²Œì‹œë¬¼ì— ìµœì í™”ëœ ì½˜í…ì¸ ë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”.

ìš”ì²­:
1. ì¸ìŠ¤íƒ€ê·¸ë¨ í™ë³´ ë¬¸êµ¬ 3ê°œ ì‘ì„±
    - ê° ë¬¸êµ¬: í›„í‚¹ â†’ í•µì‹¬ ë©”ì‹œì§€ â†’ CTA
    - ì´ëª¨í‹°ì½˜ ì‚¬ìš©
    - ë¬¸ì²´ ìŠ¤íƒ€ì¼: {tone}
2. í•´ì‹œíƒœê·¸ {hashtag_count}ê°œ ì¶”ì²œ (ì¤‘ë³µ ì œê±°)

[ì •ë³´]
ì„œë¹„ìŠ¤ ì¢…ë¥˜: {info['service_type']}
ì„œë¹„ìŠ¤ëª…: {info['service_name']}
í•µì‹¬ íŠ¹ì§•: {info['features']}
ì§€ì—­: {info['location']}
ì´ë²¤íŠ¸: ì—†ìŒ

ì¶œë ¥ í˜•ì‹:
ë¬¸êµ¬:
1. [ë¬¸êµ¬1]
2. [ë¬¸êµ¬2]
3. [ë¬¸êµ¬3]

í•´ì‹œíƒœê·¸:
#[íƒœê·¸1] #[íƒœê·¸2] ... #[íƒœê·¸N]
"""
    try:
        response = client.responses.create(
            model=model,
            input=prompt,
            reasoning={"effort":"minimal"},
            max_output_tokens=512, 
        )
        return response.output_text.strip()
    except Exception as e:
        print(f"GPT-5 Mini í˜¸ì¶œ ì˜¤ë¥˜: {e}")
        raise

def generate_t2i_core(pipe: StableDiffusionXLPipeline, prompt: str, width: int, height: int, steps: int) -> bytes:
    """SDXL T2Ië¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ê³  PNG ë°”ì´íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    negative_prompt = "low quality, blurry, text, watermark, distorted, ugly, tiling, poorly drawn"
    
    result = pipe(
        prompt=prompt, 
        negative_prompt=negative_prompt, 
        width=width, 
        height=height, 
        num_inference_steps=steps,
        guidance_scale=7.5 # ê¸°ë³¸ê°’ ì‚¬ìš©
    )
    image = result.images[0]
    
    # BytesIOë¡œ ë³€í™˜ í›„ ë°˜í™˜
    buf = io.BytesIO()
    image.save(buf, format="PNG")
    return buf.getvalue()

def generate_i2i_core(pipe: StableDiffusionXLImg2ImgPipeline, input_image_bytes: bytes, prompt: str, strength: float, width: int, height: int, steps: int) -> bytes:
    """SDXL I2Ië¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ í¸ì§‘/í•©ì„±í•˜ê³  PNG ë°”ì´íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    negative_prompt = "low quality, blurry, text, watermark, distorted, ugly, tiling, poorly drawn"
    
    # 1. ì›ë³¸ ì´ë¯¸ì§€ ë¡œë“œ ë° ë¦¬ì‚¬ì´ì¦ˆ (I2IëŠ” ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°ì— ì˜í–¥ì„ ë°›ìŒ)
    input_image = Image.open(io.BytesIO(input_image_bytes)).convert("RGB").resize((width, height))
    
    # 2. I2I íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    result = pipe(
        prompt=prompt, 
        image=input_image,
        strength=strength,
        negative_prompt=negative_prompt, 
        num_inference_steps=steps,
        guidance_scale=7.5
    )
    image = result.images[0]
    
    # 3. BytesIOë¡œ ë³€í™˜ í›„ ë°˜í™˜
    buf = io.BytesIO()
    image.save(buf, format="PNG")
    return buf.getvalue()


# ============================================================
# ğŸš€ API ì—”ë“œí¬ì¸íŠ¸ ì •ì˜
# ============================================================

@app.post("/api/caption", response_model=CaptionResponse)
async def create_caption(request: CaptionRequest):
    """
    GPT-5 Minië¥¼ ì‚¬ìš©í•˜ì—¬ í™ë³´ ë¬¸êµ¬ì™€ í•´ì‹œíƒœê·¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    if not openai_client:
        raise HTTPException(status_code=503, detail="GPT-5 Mini ì„œë¹„ìŠ¤ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
    info = {
        "service_type": request.service_type,
        "service_name": request.service_name,
        "features": request.features,
        "location": request.location,
        "event_info": "ì—†ìŒ"
    }

    try:
        output_text = generate_caption_core(openai_client, MODEL_GPT_MINI, request.tone, info)
        return CaptionResponse(output_text=output_text)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë¬¸êµ¬ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


@app.post("/api/generate_t2i", response_model=T2IResponse)
async def generate_t2i_image(request: T2IRequest):
    """
    SDXL T2Ië¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    if not T2I_PIPE:
        raise HTTPException(status_code=503, detail="SDXL T2I ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    try:
        # ë¹„ë™ê¸°ì ìœ¼ë¡œ GPU ì‘ì—…ì„ ì‹¤í–‰í•˜ëŠ” ê²½ìš° (FastAPIì˜ ê¸°ë³¸ ë™ì‘)
        image_bytes = await app.loop.run_in_executor(
            None, # ê¸°ë³¸ Executor (ì“°ë ˆë“œ í’€)
            generate_t2i_core,
            T2I_PIPE,
            request.prompt,
            request.width,
            request.height,
            request.steps
        )
        
        # Base64ë¡œ ì¸ì½”ë”©í•˜ì—¬ JSON ì‘ë‹µìœ¼ë¡œ ë°˜í™˜
        image_base64 = base64.b64encode(image_bytes).decode('utf-8')
        return T2IResponse(image_base64=image_base64)
    except Exception as e:
        # GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ë“±ì˜ ì˜¤ë¥˜ ì²˜ë¦¬
        raise HTTPException(status_code=500, detail=f"T2I ì´ë¯¸ì§€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


@app.post("/api/generate_i2i", response_model=T2IResponse) # I2Ië„ T2Iì™€ ë™ì¼í•˜ê²Œ Base64 ì‘ë‹µ
async def generate_i2i_image(request: I2IRequest):
    """
    SDXL I2Ië¥¼ ì‚¬ìš©í•˜ì—¬ ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í¸ì§‘/í•©ì„± ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    if not I2I_PIPE:
        raise HTTPException(status_code=503, detail="SDXL I2I ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
    try:
        # Base64 ë””ì½”ë”© (ì…ë ¥ ì´ë¯¸ì§€ ë°”ì´íŠ¸)
        input_image_bytes = base64.b64decode(request.input_image_base64)
        
        # ë¹„ë™ê¸°ì ìœ¼ë¡œ GPU ì‘ì—…ì„ ì‹¤í–‰í•˜ëŠ” ê²½ìš°
        image_bytes = await app.loop.run_in_executor(
            None,
            generate_i2i_core,
            I2I_PIPE,
            input_image_bytes,
            request.prompt,
            request.strength,
            request.width,
            request.height,
            request.steps
        )
        
        # Base64ë¡œ ì¸ì½”ë”©í•˜ì—¬ JSON ì‘ë‹µìœ¼ë¡œ ë°˜í™˜
        image_base64 = base64.b64encode(image_bytes).decode('utf-8')
        return T2IResponse(image_base64=image_base64)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"I2I ì´ë¯¸ì§€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# ìƒíƒœ í™•ì¸ìš© ì—”ë“œí¬ì¸íŠ¸
@app.get("/status")
def get_status():
    return {
        "gpt_ready": openai_client is not None,
        "sdxl_t2i_ready": T2I_PIPE is not None,
        "sdxl_i2i_ready": I2I_PIPE is not None,
        "device": DEVICE
    }