"""
FLUX.1-dev FP8 ì–‘ìí™” ëª¨ë¸ ë¡œë”© (LoRA í•™ìŠµìš©)

ì‚¬ìš©ë²•:
    python scripts/load_flux_fp8_for_lora.py

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ”:
1. FLUX.1-devë¥¼ FP8ë¡œ ì–‘ìí™”í•˜ì—¬ ë©”ëª¨ë¦¬ì— ë¡œë“œ
2. LoRA í•™ìŠµì„ ìœ„í•œ ì¤€ë¹„ (base_model ë°˜í™˜)
3. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥
"""
import torch
from diffusers import FluxTransformer2DModel, DiffusionPipeline
from torchao.quantization import quantize_, int8_weight_only
import gc


def load_flux_fp8_for_lora(
    model_path: str = "/home/shared/FLUX.1-dev",
    device: str = "cuda",
    dtype: torch.dtype = torch.bfloat16
):
    """
    FLUX.1-devë¥¼ FP8ë¡œ ì–‘ìí™”í•˜ì—¬ ë¡œë“œ

    Args:
        model_path: FLUX.1-dev ëª¨ë¸ ê²½ë¡œ
        device: ë””ë°”ì´ìŠ¤ (cuda)
        dtype: ë°ì´í„° íƒ€ì… (bfloat16)

    Returns:
        pipe: FLUX íŒŒì´í”„ë¼ì¸ (FP8 ì–‘ìí™”ë¨)
    """
    print("=" * 60)
    print("FLUX.1-dev FP8 ì–‘ìí™” ë¡œë”© (LoRA í•™ìŠµìš©)")
    print("=" * 60)

    # 1. Transformer ë¡œë“œ
    print("\nğŸ“¥ FLUX Transformer ë¡œë”© ì¤‘...")
    transformer = FluxTransformer2DModel.from_pretrained(
        model_path,
        subfolder="transformer",
        torch_dtype=dtype
    )
    print("âœ… Transformer ë¡œë“œ ì™„ë£Œ")

    # 2. FP8 ì–‘ìí™” ì ìš©
    print("\nğŸ”„ FP8 ì–‘ìí™” ì ìš© ì¤‘... (5-15ë¶„ ì†Œìš”)")
    quantize_(transformer, int8_weight_only())
    print("âœ… FP8 ì–‘ìí™” ì™„ë£Œ")

    # 3. íŒŒì´í”„ë¼ì¸ êµ¬ì„±
    print("\nğŸ”§ íŒŒì´í”„ë¼ì¸ êµ¬ì„± ì¤‘...")
    pipe = DiffusionPipeline.from_pretrained(
        model_path,
        transformer=transformer,
        torch_dtype=dtype
    )
    print("âœ… íŒŒì´í”„ë¼ì¸ êµ¬ì„± ì™„ë£Œ")

    # 4. GPUë¡œ ì´ë™
    print(f"\nğŸš€ GPUë¡œ ì´ë™ ì¤‘... (device: {device})")
    pipe = pipe.to(device)
    print("âœ… GPU ì´ë™ ì™„ë£Œ")

    # 5. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥
    if device == "cuda":
        print("\n" + "=" * 60)
        print("ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰")
        print("=" * 60)
        allocated = torch.cuda.memory_allocated() / 1024**3
        reserved = torch.cuda.memory_reserved() / 1024**3
        print(f"í• ë‹¹ëœ ë©”ëª¨ë¦¬: {allocated:.2f} GB")
        print(f"ì˜ˆì•½ëœ ë©”ëª¨ë¦¬: {reserved:.2f} GB")
        print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬: {23 - reserved:.2f} GB (L4 ê¸°ì¤€)")

    return pipe


def prepare_for_lora_training(pipe):
    """
    LoRA í•™ìŠµ ì¤€ë¹„

    Args:
        pipe: FLUX íŒŒì´í”„ë¼ì¸

    Returns:
        base_model: LoRA í•™ìŠµì— ì‚¬ìš©í•  base model
    """
    print("\n" + "=" * 60)
    print("LoRA í•™ìŠµ ì¤€ë¹„")
    print("=" * 60)

    # Transformerë¥¼ LoRA í•™ìŠµ ëª¨ë“œë¡œ ì „í™˜
    transformer = pipe.transformer

    # Gradient checkpointing í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½)
    if hasattr(transformer, "enable_gradient_checkpointing"):
        transformer.enable_gradient_checkpointing()
        print("âœ… Gradient checkpointing í™œì„±í™”")

    # í•™ìŠµ ëª¨ë“œë¡œ ì „í™˜
    transformer.train()
    print("âœ… í•™ìŠµ ëª¨ë“œ ì „í™˜ ì™„ë£Œ")

    print("\nğŸ“Œ ë‹¤ìŒ ë‹¨ê³„:")
    print("1. PEFT ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ LoRA ì„¤ì •:")
    print("   from peft import LoraConfig, get_peft_model")
    print("   lora_config = LoraConfig(")
    print("       r=8,  # LoRA rank")
    print("       lora_alpha=16,")
    print("       target_modules=['to_q', 'to_k', 'to_v', 'to_out'],")
    print("       lora_dropout=0.1")
    print("   )")
    print("   model = get_peft_model(pipe.transformer, lora_config)")
    print("\n2. ë°ì´í„°ì…‹ ì¤€ë¹„ ë° í•™ìŠµ ì‹œì‘")

    return pipe


def unload_model(pipe):
    """ëª¨ë¸ ì–¸ë¡œë“œ (ë©”ëª¨ë¦¬ í•´ì œ)"""
    print("\n" + "=" * 60)
    print("ëª¨ë¸ ì–¸ë¡œë“œ ì¤‘...")
    print("=" * 60)

    del pipe
    gc.collect()

    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    print("âœ… ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")


if __name__ == "__main__":
    # 1. FLUX FP8 ë¡œë“œ
    pipe = load_flux_fp8_for_lora()

    # 2. LoRA í•™ìŠµ ì¤€ë¹„
    pipe = prepare_for_lora_training(pipe)

    print("\n" + "=" * 60)
    print("ì¤€ë¹„ ì™„ë£Œ!")
    print("=" * 60)
    print("\nì´ì œ LoRA í•™ìŠµì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    print("pipe.transformerë¥¼ ì‚¬ìš©í•˜ì—¬ PEFT ì„¤ì • í›„ í•™ìŠµí•˜ì„¸ìš”.")

    # ì˜ˆì‹œ: í•™ìŠµ í›„ ì €ì¥
    print("\nğŸ“Œ LoRA ê°€ì¤‘ì¹˜ ì €ì¥ ë°©ë²•:")
    print("   model.save_pretrained('./lora_weights')")
    print("\nğŸ“Œ LoRA ì ìš© ë°©ë²•:")
    print("   from peft import PeftModel")
    print("   model = PeftModel.from_pretrained(base_model, './lora_weights')")

    # í…ŒìŠ¤íŠ¸ ìƒì„± (ì„ íƒ)
    test_generation = input("\ní…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„±ì„ í•´ë³´ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
    if test_generation.lower() == 'y':
        print("\nğŸ¨ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„± ì¤‘...")
        image = pipe(
            prompt="A cute cat",
            width=1024,
            height=1024,
            num_inference_steps=4,
            guidance_scale=3.5,
            generator=torch.Generator(device="cuda").manual_seed(42)
        ).images[0]
        image.save("test_flux_fp8.png")
        print("âœ… ì €ì¥ ì™„ë£Œ: test_flux_fp8.png")

    # ì–¸ë¡œë“œ ì—¬ë¶€
    unload = input("\nëª¨ë¸ì„ ì–¸ë¡œë“œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
    if unload.lower() == 'y':
        unload_model(pipe)
