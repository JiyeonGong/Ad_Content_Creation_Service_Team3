"""
FLUX.1-dev FP8 ì–‘ìí™” ëª¨ë¸ ë¡œë”© (LoRA í•™ìŠµìš©)

ì‚¬ìš©ë²•:
    python scripts/load_flux_fp8_for_lora.py

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ”:
1. ì‚¬ì „ ì–‘ìí™”ëœ FLUX FP8 ëª¨ë¸ì„ Hugging Faceì—ì„œ ë¡œë“œ
2. LoRA í•™ìŠµì„ ìœ„í•œ ì¤€ë¹„ (base_model ë°˜í™˜)
3. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥

ì°¸ê³ :
- ì‚¬ì „ ì–‘ìí™” ëª¨ë¸: diffusers/FLUX.1-dev-torchao-fp8 (~12GB)
- GPU 22GBì— ì™„ì „íˆ ë¡œë“œ ê°€ëŠ¥ (CPU offload ë¶ˆí•„ìš”)
"""
import torch
from diffusers import FluxPipeline
import gc


def load_flux_fp8_for_lora(
    model_id: str = "diffusers/FLUX.1-dev-torchao-fp8",
    device: str = "cuda",
    dtype: torch.dtype = torch.bfloat16,
    cache_dir: str = "/home/shared/models"
):
    """
    ì‚¬ì „ ì–‘ìí™”ëœ FLUX FP8 ëª¨ë¸ ë¡œë“œ

    Args:
        model_id: Hugging Face ëª¨ë¸ ID
        device: ë””ë°”ì´ìŠ¤ (cuda)
        dtype: ë°ì´í„° íƒ€ì… (bfloat16)
        cache_dir: ëª¨ë¸ ìºì‹œ ë””ë ‰í† ë¦¬

    Returns:
        pipe: FLUX íŒŒì´í”„ë¼ì¸ (FP8 ì‚¬ì „ ì–‘ìí™”)
    """
    print("=" * 60)
    print("FLUX.1-dev FP8 ì‚¬ì „ ì–‘ìí™” ëª¨ë¸ ë¡œë”© (LoRA í•™ìŠµìš©)")
    print("=" * 60)

    # 1. ì‚¬ì „ ì–‘ìí™”ëœ FP8 ëª¨ë¸ ë¡œë“œ
    # âš ï¸ ì‚¬ì „ ì–‘ìí™”ëœ torchao ëª¨ë¸ì€ CPU/disk offload ë¯¸ì§€ì›
    # â†’ device_map ì—†ì´ ì§ì ‘ .to(device) ì‚¬ìš©
    print("\nğŸ“¥ ì‚¬ì „ ì–‘ìí™”ëœ FLUX FP8 ëª¨ë¸ ë¡œë”© ì¤‘...")
    print(f"  ëª¨ë¸: {model_id}")
    print(f"  ìºì‹œ: {cache_dir}")
    print("  â„¹ï¸  ì–‘ìí™” ê³¼ì • ë¶ˆí•„ìš” - ë°”ë¡œ ë¡œë”©! (~12GB)")

    pipe = FluxPipeline.from_pretrained(
        model_id,
        torch_dtype=dtype,
        use_safetensors=False,  # torchao ì–‘ìí™” ëª¨ë¸ì€ pickle í˜•ì‹
        cache_dir=cache_dir
    ).to(device)  # GPUë¡œ ì§ì ‘ ì´ë™

    print("âœ… FP8 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    # GPU ë©”ëª¨ë¦¬ í™•ì¸
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3
        print(f"ğŸ“Š GPU ë©”ëª¨ë¦¬: {allocated:.2f} GB")

    # 5. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥
    if device == "cuda":
        print("\n" + "=" * 60)
        print("ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰")
        print("=" * 60)
        allocated = torch.cuda.memory_allocated() / 1024**3
        reserved = torch.cuda.memory_reserved() / 1024**3
        print(f"í• ë‹¹ëœ ë©”ëª¨ë¦¬: {allocated:.2f} GB")
        print(f"ì˜ˆì•½ëœ ë©”ëª¨ë¦¬: {reserved:.2f} GB")
        print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬: {23 - reserved:.2f} GB (L4 ê¸°ì¤€)")

    return pipe


def prepare_for_lora_training(pipe):
    """
    LoRA í•™ìŠµ ì¤€ë¹„

    Args:
        pipe: FLUX íŒŒì´í”„ë¼ì¸

    Returns:
        base_model: LoRA í•™ìŠµì— ì‚¬ìš©í•  base model
    """
    print("\n" + "=" * 60)
    print("LoRA í•™ìŠµ ì¤€ë¹„")
    print("=" * 60)

    # Transformerë¥¼ LoRA í•™ìŠµ ëª¨ë“œë¡œ ì „í™˜
    transformer = pipe.transformer

    # Gradient checkpointing í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½)
    if hasattr(transformer, "enable_gradient_checkpointing"):
        transformer.enable_gradient_checkpointing()
        print("âœ… Gradient checkpointing í™œì„±í™”")

    # í•™ìŠµ ëª¨ë“œë¡œ ì „í™˜
    transformer.train()
    print("âœ… í•™ìŠµ ëª¨ë“œ ì „í™˜ ì™„ë£Œ")

    print("\nğŸ“Œ ë‹¤ìŒ ë‹¨ê³„:")
    print("1. PEFT ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ LoRA ì„¤ì •:")
    print("   from peft import LoraConfig, get_peft_model")
    print("   lora_config = LoraConfig(")
    print("       r=8,  # LoRA rank")
    print("       lora_alpha=16,")
    print("       target_modules=['to_q', 'to_k', 'to_v', 'to_out'],")
    print("       lora_dropout=0.1")
    print("   )")
    print("   model = get_peft_model(pipe.transformer, lora_config)")
    print("\n2. ë°ì´í„°ì…‹ ì¤€ë¹„ ë° í•™ìŠµ ì‹œì‘")

    return pipe


def unload_model(pipe):
    """ëª¨ë¸ ì–¸ë¡œë“œ (ë©”ëª¨ë¦¬ í•´ì œ)"""
    print("\n" + "=" * 60)
    print("ëª¨ë¸ ì–¸ë¡œë“œ ì¤‘...")
    print("=" * 60)

    del pipe
    gc.collect()

    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    print("âœ… ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")


if __name__ == "__main__":
    # 1. FLUX FP8 ë¡œë“œ
    pipe = load_flux_fp8_for_lora()

    # 2. LoRA í•™ìŠµ ì¤€ë¹„
    pipe = prepare_for_lora_training(pipe)

    print("\n" + "=" * 60)
    print("ì¤€ë¹„ ì™„ë£Œ!")
    print("=" * 60)
    print("\nì´ì œ LoRA í•™ìŠµì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    print("pipe.transformerë¥¼ ì‚¬ìš©í•˜ì—¬ PEFT ì„¤ì • í›„ í•™ìŠµí•˜ì„¸ìš”.")

    # ì˜ˆì‹œ: í•™ìŠµ í›„ ì €ì¥
    print("\nğŸ“Œ LoRA ê°€ì¤‘ì¹˜ ì €ì¥ ë°©ë²•:")
    print("   model.save_pretrained('./lora_weights')")
    print("\nğŸ“Œ LoRA ì ìš© ë°©ë²•:")
    print("   from peft import PeftModel")
    print("   model = PeftModel.from_pretrained(base_model, './lora_weights')")

    # í…ŒìŠ¤íŠ¸ ìƒì„± (ì„ íƒ)
    test_generation = input("\ní…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„±ì„ í•´ë³´ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
    if test_generation.lower() == 'y':
        print("\nğŸ¨ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„± ì¤‘...")
        image = pipe(
            prompt="A cute cat",
            width=1024,
            height=1024,
            num_inference_steps=28,  # flux-dev ê¸°ë³¸ê°’
            guidance_scale=3.5,
            generator=torch.Generator(device="cuda").manual_seed(42)
        ).images[0]
        image.save("test_flux_fp8.png")
        print("âœ… ì €ì¥ ì™„ë£Œ: test_flux_fp8.png")

    # ì–¸ë¡œë“œ ì—¬ë¶€
    unload = input("\nëª¨ë¸ì„ ì–¸ë¡œë“œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
    if unload.lower() == 'y':
        unload_model(pipe)
