# /home/spai0323/Ad_Content_Creation_Service_Team3/test/prompt_self_test.py
import os
import sys
import re
from typing import Dict, Any, Optional
from pprint import pprint

# ============================================================
# ğŸ“Œ ê²½ë¡œ ì„¤ì • â€” backend íŒ¨í‚¤ì§€ê°€ import ê°€ëŠ¥í•˜ë„ë¡ sys.pathì— ì¶”ê°€
# ============================================================

PROJECT_ROOT = "/home/spai0323/Ad_Content_Creation_Service_Team3"
SRC_PATH = os.path.join(PROJECT_ROOT, "src")   # backend ìƒìœ„ í´ë”

if SRC_PATH not in sys.path:
    sys.path.append(SRC_PATH)

# ì´ì œ backend íŒ¨í‚¤ì§€ import ê°€ëŠ¥
from backend import services
from backend.model_registry import get_registry

registry = get_registry()   # ë°˜ë“œì‹œ import ë°”ë¡œ ì•„ë˜!

# ============================================================
# ğŸ” ë³´ì¡° í•¨ìˆ˜
# ============================================================

def has_korean(text: str) -> bool:
    """í•œê¸€ í¬í•¨ ì—¬ë¶€ ê°„ë‹¨ ì²´í¬"""
    return bool(re.search(r"[ê°€-í£]", text or ""))


def rough_token_count(text: str) -> int:
    """ì•„ì£¼ ë‹¨ìˆœí•œ í† í° ìˆ˜ ê·¼ì‚¬ì¹˜ (ê³µë°± ê¸°ì¤€)"""
    if not text:
        return 0
    return len(text.split())


def print_header(title: str):
    print("\n" + "=" * 80)
    print(f"ğŸ” {title}")
    print("=" * 80)


# ============================================================
# ğŸ”¬ ë‹¨ì¼ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
# ============================================================

def run_single_test(
    name: str,
    raw_prompt: str,
    context: Optional[Dict[str, Any]],
    model_name: str
):
    print_header(f"í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {name}")
    print(f"ğŸ“Œ ëª¨ë¸: {model_name}")
    print(f"ğŸ“¥ RAW í”„ë¡¬í”„íŠ¸: {raw_prompt!r}")
    if context:
        print(f"ğŸ“¥ ì»¨í…ìŠ¤íŠ¸: {context}")

    model_config = registry.get_model(model_name)
    if not model_config:
        print(f"âŒ ëª¨ë¸ ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_name}")
        return

    try:
        final_prompt = services.build_final_prompt_v2(
            raw_prompt=raw_prompt,
            context=context,
            model_config=model_config,
        )
    except Exception as e:
        print(f"âŒ build_final_prompt_v2 ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        return

    print("-" * 80)
    print(f"ğŸ“¤ ìµœì¢… í”„ë¡¬í”„íŠ¸:\n{final_prompt}\n")

    raw_tokens = rough_token_count(raw_prompt)
    final_tokens = rough_token_count(final_prompt)

    print(f"ğŸ”¢ í† í° ìˆ˜ ì¶”ì •: RAW={raw_tokens}, FINAL={final_tokens}")

    warnings = []

    # FLUX ëª¨ë¸ íŒì •
    is_flux = "flux" in (getattr(model_config, "type") or "").lower()

    if is_flux:
        if final_tokens > 70:
            warnings.append(f"FLUX í”„ë¡¬í”„íŠ¸ê°€ ë„ˆë¬´ ê¸¸ì–´ ë³´ì…ë‹ˆë‹¤ (ì•½ {final_tokens} í† í°).")

        if has_korean(final_prompt):
            warnings.append("FLUX ìµœì¢… í”„ë¡¬í”„íŠ¸ì— í•œê¸€ì´ ë‚¨ì•„ ìˆìŠµë‹ˆë‹¤.")

        banned_keywords = ["negative prompt", "low quality", "worst quality"]
        for kw in banned_keywords:
            if kw.lower() in final_prompt.lower():
                warnings.append(f"FLUX ê¸ˆì§€ í‘œí˜„ í¬í•¨ ê°€ëŠ¥ì„±: {kw}")

    if raw_prompt.strip() and not final_prompt.strip():
        warnings.append("RAWëŠ” ë¹„ì–´ìˆì§€ ì•Šì€ë° FINALì´ ë¹„ì–´ ìˆìŒ")

    if warnings:
        print("\nâš  ê²½ê³ :")
        for w in warnings:
            print(f"  - {w}")
    else:
        print("âœ… íŠ¹ì´ì‚¬í•­ ì—†ìŒ")


# ============================================================
# ğŸ§ª í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‹¤í–‰
# ============================================================

def main():
    print_header("í”„ë¡¬í”„íŠ¸ ìµœì í™” ì…€í”„ í…ŒìŠ¤íŠ¸ íˆ´")

    # OpenAI ì‚¬ìš© ì—¬ë¶€
    openai_key = os.getenv("OPENAI_API_KEY")
    if not openai_key:
        print("âš  OPENAI_API_KEY ì—†ìŒ â†’ GPT ìµœì í™”ëŠ” ë¹„í™œì„±í™”ë¨.")
    else:
        print("âœ… OPENAI_API_KEY ì„¤ì •ë¨ â†’ GPT ìµœì í™” ì‚¬ìš© ê°€ëŠ¥")

    opt_config = registry.get_prompt_optimization_config()
    print(f"\nğŸ§© prompt_optimization ì„¤ì •: {opt_config}")

    # FLUX ëª¨ë¸ ìš°ì„  ì„ íƒ
    candidate_models = list(registry.models.keys())
    flux_models = [
        name for name in candidate_models
        if "flux" in (registry.get_model(name).type or "").lower()
    ]
    if flux_models:
        default_model_name = flux_models[0]
    else:
        default_model_name = registry.get_primary_model()

    print(f"\nğŸ¯ ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ëª¨ë¸: {default_model_name}\n")

    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ êµ¬ì„±
    test_cases = [
        {
            "name": "í•œêµ­ì–´ ê°„ë‹¨ í”„ë¡¬í”„íŠ¸",
            "raw": "ë”°ëœ»í•œ ê°ì„±ì˜ ìš”ê°€ ìŠ¤íŠœë””ì˜¤",
            "context": {"style": "clean", "mood": "warm"},
        },
        {
            "name": "í•œêµ­ì–´ + caption",
            "raw": "í¸ì•ˆí•œ ë¶„ìœ„ê¸°ì˜ í•„ë¼í…ŒìŠ¤ ê³µê°„",
            "context": {
                "style": "instagram",
                "mood": "cozy",
                "caption": "1:1 í”„ë¦¬ë¯¸ì—„ ì¼€ì–´ í”„ë¡œê·¸ë¨"
            },
        },
        {
            "name": "ì˜ì–´ í”„ë¡¬í”„íŠ¸",
            "raw": "a bright fitness studio with soft natural light",
            "context": {"style": "professional", "mood": "fresh"},
        },
        {
            "name": "ì•„ì£¼ ì§§ì€ í”„ë¡¬í”„íŠ¸",
            "raw": "í—¬ìŠ¤ì¥",
            "context": {"style": "minimal", "mood": "clean"},
        },
        {
            "name": "ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ (í•œêµ­ì–´ ê¸´ë¬¸ì¥)",
            "raw": (
                "ê°•ë‚¨ì— ìœ„ì¹˜í•œ í”„ë¦¬ë¯¸ì—„ í—¬ìŠ¤ì¥, ìì—°ê´‘ì´ ë“¤ì–´ì˜¤ëŠ” ë„“ì€ ê³µê°„, "
                "ì „ë¬¸ íŠ¸ë ˆì´ë„ˆì˜ 1:1 ì½”ì¹­, ê³ ê¸‰ ë¨¸ì‹ ë“¤ë¡œ ê°€ë“ ì°¬ ì‹œì„¤"
            ),
            "context": {"style": "luxury", "mood": "premium"},
        },
    ]

    # ì‹¤í–‰
    for case in test_cases:
        run_single_test(
            name=case["name"],
            raw_prompt=case["raw"],
            context=case.get("context"),
            model_name=default_model_name,
        )

    print("\n\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‹¤í–‰ ì™„ë£Œ!\n")


if __name__ == "__main__":
    main()
