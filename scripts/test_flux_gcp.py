# FLUX ì–‘ìí™” ëª¨ë¸ LoRA í•™ìŠµ ì‹¤í—˜ìš© ìŠ¤í¬ë¦½íŠ¸

import os
import sys
import argparse
import time
from pathlib import Path
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import yaml
import torch
from PIL import Image

# src/backend ëª¨ë“ˆ ì„í¬íŠ¸
from src.backend.model_loader import ModelLoader
from src.backend.model_registry import get_registry


def load_test_config(config_path: str) -> dict:
    """í…ŒìŠ¤íŠ¸ ì„¤ì • íŒŒì¼ ë¡œë“œ"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def setup_output_dir(base_dir: str) -> Path:
    """ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = Path(base_dir) / timestamp
    output_dir.mkdir(parents=True, exist_ok=True)
    return output_dir


def save_image(image: Image.Image, output_dir: Path, scenario_name: str, idx: int) -> str:
    """ì´ë¯¸ì§€ ì €ì¥"""
    filename = f"{scenario_name}_{idx:02d}.png"
    filepath = output_dir / filename
    image.save(filepath, format="PNG")
    return str(filepath)


def save_metadata(output_dir: Path, scenario: dict, generation_time: float):
    """ë©”íƒ€ë°ì´í„° ì €ì¥"""
    metadata = {
        "scenario_name": scenario["name"],
        "prompt": scenario["prompt"],
        "parameters": {
            "width": scenario["width"],
            "height": scenario["height"],
            "steps": scenario["num_inference_steps"],
            "guidance_scale": scenario.get("guidance_scale"),
            "seed": scenario.get("seed"),
        },
        "generation_time_seconds": round(generation_time, 2),
        "description": scenario.get("description", ""),
    }

    metadata_file = output_dir / f"{scenario['name']}_metadata.yaml"
    with open(metadata_file, 'w', encoding='utf-8') as f:
        yaml.dump(metadata, f, allow_unicode=True, default_flow_style=False)


def run_scenario(loader: ModelLoader, scenario: dict, output_dir: Path, verbose: bool = True):
    """ë‹¨ì¼ ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰"""
    name = scenario["name"]
    print(f"\n{'='*60}")
    print(f"ğŸ¨ ì‹¤í—˜ ì‹œì‘: {name}")
    print(f"{'='*60}")

    if verbose:
        print(f"ğŸ“ í”„ë¡¬í”„íŠ¸: {scenario['prompt']}")
        print(f"ğŸ“ í¬ê¸°: {scenario['width']}x{scenario['height']}")
        print(f"ğŸ”¢ Steps: {scenario['num_inference_steps']}")
        print(f"ğŸšï¸ Guidance: {scenario.get('guidance_scale', 'N/A')}")
        print(f"ğŸŒ± Seed: {scenario.get('seed', 'N/A')}")
        print(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ê°œìˆ˜: {scenario['num_images']}")

    start_time = time.time()

    # ì´ë¯¸ì§€ ìƒì„± (ìˆœì°¨ì ìœ¼ë¡œ 1ê°œì”©)
    images = []
    for i in range(scenario["num_images"]):
        print(f"\n  [{i+1}/{scenario['num_images']}] ìƒì„± ì¤‘...")

        # ì‹œë“œ ì„¤ì •
        seed = scenario.get("seed")
        if seed is not None:
            generator = torch.Generator(device=loader.device).manual_seed(seed + i)
        else:
            generator = None

        # íŒŒì´í”„ë¼ì¸ íŒŒë¼ë¯¸í„°
        pipeline_params = {
            "prompt": scenario["prompt"],
            "width": scenario["width"],
            "height": scenario["height"],
            "num_inference_steps": scenario["num_inference_steps"],
            "num_images_per_prompt": 1,  # í•œ ë²ˆì— 1ê°œì”©
            "generator": generator,
        }

        # FLUXëŠ” guidance_scale ì„ íƒì 
        if scenario.get("guidance_scale") is not None:
            pipeline_params["guidance_scale"] = scenario["guidance_scale"]

        # FLUX max_sequence_length
        if loader.current_model_config.type == "flux":
            pipeline_params["max_sequence_length"] = loader.current_model_config.max_tokens

        # ì´ë¯¸ì§€ ìƒì„±
        output = loader.t2i_pipe(**pipeline_params)
        images.extend(output.images)

        # ì¤‘ê°„ ë©”ëª¨ë¦¬ ì •ë¦¬ (ë§ˆì§€ë§‰ ì´ë¯¸ì§€ ì œì™¸)
        if i < scenario["num_images"] - 1:
            import gc
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()

    generation_time = time.time() - start_time

    # ì´ë¯¸ì§€ ì €ì¥
    print(f"\nğŸ’¾ ì´ë¯¸ì§€ ì €ì¥ ì¤‘...")
    saved_paths = []
    for i, image in enumerate(images):
        path = save_image(image, output_dir, name, i)
        saved_paths.append(path)
        print(f"  âœ“ {path}")

    # ë©”íƒ€ë°ì´í„° ì €ì¥
    save_metadata(output_dir, scenario, generation_time)

    print(f"\nâœ… ì™„ë£Œ!")
    print(f"  ìƒì„± ì‹œê°„: {generation_time:.2f}ì´ˆ")
    print(f"  í‰ê· /ì´ë¯¸ì§€: {generation_time/len(images):.2f}ì´ˆ")

    # GPU ë©”ëª¨ë¦¬ ì •ë³´ (ìˆìœ¼ë©´)
    if torch.cuda.is_available():
        memory_allocated = torch.cuda.memory_allocated() / 1024**3
        memory_reserved = torch.cuda.memory_reserved() / 1024**3
        print(f"  GPU ë©”ëª¨ë¦¬: {memory_allocated:.2f}GB (í• ë‹¹) / {memory_reserved:.2f}GB (ì˜ˆì•½)")

    return saved_paths


def main():
    parser = argparse.ArgumentParser(description="GCP VM FLUX í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸")
    parser.add_argument(
        "--config",
        type=str,
        default="configs/test_flux_gcp.yaml",
        help="í…ŒìŠ¤íŠ¸ ì„¤ì • íŒŒì¼ ê²½ë¡œ",
    )
    parser.add_argument(
        "--scenario",
        type=str,
        default=None,
        help="ì‹¤í–‰í•  íŠ¹ì • ì‹œë‚˜ë¦¬ì˜¤ ì´ë¦„ (ì—†ìœ¼ë©´ enabled=trueì¸ ëª¨ë“  ì‹œë‚˜ë¦¬ì˜¤)",
    )
    args = parser.parse_args()

    # ì„¤ì • ë¡œë“œ
    print(f"ğŸ“‚ ì„¤ì • ë¡œë“œ ì¤‘: {args.config}")
    config = load_test_config(args.config)

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì¤€ë¹„
    output_dir = setup_output_dir(config["output"]["base_dir"])
    print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")

    # ModelLoader ì´ˆê¸°í™”
    print(f"\nğŸ”§ ModelLoader ì´ˆê¸°í™” ì¤‘...")
    model_name = config["model"]["name"]  # "flux-dev" ë˜ëŠ” "flux-dev-gcp"

    # ìºì‹œ ë””ë ‰í† ë¦¬ (GCP VMì—ì„œëŠ” /home/shared ì‚¬ìš©)
    cache_dir = str(Path(config["model"]["path"]).parent)

    # bfloat16 ì‚¬ìš© ì—¬ë¶€
    use_bfloat16 = config["memory_optimization"]["dtype"] == "bfloat16"

    loader = ModelLoader(cache_dir=cache_dir, use_bfloat16=use_bfloat16)

    # ëª¨ë¸ ë¡œë“œ
    print(f"\nğŸš€ ëª¨ë¸ ë¡œë”©: {model_name}")

    # ì„¤ì •ì—ì„œ ì§€ì •í•œ ëª¨ë¸ëª… ì‚¬ìš©
    success = loader.load_model(model_name)

    if not success:
        print("âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
        sys.exit(1)

    # í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰
    scenarios = config["test_scenarios"]

    # íŠ¹ì • ì‹œë‚˜ë¦¬ì˜¤ë§Œ ì‹¤í–‰
    if args.scenario:
        scenarios = [s for s in scenarios if s["name"] == args.scenario]
        if not scenarios:
            print(f"âŒ ì‹œë‚˜ë¦¬ì˜¤ '{args.scenario}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(1)
    else:
        # enabled=trueì¸ ì‹œë‚˜ë¦¬ì˜¤ë§Œ
        scenarios = [s for s in scenarios if s.get("enabled", True)]

    # run_only í•„í„° ì ìš©
    run_only = config["execution"].get("run_only", [])
    if run_only:
        scenarios = [s for s in scenarios if s["name"] in run_only]

    if not scenarios:
        print("âŒ ì‹¤í–‰í•  ì‹œë‚˜ë¦¬ì˜¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)

    print(f"\nğŸ“‹ ì‹¤í–‰í•  ì‹œë‚˜ë¦¬ì˜¤: {len(scenarios)}ê°œ")
    for s in scenarios:
        print(f"  - {s['name']}: {s.get('description', '')}")

    # ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰
    total_start = time.time()
    results = {}

    for i, scenario in enumerate(scenarios):
        try:
            saved_paths = run_scenario(
                loader,
                scenario,
                output_dir,
                verbose=config["execution"].get("verbose", True)
            )
            results[scenario["name"]] = {
                "status": "success",
                "paths": saved_paths,
            }

            # ë‹¤ìŒ ì‹¤í—˜ ì „ ëŒ€ê¸°
            if i < len(scenarios) - 1:
                delay = config["execution"].get("delay_between_experiments", 2)
                if delay > 0:
                    print(f"\nâ³ {delay}ì´ˆ ëŒ€ê¸° ì¤‘...")
                    time.sleep(delay)

        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()

            results[scenario["name"]] = {
                "status": "failed",
                "error": str(e),
            }

            if not config["execution"].get("continue_on_error", True):
                print("âŒ continue_on_error=false, ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                break

    total_time = time.time() - total_start

    # ìµœì¢… ìš”ì•½
    print(f"\n{'='*60}")
    print(f"ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print(f"{'='*60}")
    print(f"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ")
    print(f"\nğŸ“Š ê²°ê³¼ ìš”ì•½:")

    success_count = sum(1 for r in results.values() if r["status"] == "success")
    failed_count = len(results) - success_count

    print(f"  âœ… ì„±ê³µ: {success_count}ê°œ")
    print(f"  âŒ ì‹¤íŒ¨: {failed_count}ê°œ")

    print(f"\nğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")

    # ê²°ê³¼ ì €ì¥
    results_file = output_dir / "test_results.yaml"
    with open(results_file, 'w', encoding='utf-8') as f:
        yaml.dump({
            "total_time_seconds": round(total_time, 2),
            "success_count": success_count,
            "failed_count": failed_count,
            "results": results,
        }, f, allow_unicode=True, default_flow_style=False)

    print(f"ğŸ“„ ê²°ê³¼ íŒŒì¼: {results_file}")

    # ëª¨ë¸ ì–¸ë¡œë“œ
    print(f"\nğŸ—‘ï¸ ëª¨ë¸ ì–¸ë¡œë“œ ì¤‘...")
    loader.unload_model()

    print(f"\nâœ¨ ì™„ë£Œ!")


if __name__ == "__main__":
    main()
